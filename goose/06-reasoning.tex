\section{Modeling and reasoning about Go}%
\label{sec:goose:reasoning}

A key principle in the design of Goose is to model features of Go as code,
written as libraries on top of the base GooseLang language described in
\autoref{sec:goose:lang}. In this section we describe some of the features of Go
we model, as well as reasoning principles we develop on top to verify code that
uses these features.

Specifications in this section are written in Perennial, which is a concurrent
separation logic. The basic specification of this logic is the triple
$\hoare{P}{\cc{f()}}{Q}$, which says that if \cc{f()} is run in a state
satisfying the precondition $P$ and terminates, the final state will satisfy the
postcondition $Q$. Separation logic additionally means that $P$ and $Q$ can
describe only the state actually involved in \cc{f()}, implicitly stating other
state is unmodified (we can give so-called ``small footprint'' specifications).
Assertions make use of the \emph{points-to assertion} $\ell \mapsto v$, which
gives the value of one address in memory. For example, some basic specifications
in separation logic are those for the load and store operations:
%
\begin{mathpar}
  \hoare{\ell \mapsto v}{!\ell}{\Ret{v} \ell \mapsto v}

  \hoare{\ell \mapsto v_0}{\ell \gets v}{\ell \mapsto v}
\end{mathpar}

The specification for load uses $\Ret{v}$ to specify what the return value is.

The specifications in this section can be appreciated with only basic
familiarity with sequential separation logic; concurrency concerns do not arise
much at this level, and there is nothing specific to crash safety. All of these
specifications are actually proven in the Perennial logic and can be used to
reason about concurrency and crash safety, though, as we did for GoTxn.

\subsection{Modeling pointers}%
\label{sec:goose:pointers}

Pointers turn out to be slightly subtle because of concurrency. In
short, GooseLang disallows concurrent reads and writes to the same
location by making such racy access undefined behavior (any
specification for a program implies that if the precondition holds, the
program never exhibits undefined behavior). The hardware provides some
guarantees, but they are relatively weak: for example, different cores
can observe writes at different times. Go's own memory model specifies
even weaker guarantees. Rather than attempt to formalize Go's rules
(which are complex and involve defining a partial order over all program
instructions), we side step the issue and make any races undefined,
which works for our intended use cases since we always synchronize
concurrent access with locks.

To disallow concurrent reads and writes we first detect them. The key is to make
$x \gets v$, the primitive store operation, \emph{non-atomic} by splitting it
into two operations. The GooseLang semantics tracks the behavior of these
operations by augmenting the heap with extra information; each address in the
semantics has a $\textdom{NonAtom}$ which can be
$\goosekw{Reading} \app n \app v$ if there are $n$ readers and the value is $v$,
or $\goosekw{Writing} \app v$ if a thread is currently writing. We actually only
use non-atomic stores for normal pointers, but we have support for non-atomic
reads as well for iterating over maps.

Ordinarily values in the heap are of the form $\goosekw{Reading} \app 0 \app v$,
to indicate no readers or writers. Writes are split into
$\goosekw{PrepareWrite}(\ell)$, which sets the value of $\ell$ to
$\goosekw{Writing} \app v_{0}$, and $\goosekw{FinishStore}(\ell, v)$ which sets
it to $\goosekw{Reading} \app 0 \app v$. A concurrent write will be undefined
since $\goosekw{PrepareWrite}(\ell)$ requires no concurrent writers, and
similarly for a concurrent read which is undefined if the address is being
written. Non-atomic reads are similar with $\goosekw{StartRead}$ and
$\goosekw{FinishRead}$; these increment and decrement the number of readers,
respectively, so that multiple readers can run concurrently but any concurrent
writer has undefined behavior.

Next, we need reasoning principles to abstract away this complexity from
program verification. Separation logic turns out to provide the right
language to reason about racy access. When a thread owns
$\ell \mapsto v$, we know no other thread can have access to location
$\ell$, so the specifications for reads and writes are unaffected by the
operations being non-atomic (although their proofs are a bit more
complicated to deal with the new semantics). The only change is that the
Read operation is no longer an atomic primitive but a function that
takes two execution steps. In Iris this means that two threads cannot
share memory with an invariant and must mediate access with a lock,
which transfers ownership of the $\ell \mapsto v$ for multiple execution
steps.

\subsection{Locks}

\newcommand{\Acquire}{\goosedef{Acquire}}
\newcommand{\CAS}{\goosedef{CAS}}

As is typical in Goose, locks are not built-in to GooseLang but modeled
using an implementation based on simpler primitives. Since locks are the
only synchronization primitive, implementing them requires shared
concurrent access, which ordinary pointers do not have in GooseLang.
Instead, the language also includes a primitive atomic compare-and-swap
operation that is only used to implement a model of locks. We could also
use the same operation to model Go's low-level atomic operations, like
\cc{atomic.CompareAndSwapUint64} and \cc{atomic.LoadUint64}, but
have not implemented this yet since we don't have code that uses these
low-level synchronization primitives.

The model of locks is simple enough to give the code in its entirety. The lock
is represented as a pointer to a boolean that is true if the lock is held. As a
helper we define $\CAS$ (compare-and-swap), a variant of compare-and-exchange
that only returns a boolean on success and not also the previous value.

\begin{align*}
  \CAS &\defeq \gooselambda{x, v1, v2} \pi_{2}\app \goosekw{CmpXchg}(x, v1, v2) \\
  \goosedef{NewLock} & \defeq \gooselambda{\_} \goosekw{ref} \app \goosekw{false} \\
  \Acquire &\defeq \gooselambda{l} \\
       &\goosekw{let}\app f = (\goosekw{rec}\: \textlog{tryAcquire}(\_) = \\
       &\qquad \gooseif{\CAS \app l \app \goosekw{false} \app \goosekw{true}}%
         {()}{\textlog{tryAcquire} \app ()}) \app\goosekw{in} \\
       &f \app () \\
  \goosedef{Release} &\defeq \gooselambda{l} l \gets \goosekw{false} \\
\end{align*}

Acquiring a lock is modeled as repeatedly using
$\CAS \app l \app \goosekw{false} \app \goosekw{true}$ to
atomically test that the lock is false and set it to true if so, while release
stores false to the lock. This implementation as a spin lock is merely an
operational model that captures what the lock does: acquire blocks until the
lock is free and sets it to locked, while release frees the lock. This code is
used to model Go's builtin \cc{*sync.Mutex}, which is implemented more
efficiently than spin locks with cooperation from the runtime and operating
system.

The specification for locks is a typical one for concurrent separation logic,
based on associating a \emph{lock invariant} with each lock, which is a predicate that holds when the lock
is free. Because this is a separation logic, we can also interpret the lock
invariant as ownership over some data (for example, some region of memory); the
lock mediates access to this ownership, handing it out when the lock is acquired
and requiring it back when the lock is released. We prove this specification
sound against the GooseLang spin-lock implementation.

\subsection{Structs}

\newidentmacro{LoadTyped}
\newidentmacro{StoreTyped}
\newidentmacro{loadField}
\newidentmacro{storeField}
\newdefmacro{structType}

One of the most important features of Go to support is structs. Goose support
for structs uses a form of \emph{shallow embedding} using a combination of
Gallina (Coq's functional language) and GooseLang. We encode
higher-level primitives like field access on top of GooseLang primitives like
tuples and continguous memory.

Struct types are represented using a combination of two Gallina types,
$t \in \textdom{GooseType}$ gives the type of a struct, while
$s \in \textdom{StructDecl}$ is an (anonymous) struct declaration that gives its
field names and their types. The definitions of these two types are given in
\autoref{fig:goose:types}. The exact types are not important in the semantics,
but we do use the shape of the struct to determine how they are laid out in
memory to support pointers to individual fields. Using types to represent these
shapes makes the translation much simpler, since we have access to types from
the Go type checker via the \cc{go/types} package. As an example of how
approximate these types are, it is sufficient to have a \goosekw{ptrT} type for
all pointers; we will use the Go types to determine its type and the types of
its fields if it is a pointer to a struct.

\begin{figure}[ht!]
\begin{mathpar}
  \begin{array}{lrcl}
    \textdom{GooseType} & t &::=& \goosekw{uint64T} \ALT \goosekw{boolT} \ALT
                                  \goosekw{unitT} \ALT
                                  \dots \\
                        &&\ALT & t \times t \ALT \goosekw{ptrT} \ALT \dots \\
    \textdom{StructDecl} & s &\in& \textlog{list}(\textlog{string} \times
                                   \textlog{GooseType}) \\
    \multicolumn{2}{r}{\structType([])} &\defeq& \goosekw{unitT} \\
    \multicolumn{2}{r}{\structType((f, t) :: s)} &\defeq& t \times \structType(s)
  \end{array}
\end{mathpar}
\caption{GooseLang types and struct declarations. These are used in the
  semantics only to give a ``shape'' to data for accessing struct fields, and
  not to represent the Go type system.}%
\label{fig:goose:types}
\end{figure}

First we need to handle struct values. We treat a struct value as just a tuple
of its fields. The definition of the struct gives an ordering of the fields.
This is enough to construct a struct from its fields and to access a field by
name. Here the shallow embedding comes in: we define struct declarations in
Gallina, and struct construction and field access are Gallina definitions that
produce GooseLang expressions, rather than all of this being directly
implemented in GooseLang.

Structs in memory are more interesting than struct values. Structs could
be stored in a single location; due to our non-atomic semantics for
memory, this would be sound even for structs larger than a machine word.
However, this model would be too restrictive: it is safe for threads to
concurrently access \emph{different fields}, just not the same field,
and we actually take advantage of this property (largely to write more
natural Go code; working around this restriction requires splitting
structs up if they are stored in memory).

To support this concurrency, we model a struct in memory with a
flattened representation, with each base element in a separate memory
cell. The flattening applies recursively to fields that are themselves
structs, until a base literal is reached (like an integer or boolean);
base elements are at most a machine word, but can be smaller. When
allocating a new pointer, the semantics flattens composite values and
stores the elements in a sequence of contiguous addresses.

With a flattened representation we need non-trivial code to read a struct
through a pointer, particularly when some of its fields are themselves flattened
structs. Any load of a value from memory is translated to a call to
$\LoadTyped$. This is a Gallina definition of type
$\textdom{GooseType} \to \textdom{Expr}$, where that expression is itself a
GooseLang function that takes a pointer. Hence you can think of it is a macro
for GooseLang that is represented in Gallina as a meta language. $\LoadTyped(t)$
is directed by a type $t$ passed in Gallina to determine how to load and
assemble the fields of a struct of type $t$.

For the purpose of proofs we represent a pointer to an arbitrary type
$t$ with a typed points-to fact of the form $\ell \mapsto_t v$. This
definition expands to a number of primitive points-to facts, one for
each base element. The specification for loading says
$\hoare{\ell \mapsto_t v}{\LoadTyped(t) \app \ell}{\Ret{v} \ell \mapsto_t v}$, which
(much like the primitive load $!\ell$) hides the fact that something
non-atomic is happening and looks like an ordinary dereference.
Similarly, $\StoreTyped$ also takes a type, although the specification
requires the caller to prove that the value has the right shape (in
reality it always will because the Go code we translate from is
well-typed).

The payoff of structs being many independent locations is that it is
possible to model references to individual struct fields. From a pointer
to the root of the struct, a field pointer is simply an offset from that
pointer (skipping the flattened representations of the previous fields).
This offset calculation is much like the code to read a struct from
memory, except that it merely computes a single offset rather than
iterating over all the fields and offsets.

The Go language reference specifies that each field acts like an
independent ``variable'' (which is stored in the GooseLang heap when it
is mutable in Go), so this model should accurately reflect the
specification. Moreover modeling structs as independent locations is
also justified as being similar to how the implementation works. Structs
in memory are in reality represented by contiguous memory, and field
access is implemented by computing a pointer from the base of the
struct. The main difference between the physical implementation and the
model is that we use a single, abstract memory location for each field,
whereas the implementation encodes all data into bytes.

Recall that $\ell \mapsto_t v$ is internally composed of untyped
points-to facts for all the base elements of $v$. In order to reason
about $v$'s fields, we introduce a new struct field points-to fact,
written $\ell \mapsto_{s.f} v$, which asserts ownership of just field
$f$ of a struct with descriptor $s$ rooted at $\ell$, and gives that field's
value as $v$. A recursive function gives an ``exploded'' set of struct
fields by iterating over $t$'s fields and $v$ simultaneously. Then,
we give a proof that $\ell \mapsto_{\structType(s)} v$ is equivalent to the separating
conjunction of this exploded list. The result is a convenient lemma for
reasoning about a struct using its fields: in the forward direction, the
equivalence breaks a large typed points-to into individual fields (with
the values computed from $v$), while in the other direction it allows
to prove a $\ell \mapsto_{\structType(s)} v$ by gathering up all the fields.

The struct field points-to is indispensable in proofs, because the
pattern of \cc{x.f} in Go when $x$ is a pointer is in fact a field
load (in C, this would be written \cc{x->f}). The model
for loading a struct field is a function $\loadField(s, f) \app x$
which is implemented in two steps, first computing the offset to field
$f$ and then dereferencing the computed pointer (in both cases the struct descriptor $s$
describes how to interpret field $f$). Having a field points-to gives
a natural specification for this type of load:
$\hoare{\ell \mapsto_{s.f} v}{\loadField(t, f) \app \ell}{\Ret{v} \ell \mapsto_{s.f} v}$.

The lemmas about breaking apart and recombining structs are all proven
against a simpler model of structs that only requires flattening and
offset calculations. In a sense the model is the trusted code, but the
fact that the struct maps-to exploding lemma is true that all of the
expected Hoare triples hold provides strong evidence that the model is
also doing the right thing. For example, the exploding lemma shows that
field offsets are disjoint, since the struct maps-to can be broken into
field points-to facts for each field.

\subsection{Slices}

One of the most commonly used data structures in Go is the slice
\cc{[]T}, which is a dynamically-sized array of values of type
\cc{T}. Goose supports a wide range of operations on slices,
including appending and sub-slicing; it turns out that the semantics of
mutable slices is non-trivial in Go, resulting in an interesting
semantics and reasoning principles.

A slice is a combination of a pointer, length, and capacity. Slices are
views into a contiguous memory allocation; this view can be narrowed
with sub-slicing operations of the form \cc{s[i:j]}, resulting
in aliased slices. The elements between the length and capacity are not
directly accessible but are used to support efficient amortized appends.
Go's built-in slice operations include bounds checks on all slice
operations and panic if a memory access or sub-slice operation goes out
of bounds.

GooseLang has a primitive for contiguous memory, which we use to model
the allocation underlying a slice (though these are not directly
accessible to Go code, since they do not carry enough information for
bounds checking). On top of these we model slices as a tuple of a base
pointer, length, and capacity.

The GooseLang slice model is directly inspired by the implementation of
slices, including modeling slice capacity. Initially we had a more
abstract model that ignored capacity (which would appear to be just an
optimization), but were surprised to find that this was insufficient to
even accurately model subslicing and appending. Directly modeling slice
capacity was the simplest solution to obtain a model that is faithful to
the Go implementation. The Go language reference isn't specific about
what the slice capacity after various operations should be, so our
GooseLang model picks a non-deterministic capacity in several places
(within appropriate bounds).

\newidentmacro{ptr}
\newidentmacro{len}
\newidentmacro{cap}

The most basic operations on slices are indexing and storing. The
GooseLang model of $s$ is a three-tuple, but for clarity we will refer
to its elements as $\ptr(s)$, $\len(s)$, and $\cap(s)$. The
translation of \cc{s[i]} is essentially a load from
$\ptr(s) + i$ (or undefined behavior if this offset is out-of-bounds).
Similarly \cc{s[i] = x} stores to the same location. We
translate Go's \cc{len(s)} directly to $\len(s)$. Go also supports
accessing a slice's capacity, but this is rarely used and Goose does not
support it.

The Go \cc{append} operation is the most sophisticated to model. The
behavior of \cc{append(s, x)} where \cc{s: []T} and
\cc{x: T} depends on whether there is extra capacity to store the
new element \cc{x}. If there is capacity, then \cc{x} is stored
there and the append returns a new slice with the same pointer but a
larger length. If there is no capacity, then \cc{append} must
allocate a new slice, copy the existing elements to it, and then store
\cc{x}. In the latter case \cc{append} returns a slice with a
fresh pointer.

The difficulty with Go slices arise when supporting subslicing. Consider
\cc{s[:i]}, where \cc{i} is less than \cc{len(s)}.
Clearly this slice should have the same pointer and length \cc{i},
but what should its capacity be? Surprisingly, the capacity of this
prefix is the full capacity of \cc{s}, which means that the unused
elements of \cc{s[:i]} \emph{include the elements of \cc{s}}
beyond the index \cc{i}. As a result, \cc{append(s[:i], x)}
in fact modifies \cc{s[i]}. GooseLang takes care to model this
behavior by implementing \cc{append} exactly as above, taking into
account that \cc{append(s, x)} might be an in-place operation.

The GooseLang model is specifically designed to be sound by sticking to
the Go implementation as closely as possible, but we want reasoning
about slices to be convenient and high-level, without worrying about
slice capacity directly. The design of GooseLang nicely separates the
model from the reasoning principles --- we verify specifications against
the concrete model, so that only the model is trusted and not the
separation logic specifications.

\newcommand{\sliceRep}{\mathtt{sliceRep}}
\newcommand{\sliceCap}{\mathtt{sliceCap}}

\newcommand{\lappend}{\mdoubleplus}

The GooseLang model of slices is based on two abstract predicates:
$\sliceRep(s, l)$ and $\sliceCap(s)$. To model the slice values
themselves we use $s : \textlog{Slice}$ where $\textlog{Slice}$ is a Gallina record; a
function $\textlog{SliceVal}(s) : Val$ converts the Gallina representation to
the GooseLang tuple that the slice model uses. We will only present the
\emph{untyped} version of this specification where $l : \textlog{list}(\textdom{Val})$, but
GooseLang also has a typed version where $l : \textlog{list}(T)$ where there is
some (Gallina) function $\mathtt{to\_val} : T \to \textdom{Val}$. The typed version is
practically convenient in proofs but is only a small extension to the
untyped version.

The first predicate $\sliceRep(s, l)$ gives the abstract value of
$s$, the list of values it contains, excluding additional capacity. It
also represents ownership over all these elements, in terms of the
underlying struct points-to facts. We use this predicate to specify
loads and stores:

\[
  \hoareV{\sliceRep(s, l) * i < |l|}%
{\mathtt{s[i]}}%
{\Ret{v} v = l !! i * \sliceRep(s, l)}
\]
\[
  \hoareV{\sliceRep(s, l) * i < |l|}%
 {\mathtt{s[i] = v}}%
{\Ret{v} v = l !! i * \sliceRep(s, l[i := v])}
\]

Next, $\sliceCap(s)$ is an abstract predicate that represents
\emph{ownership over the capacity} of $s$. It is necessary to append,
since appending might need to write to the capacity, but unneeded to
read and write to a slice.
\[
\hoareV{\sliceRep(s, l) * \sliceCap(s)}%
{\mathtt{append(s, x)}}%
{\Ret{s'} \sliceRep(s', l \lappend [x]) * \sliceCap(s')}
\]

\newidentmacro{sliceFull}

This specification is fairly simple. In fact, we often use a shorthand
$\sliceFull(s, l) = \sliceRep(s, l) * \sliceCap(s)$ when the proof will
always retain ownership of slice capacity, in which case the spec looks
even simpler. However, the proof is non-trivial, since in one case it
moves ownership from $\sliceCap(s)$ to $\sliceRep(s', l \lappend [x])$
(where $ptr(s') = ptr(s)$), while in the other it constructs a
completely new allocation for $s'$.

\newidentmacro{sliceTake}
\newidentmacro{sliceDrop}

The most interesting rules are for subslicing and how they interact with
capacity. Consider \cc{s[:i]} again. While Go has no formal
notion of ownership, our specifications do. We can model the
\emph{value} for \cc{s[:i]} easily enough; call it
$\sliceTake(s, i)$ (it simply reduces the length and keeps the capacity
of $s$, as specified by Go). Now we need to decide how ownership of
$\sliceRep(s, l) * \sliceCap(s)$ should relate to ownership of
$\sliceRep(sliceTake(s, i), take(l, i))$. It turns out there are two
possibilities: we can either give up ownership of the remainder of $s$
in exchange for $\cap(\sliceTake(s, i))$, or we can ignore the
capacity of the subslice and keep
$\sliceRep(\sliceDrop(s, i), drop(l, i))$. These are incomparable and
unexpressed in the code: the decision is based on whether we intend to
append to the subslice but stop using the old slice, or whether we want
to continue using the remainder of \cc{s}.

\tej{why not just use \cc{s[:i]} for $\sliceTake(s, i)$ and $l[:i]$ for
$take(l, i)$? overloading will make everything much easier to read}

Concretely, GooseLang verifies the following entailment for reasoning
about subslicing in terms of the slice model:

$\sliceFull(s, l) \vdash \sliceFull(\sliceTake(s, i), take(l, i))$

This entailment precisely captures how retaining ownership of the
capacity of $\sliceTake(s, i)$ requires giving up the remainder of
$s$.

\begin{align*}
  &\sliceRep(s, l) \dashv\vdash \\
  &\quad \sliceRep(\sliceTake(s, i), take(l, i)) \sep {} \\
  &\quad \sliceRep(\sliceDrop(s, i), drop(l, i))
\end{align*}

This alternative bidirectional entailment splits $s$ into two parts,
but gives up ownership over $\sliceTake(s, i)$'s capacity in exchange
for using those elements in \cc{s[:i]}. From this point it will
not be possible to prove the safety of appending to \cc{s[:i]},
since this would conflict with the separate ownership over
\cc{s[i:]}.

\subsection{Maps}

\newidentmacro{mapVal}
\newidentmacro{mapRep}
\newidentmacro{mapDelete}
\newidentmacro{mapInsert}
\newidentmacro{mapIter}

After slices, maps are the next most commonly used collection type in
Go. We implement maps as lists of key-value pairs, stored in a single
memory location in reverse insertion order. Go's builtin maps are
\emph{not} thread-safe, so the model enforces single-threaded access by
marking the map as being read while reading from it; this re-uses the
race detection for other pointers to ensure that racy access to a map is
undefined behavior, while allowing concurrent read-read access. Maps
support all the Go operations: insertions, reads (including returning
whether the key is present), \cc{len} to get the number of elements
in the map, deletion, and iteration. Go map iteration is
non-deterministic and in practice random, but we did not model this
since it would be challenging to do so; however, the reasoning
principles for map iteration do not expose an iteration order.

The implementation of maps is the most involved out of any of the Go
primitives. It required directly implementing maps (albeit
inefficiently, using an association list) using recursive GooseLang
code. GooseLang is an untyped language, so our first attempts had basic
errors like missing arguments. We improved our confidence in this
implementation both by testing it and by verifying it. Both of these
essentially rule out type errors (regardless of what specification we give),
and the specification is simple enough
to be a reliable test of behavior. Both simple tests and verification
cover easy mistakes like reading the oldest write to a key rather than
the latest, or duplicate keys during iteration (the implementation must
skip over a key after observing it once).

The proof and specification for maps is relatively easy since they are
not safe to use concurrently, so the proof assumes ownership over the entire map. We
treat a map as a pointer to an abstract map value, a GooseLang value
that encodes the entire map data as a list of key-value pairs. The
specification is based on a pure relation $\mapVal(v, m)$ that relates
this encoded value to a Gallina map $m$, which uses \cc{gmap} from
stdpp; for simplicity we use \cc{gmap u64 val} and limit map
keys to integers. Values are not a visible notion to the Go code, since
it always interacts with maps via their pointer, so the specifications
all use $\mapRep(\ell, m) = \exists v.\, \ell \mapsto v * \mapVal(v, m)$. The
indirection is important, since the Go map value
\cc{m : map[uint64]V} is in fact a reference to a map that is
mutated in-place (unlike a slice, which has both pure data --- pointer,
length, and capacity --- and heap data).

For example, this is the specification for map deletion:
\[
\hoare{\mapRep(l, m)}{\mapDelete(l, k)}{\mapRep(l, delete(m, k))}
\]

Map iteration has a more sophisticated specification. Suppose we have a generic loop
over a map in Go like the following:

\begin{verbatim}
for k, v := range m {
  body(k, v)
}
\end{verbatim}

The model for this entire construct is given by $\cc{MapIter}(m, body)$, where
$m$ is a reference to the map and $body$ is an expression for the body of the
loop. Goose translates generic loop bodies, so the Go code does not literally
need to consist of a call to a separate function. The possibility of
\emph{iterator invalidation} adds one subtlety to Go's map iteration --- it
would be incorrect for the body of the loop to modify the map (it might be sound
to write to the map without modifying the domain, but we do not attempt to model
this). The model of maps puts the entire contents of a map in one heap location,
so naively we would not enforce this property. The solution is to mark the map's
reference as being read for the entire duration of iteration, using the
$\goosekw{StartRead}$ and $\goosekw{FinishRead}$ GooseLang primitives at the
beginning and end of $\cc{MapIter}$. If the map value in the heap is
$\goosekw{Reading} \app n \app v$, these operations increment and decrement
(respectively) the reader count $n$, so that any writes within the body have
undefined behavior.

Iteration gets a \emph{higher-order} specification that assumes a specification
for the body, showing it preserves a loop invariant $P$ over the part of the map
consumed so far:

\[
  \infer{
    \forall m_{0}, k, v.\,
    k \notin m_{0} \land m[k] = v \to \\\\
    \hoare{P(m_0)}{body \app k \app v)}{P( m_{0}\mapupd{k}{v} )}
}
{
  \hoare{\mapRep(\ell, m) \sep P(\emptyset)}%
{\cc{MapIter}(\ell, body)}%
{\mapRep(\ell, m) \sep P(m)}
}
\]

On top of this generic specification we prove some alternate specifications that
express the invariant in slightly different ways --- for example, it is often
useful to express the invariant in terms of both the map iterated over so far
and the remaining subset of the map.

Map iteration in Go happens in a non-deterministic order\footnote{In fact the
runtime randomizes the starting position of the iteration, to avoid callers
accidentally relying on any particular behavior. See
\href{https://github.com/golang/go/blob/c379c3d58d5482f4c8fe97466a99ce70e630ad44/src/runtime/map.go\#L844-L850}%
{\cc{mapiterinit} from src/runtime/map.go}.}
Thus strictly speaking $\cc{MapIter}$ should shuffle the elements of the map
before iterating over it, in order to model the non-determinism of the
implementation. We do not (currently) do this, simply because the shuffle would
be hard to implement in GooseLang. However, the specification for map iteration
does not expose an iteration order and would apply unchanged to this more
precise model. All proofs go through this specification, so our proofs should
remain unchanged if $\cc{MapIter}$ started modeling non-determinitic ordering.
