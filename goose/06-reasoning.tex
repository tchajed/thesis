\section{Modeling and reasoning about Go}%
\label{sec:goose:reasoning}

\subsection{Modeling pointers}%
\label{sec:goose:pointers}

Pointers turn out to be slightly subtle because of concurrency. In
short, GooseLang disallows concurrent reads and writes to the same
location by making such racy access undefined behavior (any
specification for a program implies that if the precondition holds, the
program never exhibits undefined behavior). The hardware provides some
guarantees, but they are relatively weak: for example, different cores
can observe writes at different times. Go's own memory model specifies
even weaker guarantees. Rather than attempt to formalize Go's rules
(which are complex and involve defining a partial order over all program
instructions), we side step the issue and make any races undefined,
which works for our intended use cases since we always synchronize
concurrent access with locks.

To disallow concurrent reads and writes we first detect them. The key is to make
$x \gets v$, the primitive store operation, \emph{non-atomic} by splitting it
into two operations. The GooseLang semantics tracks the behavior of these
operations by augmenting the heap with extra information; each address in the
semantics has a $\textdom{NonAtom}$ which can be
$\goosekw{Reading} \app n \app v$ if there are $n$ readers and the value is $v$,
or $\goosekw{Writing} \app v$ if a thread is currently writing. We actually only
use non-atomic stores for normal pointers, but we have support for non-atomic
reads as well for iterating over maps.

Ordinarily values in the heap are of the form $\goosekw{Reading} \app 0 \app v$,
to indicate no readers or writers. Writes are split into
$\goosekw{PrepareWrite}(\ell)$, which sets the value of $\ell$ to
$\goosekw{Writing} \app v_{0}$, and $\goosekw{FinishStore}(\ell, v)$ which sets
it to $\goosekw{Reading} \app 0 \app v$. A concurrent write will be undefined
since $\goosekw{PrepareWrite}(\ell)$ requires no concurrent writers, and
similarly for a concurrent read which is undefined if the address is being
written. Non-atomic reads are similar with $\goosekw{StartRead}$ and
$\goosekw{FinishRead}$; these increment and decrement the number of readers,
respectively, so that multiple readers can run concurrently but any concurrent
writer has undefined behavior.

Next, we need reasoning principles to abstract away this complexity from
program verification. Separation logic turns out to provide the right
language to reason about racy access. When a thread owns
$l \mapsto v$, we know no other thread can have access to location
$l$, so the specifications for reads and writes are unaffected by the
operations being non-atomic (although their proofs are a bit more
complicated to deal with the new semantics). The only change is that the
Read operation is no longer an atomic primitive but a function that
takes two execution steps. In Iris this means that two threads cannot
share memory with an invariant and must mediate access with a lock,
which transfers ownership of the $l \mapsto v$ for multiple execution
steps.

\subsection{Locks}

\newcommand{\Acquire}{\goosedef{Acquire}}
\newcommand{\CAS}{\goosedef{CAS}}

As is typical in Goose, locks are not built-in to GooseLang but modeled
using an implementation based on simpler primitives. Since locks are the
only synchronization primitive, implementing them requires shared
concurrent access, which ordinary pointers do not have in GooseLang.
Instead, the language also includes a primitive atomic compare-and-swap
operation that is only used to implement a model of locks. We could also
use the same operation to model Go's low-level atomic operations, like
\cc{atomic.CompareAndSwapUint64} and \cc{atomic.LoadUint64}, but
have not implemented this yet since we don't have code that uses these
low-level synchronization primitives.

The model of locks is simple enough to give the code in its entirety. The lock
is represented as a pointer to a boolean that is true if the lock is held. As a
helper we define $\CAS$ (compare-and-swap), a variant of compare-and-exchange
that only returns a boolean on success and not also the previous value.

\begin{align*}
  \CAS &\defeq \gooselambda{x, v1, v2} \pi_{2}\app \goosekw{CmpXchg}(x, v1, v2) \\
  \goosedef{NewLock} & \defeq \gooselambda{\_} \goosekw{ref} \app \goosekw{false} \\
  \Acquire &\defeq \gooselambda{l} \\
       &\goosekw{let}\app f = (\goosekw{rec}\: tryAcquire(\_) = \\
       &\qquad \gooseif{\CAS \app l \app \goosekw{true} \app \goosekw{false}}%
         {()}{tryAcquire \app ()}) \app\goosekw{in} \\
       &f \app () \\
  \goosedef{Release} &\defeq \gooselambda{l} l \gets \goosekw{false} \\
\end{align*}

Acquiring a lock is modeled as repeatedly using
$\CAS \app l \app \goosekw{false} \app \goosekw{true}$ to
atomically test that the lock is false and set it to true if so, while release
stores false to the lock. This implementation as a spin lock is merely an
operational model that captures what the lock does: acquire blocks until the
lock is free and sets it to locked, while release frees the lock. This code is
used to model Go's builtin \cc{*sync.Mutex}, which is implemented more
efficiently than spin locks with cooperation from the runtime and operating
system.

The reasoning principles for locks are more sophisticated than the spin-lock
implementation. As is typical in concurrent separation logic, we associate a
\emph{lock invariant} to the lock, which is a predicate that holds when the lock
is free. Because this is a separation logic, we can also interpret the lock
invariant as ownership over some data (for example, some region of memory); the
lock mediates access to this ownership, handing it out when the lock is acquired
and requiring it back when the lock is released. We prove this specification
sound against the GooseLang spin-lock implementation.

\subsection{Structs}

\newidentmacro{LoadTyped}
\newidentmacro{StoreTyped}
\newidentmacro{loadField}
\newidentmacro{storeField}

One of the first features needed when writing any Go is support for
structs. We treat a struct value as just a tuple of its fields, while
the struct definition gives the names of those fields. This data is
enough to implement constructing a struct from its fields and accessing
a field by name, which we implement in Gallina.

Structs in memory are more interesting than struct values. Structs could
be stored in a single location; due to our non-atomic semantics for
memory, this would be sound even for structs larger than a machine word.
However, this model would be too restrictive: it is safe for threads to
concurrently access \emph{different fields}, just not the same field,
and we actually take advantage of this property (largely to write more
natural Go code; working around this restriction requires splitting
structs up if they are stored in memory).

To support this concurrency, we model a struct in memory with a
flattened representation, with each base element in a separate memory
cell. The flattening applies recursively to fields that are themselves
structs, until a base literal is reached (like an integer or boolean);
base elements are at most a machine word, but can be smaller. When
allocating a new pointer, the semantics flattens composite values and
stores the elements in a sequence of contiguous addresses.

With a flattened representation we need non-trivial code to read a
struct through a pointer, particularly when some of its fields are
themselves flattened structs. We implemented this code by augmenting the
``schema'' that represents a struct type with not only the fields, but
their types as well. The exact types are not important, but we do need
the entire tree of how big each field is and the shape of each field in
order to determine the location and extent of any given field. Using
types to represent these shapes makes the translation much simpler,
since we have access to the type of every sub-expression from the Go
type checker. Any load of a value from memory is translated to a Gallina
$\LoadTyped$ macro that takes a Coq representation of the type being loaded
and uses it to determine what offsets to load.

For the purpose of proofs we represent a pointer to an arbitrary type
$t$ with a typed points-to fact of the form $l \mapsto_t v$. This
definition expands to a number of primitive points-to facts, one for
each base element. The specification for loading says
$\hoare{l \mapsto_t v}{\LoadTyped(t, l)}{\Ret{v} l \mapsto_t v}$, which
(much like the non-atomic primitive Load) hides the fact that something
non-atomic is happening and looks like an ordinary dereference.
Similarly, StoreTyped also takes a type, although the specification
requires the caller to prove that the value has the right shape (in
reality it always will because the Go code we translate from is
well-typed).

The payoff of structs being many independent locations is that it is
possible to model references to individual struct fields. From a pointer
to the root of the struct, a field pointer is simply an offset from that
pointer (skipping the flattened representations of the previous fields).
This offset calculation is much like the code to read a struct from
memory, except that it merely computes a single offset rather than
iterating over all the fields and offsets.

The Go language reference specifies that each field acts like an
independent ``variable'' (which is stored in the GooseLang heap when it
is mutable in Go), so this model should accurately reflect the
specification. Moreover modeling structs as independent locations is
also justified as being similar to how the implementation works. Structs
in memory are in reality represented by contiguous memory, and field
access is implemented by computing a pointer from the base of the
struct. The main difference between the physical implementation and the
model is that we use a single, abstract memory location for each field,
whereas the implementation encodes all data into bytes.

Recall that $l \mapsto_t v$ is internally composed of untyped
points-to facts for all the base elements of $v$. In order to reason
about $v$'s fields, we introduce a new struct field points-to fact,
written $l \mapsto_{t.f} v$, which asserts ownership of just field
$f$ of a struct of type $t$ rooted at $l$, and gives that field's
value as $v$. A recursive function gives an ``exploded'' set of struct
fields by iterating over $t$'s fields and $v$ simultaneously. Then,
we give a proof that $l \mapsto_t v$ is equivalent to the separating
conjunction of this exploded list. The result is a convenient lemma for
reasoning about a struct using its fields: in the forward direction, the
equivalence breaks a large typed points-to into individual fields (with
the values computed from $v$), while in the other direction it allows
to prove a $l \mapsto_t v$ by gathering up all the fields.

The struct field points-to is indispensable in proofs, because the
pattern of \cc{x.f} in Go when $x$ is a pointer is in fact a field
load (in C, this would be written \cc{x->f}). The model
for loading a struct field is a function $\loadField(x, t, f)$
which is implemented in two steps, first computing the offset to field
$f$ and then dereferencing the computed pointer (in both cases the struct type $t$
describes how to interpret field $f$). Having a field points-to gives
a natural specification for this type of load:
$\hoare{l \mapsto_{t.f} v}{\loadField(x, t, f)}{\Ret{v} l \mapsto_{t.f} v}$.

The lemmas about breaking apart and recombining structs are all proven
against a simpler model of structs that only requires flattening and
offset calculations. In a sense the model is the trusted code, but the
fact that the struct maps-to exploding lemma is true that all of the
expected Hoare triples hold provides strong evidence that the model is
also doing the right thing. For example, the exploding lemma shows that
field offsets are disjoint, since the struct maps-to can be broken into
field points-to facts for each field.

Something to emphasize above: all of the struct code is generic for
struct type $t$, which in the code is concretely the ``schema''
described above, a list of fields and types (the code calls this a
``descriptor'' and uses $d$ as the metavariable, to avoid confusion
with a generic type $t$).

\subsection{Slices}

One of the most commonly used data structures in Go is the slice
\cc{[]T}, which is a dynamically-sized array of values of type
\cc{T}. Goose supports a wide range of operations on slices,
including appending and sub-slicing; it turns out that the semantics of
mutable slices is non-trivial in Go, resulting in an interesting
semantics and reasoning principles.

A slice is a combination of a pointer, length, and capacity. Slices are
views into a contiguous memory allocation; this view can be narrowed
with sub-slicing operations of the form \cc{s[i:j]}, resulting
in aliased slices. The elements between the length and capacity are not
directly accessible but are used to support efficient amortized appends.
Go's built-in slice operations include bounds checks on all slice
operations and panic if a memory access or sub-slice operation goes out
of bounds.

GooseLang has a primitive for contiguous memory, which we use to model
the allocation underlying a slice (though these are not directly
accessible to Go code, since they do not carry enough information for
bounds checking). On top of these we model slices as a tuple of a base
pointer, length, and capacity.

The GooseLang slice model is directly inspired by the implementation of
slices, including modeling slice capacity. Initially we had a more
abstract model that ignored capacity (which would appear to be just an
optimization), but were surprised to find that this was insufficient to
even accurately model subslicing and appending. Directly modeling slice
capacity was the simplest solution to obtain a model that is faithful to
the Go implementation. The Go language reference isn't specific about
what the slice capacity after various operations should be, so our
GooseLang model picks a non-deterministic capacity in several places
(within appropriate bounds).

\newidentmacro{ptr}
\newidentmacro{len}
\newidentmacro{cap}

The most basic operations on slices are indexing and storing. The
GooseLang model of $s$ is a three-tuple, but for clarity we will refer
to its elements as $\ptr(s)$, $\len(s)$, and $\cap(s)$. The
translation of \cc{s[i]} is essentially a load from
$\ptr(s) + i$ (or undefined behavior if this offset is out-of-bounds).
Similarly \cc{s[i] = x} stores to the same location. We
translate Go's \cc{len(s)} directly to $\len(s)$. Go also supports
accessing a slice's capacity, but this is rarely used and Goose does not
support it.

The Go \cc{append} operation is the most sophisticated to model. The
behavior of \cc{append(s, x)} where \cc{s: []T} and
\cc{x: T} depends on whether there is extra capacity to store the
new element \cc{x}. If there is capacity, then \cc{x} is stored
there and the append returns a new slice with the same pointer but a
larger length. If there is no capacity, then \cc{append} must
allocate a new slice, copy the existing elements to it, and then store
\cc{x}. In the latter case \cc{append} returns a slice with a
fresh pointer.

The difficulty with Go slices arise when supporting subslicing. Consider
\cc{s[:i]}, where \cc{i} is less than \cc{len(s)}.
Clearly this slice should have the same pointer and length \cc{i},
but what should its capacity be? Surprisingly, the capacity of this
prefix is the full capacity of \cc{s}, which means that the unused
elements of \cc{s[:i]} \emph{include the elements of \cc{s}}
beyond the index \cc{i}. As a result, \cc{append(s[:i], x)}
in fact modifies \cc{s[i]}. GooseLang takes care to model this
behavior by implementing \cc{append} exactly as above, taking into
account that \cc{append(s, x)} might be an in-place operation.

The GooseLang model is specifically designed to be sound by sticking to
the Go implementation as closely as possible, but we want reasoning
about slices to be convenient and high-level, without worrying about
slice capacity directly. The design of GooseLang nicely separates the
model from the reasoning principles --- we verify specifications against
the concrete model, so that only the model is trusted and not the
separation logic specifications.

\newcommand{\sliceRep}{\mathtt{sliceRep}}
\newcommand{\sliceCap}{\mathtt{sliceCap}}

The GooseLang model of slices is based on two abstract predicates:
$\sliceRep(s, l)$ and $\sliceCap(s)$. To model the slice values
themselves we use $s : Slice$ where $Slice$ is a Gallina record; a
function $SliceVal(s) : Val$ converts the Gallina representation to
the GooseLang tuple that the slice model uses. We will only present the
\emph{untyped} version of this specification where $l : list val$, but
GooseLang also has a typed version where $l : list T$ where there is
some (Gallina) function $\mathtt{to\_val} : T -> Val$. The typed version is
practically convenient in proofs but is only a small extension to the
untyped version.

The first predicate $\sliceRep(s, l)$ gives the abstract value of
$s$, the list of values it contains, excluding additional capacity. It
also represents ownership over all these elements, in terms of the
underlying struct points-to facts. We use this predicate to specify
loads and stores:

\[
  \hoareV{\sliceRep(s, l) * i < |l|}%
{\mathtt{s[i]}}%
{\Ret{v} v = l !! i * \sliceRep(s, l)}
\]
\[
  \hoareV{\sliceRep(s, l) * i < |l|}%
 {\mathtt{s[i] = v}}%
{\Ret{v} v = l !! i * \sliceRep(s, l[i := v])}
\]

Next, $\sliceCap(s)$ is an abstract predicate that represents
\emph{ownership over the capacity} of $s$. It is necessary to append,
since appending might need to write to the capacity, but unneeded to
read and write to a slice.
\[
\hoareV{\sliceRep(s, l) * \sliceCap(s)}%
{\mathtt{append(s, x)}}%
{\Ret{s'} \sliceRep(s', l ++ [x]) * \sliceCap(s')}
\]

\newidentmacro{sliceFull}

This specification is fairly simple. In fact, we often use a shorthand
$\sliceFull(s, l) = \sliceRep(s, l) * \sliceCap(s)$ when the proof will
always retain ownership of slice capacity, in which case the spec looks
even simpler. However, the proof is non-trivial, since in one case it
moves ownership from $\sliceCap(s)$ to $\sliceRep(s', l ++ [x])$
(where $ptr(s') = ptr(s)$), while in the other it constructs a
completely new allocation for $s'$.

\newidentmacro{sliceTake}
\newidentmacro{sliceDrop}

The most interesting rules are for subslicing and how they interact with
capacity. Consider \cc{s[:i]} again. While Go has no formal
notion of ownership, our specifications do. We can model the
\emph{value} for \cc{s[:i]} easily enough; call it
$\sliceTake(s, i)$ (it simply reduces the length and keeps the capacity
of $s$, as specified by Go). Now we need to decide how ownership of
$\sliceRep(s, l) * \sliceCap(s)$ should relate to ownership of
$\sliceRep(sliceTake(s, i), take(l, i))$. It turns out there are two
possibilities: we can either give up ownership of the remainder of $s$
in exchange for $\cap(\sliceTake(s, i))$, or we can ignore the
capacity of the subslice and keep
$\sliceRep(\sliceDrop(s, i), drop(l, i))$. These are incomparable and
unexpressed in the code: the decision is based on whether we intend to
append to the subslice but stop using the old slice, or whether we want
to continue using the remainder of \cc{s}.

Concretely, GooseLang verifies the following entailment for reasoning
about subslicing in terms of the slice model:

$\sliceFull(s, l) \vdash \sliceFull(\sliceTake(s, i), take(l, i))$

This entailment precisely captures how retaining ownership of the
capacity of $\sliceTake(s, i)$ requires giving up the remainder of
$s$.

\begin{align*}
  &\sliceRep(s, l) \dashv\vdash \\
  &\quad \sliceRep(\sliceTake(s, i), take(l, i)) \sep {} \\
  &\quad \sliceRep(\sliceDrop(s, i), drop(l, i))
\end{align*}

This alternative bidirectional entailment splits $s$ into two parts,
but gives up ownership over $\sliceTake(s, i)$'s capacity in exchange
for using those elements in \cc{s[:i]}. From this point it will
not be possible to prove the safety of appending to \cc{s[:i]},
since this would conflict with the separate ownership over
\cc{s[i:]}.

\subsection{Maps}

\newidentmacro{mapVal}
\newidentmacro{mapRep}
\newidentmacro{mapDelete}
\newidentmacro{mapInsert}
\newidentmacro{mapIter}

After slices, maps are the next most commonly used collection type in
Go. We implement maps as lists of key-value pairs, stored in a single
memory location in reverse insertion order. Go's builtin maps are
\emph{not} thread-safe, so the model enforces single-threaded access by
marking the map as being read while reading from it; this re-uses the
race detection for other pointers to ensure that racy access to a map is
undefined behavior, while allowing concurrent read-read access. Maps
support all the Go operations: insertions, reads (including returning
whether the key is present), \cc{len} to get the number of elements
in the map, deletion, and iteration. Go map iteration is
non-deterministic and in practice random, but we did not model this
since it would be challenging to do so; however, the reasoning
principles for map iteration do not expose an iteration order.

The implementation of maps is the most involved out of any of the Go
primitives. It required directly implementing maps (albeit
inefficiently, using an association list) using recursive GooseLang
code. GooseLang is an untyped language, so our first attempts had basic
errors like missing arguments. We improved our confidence in this
implementation both by testing it and by verifying it. Both of these
essentially rule out type errors (regardless of what specification we give),
and the specification is simple enough
to be a reliable test of behavior. Both simple tests and verification
cover easy mistakes like reading the oldest write to a key rather than
the latest, or duplicate keys during iteration (the implementation must
skip over a key after observing it once).

The proof and specification for maps is relatively easy since they are
not safe to use concurrently, so the proof assumes ownership over the entire map. We
treat a map as a pointer to an abstract map value, a GooseLang value
that encodes the entire map data as a list of key-value pairs. The
specification is based on a pure relation $\mapVal(v, m)$ that relates
this encoded value to a Gallina map $m$, which uses \cc{gmap} from
stdpp; for simplicity we use \cc{gmap u64 val} and limit map
keys to integers. Values are not a visible notion to the Go code, since
it always interacts with maps via their pointer, so the specifications
all use $\mapRep(\ell, m) = \exists v.\, \ell \mapsto v * \mapVal(v, m)$. The
indirection is important, since the Go map value
\cc{m : map[uint64]V} is in fact a reference to a map that is
mutated in-place (unlike a slice, which has both pure data --- pointer,
length, and capacity --- and heap data).

For example, this is the specification for map deletion:
\[
\hoare{\mapRep(l, m)}{\mapDelete(l, k)}{\mapRep(l, delete(m, k))}
\]

Map iteration has a more sophisticated specification. Suppose we have a generic loop
over a map in Go like the following:

\begin{verbatim}
for k, v := range m {
  body(k, v)
}
\end{verbatim}

The model for this entire construct is given by $\cc{MapIter}(m, body)$, where
$m$ is a reference to the map and $body$ is an expression for the body of the
loop. Goose translates generic loop bodies, so the Go code does not literally
need to consist of a call to a separate function. Iteration gets a
\emph{higher-order} specification that assumes a specification for the body,
showing it preserves a loop invariant $P$ over the part of the map consumed so
far:

\[
  \infer{
    \forall m_{0}, k, v.\,
    k \notin m_{0} \land m[k] = v \to \\\\
    \hoare{P(m_0)}{body(k, v)}{P( insert(m_{0}, k, v) )}
}
{
  \hoare{\mapRep(\ell, m) \sep P(\emptyset)}%
{\cc{MapIter}(\ell, body)}%
{\mapRep(\ell, m) \sep P(m)}
}
\]

On top of this generic specification we prove some alternate specifications that
express the invariant in slightly different ways --- for example, it is often
useful to express the invariant in terms of both the map iterated over so far
and the remaining subset of the map.
