\section{Modeling and reasoning about Go}%
\label{sec:goose:reasoning}

A key principle in the design of Goose is to model features of Go as code,
written as libraries on top of the base GooseLang language described in
\cref{sec:goose:lang}. This section describes some of the features of Go that
Goose models, as well as reasoning principles developed on top to verify code
that uses these features.

Specifications in this section are part of the Perennial logic, described in
detail in \cref{ch:perennial}. Perennial is based on separation logic, and only
features of separation logic are needed to understand this section and not the
rest of the Perennial logic. The basic
specification in this logic is the \emph{Hoare triple}
$\hoare{P}{\cc{f()}}{Q}$, which says that if \cc{f()} is run in a state
satisfying the precondition $P$ and terminates, the final state will satisfy the
postcondition $Q$. Separation logic additionally means the specification
implicitly states other state is unmodified, supporting so-called
``small-footprint'' specifications where $P$ and $Q$
describe only the state actually involved in \cc{f()}.
Assertions make use of the \emph{points-to assertion} $\ell \mapsto v$, which
gives the value of one address in memory. For example, some basic specifications
in separation logic are those for the load and store operations:
%
\begin{mathpar}
  \infer{}{\hoare{\ell \mapsto v}{!\ell}{\Ret{v} \ell \mapsto v}}

  \infer{}{\hoare{\ell \mapsto v_0}{\ell \gets v}{\ell \mapsto v}}
\end{mathpar}
The specification for load uses $\Ret{v}$ to specify what the return value is.
Stores also execute to a value, but since it is the GooseLang unit value we omit
it in the specification.

The specifications in this section can be appreciated with only basic
familiarity with sequential separation logic. The model of pointers does handle
some subtleties related to concurrency, but as explained in
\cref{sec:goose:pointers} these are abstracted away for the purpose of proofs.
None of the reasoning principles for GooseLang are specific to crash safety. All of these
specifications are actually proven in the Perennial logic and thus can be used
as part of concurrency and crash safety proofs, as we did for GoTxn.

The Goose translator converts each function in the Go source to a Coq definition
of an analogous function in GooseLang. A call to a function \cc{foo} in the Go
code is translated to the \cc{foo} Coq declaration in GooseLang. As mentioned in
later in \cref{sec:goose:limitations}, this does have some limitations,
especially that it does not attempt to model recursion.

Each struct declaration in Go is translated to a Coq definition that declares
the struct fields, which is used by the GooseLang struct model (described in
detail in \cref{sec:goose:structs}). Top-level constants are translated in a
similar way to how Go functions are translated.

\subsection{Modeling pointers}%
\label{sec:goose:pointers}

The model of pointer loads and stores turns out to be subtle because of
concurrency. In
short, GooseLang disallows concurrent reads and writes to the same
location by making such racy access \emph{undefined behavior}. Any program verified
with Goose must show that it never has this kind of undefined behavior. This
restriction is intended to be more restrictive than the hardware and language
guarantees regarding races. The hardware provides some
guarantees, but they are typically weak: for example, it is common that
different cores can observe the effect of a write at different times (for
example, in x86-TSO~\cite{sewell:x86-tso}). Go's own
memory model documentation specifies even weaker guarantees.  Rather than
attempt to formalize Go's rules, the semantics side-steps the issue and makes
any races undefined, which works for our intended use cases since the verified
code always synchronize concurrent access with locks.\footnote{Go doesn't have
any formal semantics, but it does have a \href{https://go.dev/ref/mem}{memory
model} document that says reads may observe any concurrent write. This document
generally encourage well-synchronized programs, which is what the GooseLang
semantics enforces.}

To disallow concurrent reads and writes the semantics must first detect them. The key is to make
$x \gets v$, the primitive store operation, \emph{non-atomic} by splitting it
into two operations. The GooseLang semantics tracks the behavior of these
operations by augmenting the heap with extra information; each address in the
semantics has a $\textdom{NonAtom}$ which can be
$\nareading{n}{v}$ if there are $n$ readers and the value is $v$,
or $\nawriting{v}$ if a thread is currently writing. For most
pointers, stores are translated to a non-atomic store while loads are translated to
GooseLang's atomic \goosekw{Load} (concurrent loads to the same address are permitted in
both Go and GooseLang). GooseLang has support for non-atomic
reads as well in order to model map iteration.

Ordinarily values in the heap are of the form $\nareading{0}{v}$,
to indicate no readers or writers. Writes are split into
$\goosekw{PrepareWrite}(\ell)$, which sets the value of $\ell$ to
$\nawriting{v_{0}}$, and $\goosekw{FinishStore}(\ell, v)$ which sets
it to $\nareading{0}{v}$. A concurrent write will be undefined
since $\goosekw{PrepareWrite}(\ell)$ requires no concurrent writers, and
similarly for a concurrent read which is undefined if the address is being
written. Non-atomic reads are similar with $\goosekw{StartRead}$ and
$\goosekw{FinishRead}$; these increment and decrement the number of readers,
respectively, so that multiple readers can run concurrently but any concurrent
writer has undefined behavior.

Goose provides reasoning principles to abstract away this complexity while
verifying programs. Separation logic turns out to provide the right
framework to reason about racy access. When a thread owns
$\ell \mapsto v$ in its precondition, the logic guarantees no other thread can have access to location
$\ell$, so the specifications for reads and writes are unaffected by the
operations being non-atomic. The only change is that the
$x \gets v$ store operation is no longer an atomic primitive but a function that
takes two execution steps. In Iris this means that two threads cannot
share a pointer with an invariant and must mediate access with a lock,
which gives a thread ownership of the $\ell \mapsto v$ assertion for multiple execution
steps.

\subsection{Locks and condition variables}%
\label{sec:goose:locks}

\newcommand{\Acquire}{\goosedef{Acquire}}
\newcommand{\CAS}{\goosedef{CAS}}

Locks (Go's \cc{*sync.Mutex} type) are not built-in to GooseLang but modeled
using an implementation based on simpler primitives. Since locks are the
only synchronization primitive, implementing them requires shared
concurrent access, which ordinary pointers do not have in GooseLang.
Instead, the language also includes a primitive atomic compare-and-exchange
operation that is only used to implement a model of locks. Similar primitives
could be used to model Go's low-level atomic operations, like
\cc{atomic.CompareAndSwapUint64} and \cc{atomic.LoadUint64}, but Goose does not
support them since our code hasn't required such synchronization primitives.

The model of locks is simple enough to give the code in its entirety. The lock
is represented as a pointer to a boolean that is true if the lock is held. As a
helper we define $\CAS$ (compare-and-swap), a variant of compare-and-exchange
that only returns a boolean on success and not also the previous value.

\begin{align*}
  \CAS &\defeq \gooselambda{x, v_1, v_2} \pi_{2}\app \goosekw{CmpXchg}(x, v_1, v_2) \\
  \goosedef{NewLock} & \defeq \gooselambda{\_} \goosekw{ref} \app \goosekw{false} \\
  \Acquire &\defeq \gooselambda{l} \\
       &\goosekw{let}\app f = (\goosekw{rec}\: \textlog{tryAcquire}(\_) = \\
       &\qquad \gooseif{(\CAS \app l \app \goosekw{false} \app \goosekw{true})}%
         {()}{\textlog{tryAcquire} \app ()}) \app\goosekw{in} \\
       &f \app () \\
  \goosedef{Release} &\defeq \gooselambda{l} l \gets \goosekw{false} \\
\end{align*}

Acquiring a lock is modeled as repeatedly using
$\CAS \app l \app \goosekw{false} \app \goosekw{true}$ to
atomically test that whether the lock is free and set it to \goosekw{true} if so, while release
stores \goosekw{false} to the lock. This implementation as a spin lock is merely an
operational model that captures what the lock does: acquire blocks until the
lock is free and sets it to locked, while release frees the lock. This code is
used to model Go's builtin \cc{*sync.Mutex}, which is implemented more
efficiently than spin locks with cooperation from the runtime and operating
system.

The specification for locks is a typical one for concurrent separation logic,
based on associating a \emph{lock invariant} with each lock, which is a predicate that holds when the lock
is free:
\begin{mathpar}
    \inferH{wp-newlock}{}%
    {\hoare{P}{\goosedef{NewLock}}{\Ret \ell \islock{\ell}{P}}}

    \inferH{wp-lock-acquire}{}%
    {\hoare{\islock{\ell}{P}}{\goosedef{Acquire}(\ell)}{P}}

    \inferH{wp-lock-release}{}%
    {\hoare{P \sep \islock{\ell}{P}}{\goosedef{Release}(\ell)}{\TRUE}}
\end{mathpar}
Because this is a separation logic, we can also interpret the lock
invariant as ownership over some data (for example, some region of memory); the
lock mediates access to this ownership, handing it out when the lock is acquired
and requiring it back when the lock is released. GooseLang has a proof that the
specification holds for the spin-lock implementation.

Goose also supports Go's condition variables. As is standard, the idea is that
there is some ``condition'' associated with each condition variable; one thread
calls \cc{Signal()} or \cc{Broadcast()} when that condition is true, and another
thread calls \cc{Wait()} to wait for the condition to become true. The condition
variable is associated with a mutex in Go, which must be held to call
\cc{Wait()}. It will be released and then re-acquired when the thread is
signaled, but before the waiting thread wakes up other threads can wake up and
potentially invalidate the condition (the code must therefore call \cc{Wait()}
in a loop, waiting for the condition to be true afterward).

Because condition variables provide few guarantees, the GooseLang model is
simply that \cc{Wait()} releases and then immediately re-acquires the lock associated with the
condition variable. \cc{Signal()} in reality affects the scheduling of
\cc{Wait()} calls, but this model captures all executions. Go condition variables actually do provide a stronger
guarantee than in some other systems: \cc{Wait()} is guaranteed to return only if
\emph{some} thread has issued a \cc{Signal()}, but we did not attempt to model
or use this for reasoning purposes.

\subsection{Structs}
\label{sec:goose:structs}

\newidentmacro{LoadTyped}
\newidentmacro{StoreTyped}
\newidentmacro{loadField}
\newidentmacro{storeField}
\newdefmacro{structType}

One of the most important features of Go to support is structs. Goose support
for structs uses a form of \emph{shallow embedding} using a combination of
Gallina (Coq's functional language) and GooseLang. Goose encodes
higher-level primitives like field access on top of GooseLang primitives like
tuples and contiguous memory.

Struct types are represented using a combination of two Gallina types:
$t \in \textdom{GooseType}$ gives the type of a struct while
$s \in \textdom{StructDecl}$ is an (anonymous) struct declaration that gives its
field names and their types. The definitions of these two types are given in
\cref{fig:goose:types}. The exact types are not important in the semantics,
but the translation does use the shape of a struct to determine how it is laid
out in memory and to support pointers to individual fields. As an example of how
approximate these types are, it is sufficient to have a \goosekw{ptrT} type for
all pointers; if a pointer to a struct is actually dereferenced (or a pointer to a
struct field), then the translation uses its type in Go to determine how that
dereference should be implemented. Using types to represent struct shapes
also makes the translation simpler, since the Goose implementation has
access to accurate types from the \cc{go/types} package, which is essentially
the Go type checker.

\begin{figure}[ht!]
\begin{mathpar}
  \begin{array}{lrcl}
    \textdom{GooseType} & t &::=& \goosekw{uint64T} \ALT \goosekw{boolT} \ALT
                                  \goosekw{unitT} \ALT
                                  \dots \\
                        &&\ALT & t \times t \ALT \goosekw{ptrT} \ALT \dots \\
    \textdom{StructDecl} & s &\in& \textlog{list}(\textlog{string} \times
                                   \textlog{GooseType}) \\
    \multicolumn{2}{r}{\structType([])} &\defeq& \goosekw{unitT} \\
    \multicolumn{2}{r}{\structType((f, t) :: s)} &\defeq& t \times \structType(s)
  \end{array}
\end{mathpar}
\caption[GooseLang types and struct declarations.]%
{GooseLang types and struct declarations. These are used in the
  semantics only to give a ``shape'' to data for accessing struct fields, and
  not to represent the Go type system.}%
\label{fig:goose:types}
\end{figure}

First Goose needs to model struct values. A struct value is represented as a tuple
of its fields. The definition of the struct gives an ordering of the fields.
This is enough to construct a struct from its fields and to access a field by
name. Here the shallow embedding comes in: struct declarations are translated to
a definition of the struct type in
Gallina, and struct construction and field access are written as Gallina definitions that
produce GooseLang expressions. From the perspective of GooseLang then the struct
implementations are a type of macro, since they are implemented in the meta
language (Gallina) rather than within GooseLang itself.

Structs in memory are more interesting than struct values. Structs could
be stored in a single location; due to our non-atomic semantics for
memory, this would be sound even for structs larger than a machine word.
However, this model would be too restrictive: it is safe for threads to
concurrently access \emph{different fields}, just not the same field,
and our code actually takes advantage of this property so that code is more
natural; working around this restriction requires splitting structs up if they
are stored in memory.

To support this concurrency, Goose models a struct in memory with a
flattened representation, with each base element in a separate memory
cell. The flattening applies recursively to fields that are themselves
structs, until a base literal is reached (like an integer or boolean);
base elements are at most a machine word in size. When
allocating a new pointer, the semantics flattens composite values and
stores the elements in a sequence of contiguous addresses.

With a flattened representation the translation needs non-trivial code to read a struct
through a pointer, particularly when some of its fields are themselves flattened
structs. Any load of a value from memory is translated to a call to
$\LoadTyped$. This is a Gallina definition of type
$\textdom{GooseType} \to \textdom{Expr}$, where that expression is itself a
GooseLang function that takes a pointer. Hence one can think of it as a macro
for GooseLang that is represented in Gallina as a meta language. $\LoadTyped(t)$
is directed by a type $t$ passed in Gallina to determine how to load and
assemble the fields of a struct of type $t$.

For the purpose of proofs GooseLang's specifications use a \emph{typed points-to fact}
of the form $\ell \mapsto_t v$ to describe a pointer to a value of type $t$. This
definition expands to a number of primitive points-to facts, one for
each base element. The specification for loading a pointer to a value of type
$t$ is
\[
\hoare{\ell \mapsto_t v}{\LoadTyped(t) \app \ell}{\Ret{v} \ell \mapsto_t v},
\] which
(much like the primitive load $!\ell$) hides the fact that something
non-atomic is happening and looks like an ordinary dereference.
Similarly, $\StoreTyped$ also takes a type, although the specification
requires the caller to prove that the value has the right shape (in
reality it always will because the source code in Go is well-typed).

The payoff of structs being many independent locations is that it is
possible to model references to individual struct fields. From a pointer
to the root of the struct, a field pointer is simply an offset from that
pointer (skipping the flattened representations of the previous fields).
This offset calculation is much like the code to read a struct from
memory, except that it merely computes a single offset rather than
iterating over all the fields and offsets.

The Go language reference specifies that each field acts like an
independent ``variable'' (which is stored in the GooseLang heap when it
is mutable in Go), so this model accurately reflects the
language definition. Moreover modeling structs as independent locations is
also justified as being similar to how the implementation works. Structs
in memory are in reality represented by contiguous memory, and field
access is implemented by computing a pointer from the base of the
struct. The main difference between the physical implementation and the
model is that the model has a single, abstract memory location for each field,
whereas the implementation encodes all data into bytes.

Recall that $\ell \mapsto_t v$ is internally composed of untyped
points-to facts for all the base elements of $v$. In order to reason
about $v$'s fields, we introduce a new \emph{struct field points-to fact},
written $\ell \mapsto_{s.f} v$, which asserts ownership of just field
$f$ of a struct with descriptor $s$ rooted at $\ell$, and gives that field's
value as $v$. A recursive function gives an ``exploded'' set of struct
fields by iterating over $t$'s fields and $v$ simultaneously. Then,
we give a proof that $\ell \mapsto_{\structType(s)} v$ is equivalent to the separating
conjunction of this exploded list. The result is a convenient lemma for
reasoning about a struct using its fields: in the forward direction, the
equivalence breaks a large typed points-to into individual fields (with
the values computed from $v$), while in the other direction it allows
to prove a $\ell \mapsto_{\structType(s)} v$ by gathering up all the fields.

The struct field points-to is indispensable in proofs, because it is common to
write \cc{x.f} in Go when $x$ is a pointer (in C, this would be written \cc{x->f}). The model
for loading a struct field is a function $\loadField(s, f) \app x$
which is implemented in two steps, first computing the offset to field
$f$ and then dereferencing the computed pointer (in both cases the struct descriptor $s$
describes how to interpret field $f$). Having a field points-to gives
a natural specification for this type of load:
\begin{mathpar}
\hoare{\ell \mapsto_{s.f} v}{\loadField(t, f) \app \ell}{\Ret{v} \ell
  \mapsto_{s.f} v}
\end{mathpar}

The lemmas about breaking apart and recombining structs are all proven
against a simpler model of structs that only requires flattening and
offset calculations. In a sense the model is the trusted code, but the
fact that the struct maps-to exploding lemma and all of the
expected Hoare triples hold provides strong evidence that the model is
also doing the right thing. For example, the exploding lemma shows that
field offsets are disjoint, since the struct maps-to can be broken into
field points-to facts for each field.

\subsection{Slices}%
\label{sec:goose:slices}

One of the most commonly used data structures in Go is the slice
\cc{[]T}, which is a dynamically-sized array of values of type
\cc{T}. Goose supports a wide range of operations on slices,
including appending and sub-slicing; it turns out that the semantics of
mutable slices is non-trivial in Go, resulting in an interesting
semantics and reasoning principles.

A slice is a combination of a pointer, length, and capacity. Slices are
views into a contiguous memory allocation; this view can be narrowed
with sub-slicing operations of the form \cc{s[i:j]}, resulting
in aliased slices. The elements between the length and capacity are not
directly accessible but are used to support efficient amortized appends.
Go's built-in slice operations include bounds checks on all slice
operations and panic if a memory access or sub-slice operation goes out
of bounds.

GooseLang has a primitive for allocating contiguous memory, which Goose uses to model
the allocation underlying a slice (though these are not directly
accessible to Go code). On top of these Goose models slices as a tuple of a base
pointer, length, and capacity.

The GooseLang slice model is directly inspired by the implementation of
slices, including modeling slice capacity. Initially Goose used a more
abstract model of slices that ignored capacity, but we were surprised to find that this was insufficient to
even accurately model subslicing and appending. Directly modeling slice
capacity was the simplest solution to obtain a model that is faithful to
the Go implementation. The Go language reference isn't specific about
what the slice capacity after various operations should be, so our
GooseLang model picks a non-deterministic capacity in several places
(within appropriate bounds).

\newidentmacro{ptr}
\newidentmacro{len}
\newidentmacro{cap}

The most basic operations on slices are indexing and storing. The
GooseLang model of $s$ is a three-tuple, but for clarity this section will refer
to its elements as $\ptr(s)$, $\len(s)$, and $\cap(s)$. The
translation of \cc{s[i]} is essentially a (typed) load from
$\ptr(s) + i$ (or undefined behavior if this offset is out-of-bounds).
Similarly \cc{s[i] = x} stores to the same location. Goose translates Go's
\cc{len(s)} directly to $\len(s)$ and $\cc{cap(s)}$ to $\cap(s)$.

The Go \cc{append} operation is the most sophisticated to model. The
behavior of \cc{append(s, x)} where \texttt{s:\;[]T} and
\texttt{x:\;T} depends on whether there is extra capacity to store the
new element \cc{x}. If there is capacity, then \cc{x} is stored
there and the append returns a new slice with the same pointer but a
larger length. If there is no capacity, then \cc{append} must
allocate a new array in memory, copy the existing elements to it, and then store
\cc{x}. In the latter case \cc{append} returns a slice with a
fresh pointer.

The difficulty with Go slices arises when supporting subslicing. Consider
\cc{s[:i]}, where \cc{i} is less than \cc{len(s)}.
Clearly this slice should have the same pointer and length \cc{i},
but what should its capacity be? Surprisingly, the capacity of this
prefix is the full capacity of \cc{s}, which means that the unused
elements of \cc{s[:i]} \emph{include the elements of \cc{s}}
beyond the index \cc{i}. As a result, \cc{append(s[:i], x)}
in fact modifies \cc{s[i]}. GooseLang takes care to model this
behavior by implementing \cc{append} exactly as above, taking into
account that \cc{append(s, x)} might be an in-place operation.

The GooseLang model is specifically designed to be sound by sticking to
the Go implementation as closely as possible. On the other hand the reasoning principles
for slices are intended to be convenient and high-level, and do not involve
directly reasoning about slice capacity. The design of GooseLang separates the
model from the reasoning principles --- Goose has specifications verified against
the concrete model, so that the model is trusted and not the
separation-logic specifications.

\newcommand{\sliceRep}{\mathtt{sliceRep}}
\newcommand{\sliceCap}{\mathtt{sliceCap}}

The GooseLang model of slices is based on two abstract predicates:
$\sliceRep(s, l)$ and $\sliceCap(s)$. In the Coq implementation, the model of a slice value is
a Gallina record $s$ that must be converted explicitly to a GooseLang value, but
this thesis will not make a distinction. Furthermore this section presents just the
\emph{untyped} version of this specification where $l : \textlog{list}(\textdom{Val})$, but
GooseLang also has a typed version where $l : \textlog{list}(T)$ for a type $T$
that has an associated Gallina function $\mathtt{to\_val} : T \to \textdom{Val}$. The typed
version is convenient in proofs and can be verified as a small extension to the
untyped specification.

The first predicate $\sliceRep(s, l)$ relates $s$ to the abstract value $l$ that
it references. It
also represents ownership over the slice's elements and its capacity, in terms of the
underlying struct points-to facts. GooseLang's slice specifications use this
predicate to describe loading and storing by index:
\begin{mathpar}
  % these are wrapped in infer so that they get math-mode spacing (otherwise
  % there's an extra blank after every line)
  \infer{}{\hoareV[b]{\sliceRep(s, l) * i < |l|}%
{\mathtt{s[i]}}%
{\Ret{v} v = l[i] * \sliceRep(s, l)}}

  \infer{}{\hoareV[b]{\sliceRep(s, l) * i < |l|}%
 {\mathtt{s[i] = v}}%
{\sliceRep(s, l\mapupd{i}{v})}}
\end{mathpar}

\newidentmacro{sliceFull}
\newdefmacro{take}
\newdefmacro{drop}

Next, $\sliceCap(s)$ is an abstract predicate that represents
\emph{ownership over the capacity} of $s$. It is necessary to append,
since appending might need to write to the capacity, but unneeded to
read and write to a slice. We introduce a shorthand
$\sliceFull(s, l)$ for the append specification, which is like $\sliceRep$ but also
includes ownership over the capacity:
%
\begin{mathpar}
\infer{}{
  % need the extra curly braces
{\begin{aligned}
&\sliceFull(s, l) \defeq \\
&\quad  \sliceRep(s, l) * \sliceCap(s)
\end{aligned}}
}

\infer{}{\hoareV[t]{\sliceFull(s, l)}%
{\mathtt{append(s, x)}}%
{\Ret{s'} \sliceFull(s', l \lappend [x])}}
\end{mathpar}

This specification looks simple but has a non-trivial proof, since in the case
where there is sufficient capacity it moves ownership from $\sliceCap(s)$ to
$\sliceRep(s', l \lappend [x])$ (and $\ptr(s') = \ptr(s)$), while in the other
case where there is no capacity it allocates a new pointer for $s'$ and copies
everything over.

The most interesting rules are for subslicing and how they interact with
capacity. Consider \cc{s[:i]} again. While Go has no formal
notion of ownership, our specifications do. The model for the
\emph{value} \cc{s[:i]} is easy enough: it is a pure function that returns a new slice value
with a smaller length and the same capacity. This thesis uses $s[:i]$ as the
notation for this pure function since the two are so similar. There is one slight complication that
\cc{s[:i]} causes a runtime error in Go if $i \geq \len(s)$. Thus GooseLang
proves the following specification for indexing:
\[
  \hoare{i < \len(s)}{\cc{s[:i]}}{\Ret{s[:i]} \TRUE}
\]
This specification is slightly unusual in that it has a logical precondition but
does not refer to the heap (or any other state).

To reason about subslicing from the perspective of memory, the specification
needs to relate ownership of
$\sliceRep(s, l) * \sliceCap(s)$ to ownership of
$\sliceRep(s[:i], \take(l, i))$. It turns out there are two
possibilities that the proof engineer can choose between: either give up ownership of the remainder of $s$
in exchange for $\cap(s[:i])$, or ignore the
capacity of the subslice and keep
$\sliceRep([i:], \drop(l, i))$. These are incomparable and
unexpressed in the Go code being verified: the decision is based on whether subsequently the program will
append to the slice prefix but stop using the old slice, or if it will use both
$s[:i]$ and $s[i:]$.

To support both ways that $s[:i]$ might be used, GooseLang has two theorems for
reasoning about taking a prefix of a slice. Both of these theorems require
$i < \len(s)$.
\begin{mathpar}
  \inferH{slice-take-full}{}{%
{\begin{array}{ll}
&\sliceFull(s, l) \vdash \\
&\quad \sliceFull(s[:i], \take(l, i))
\end{array}}
}

\inferH{slice-split}{}{
  % extra braces are required to "protect" aligned, see
  % https://tex.stackexchange.com/questions/37513/inserting-arrays-into-semantics-inference-rules
  {\begin{array}{ll}
  &\sliceRep(s, l) \dashv\vdash \\
  &\quad \sliceRep(s[:i], \take(l, i)) \sep {} \\
  &\quad \sliceRep(s[i:], \drop(l, i))
  \end{array}}
  }
\end{mathpar}

The rule \ruleref{slice-take-full} captures that the program proof can get full ownership of
$s[:i]$, in exchange for dropping any ownership over the rest of $s$.
The second rule \ruleref{slice-split} instead allows the program proof to split $s$ into two
parts at the $i$th index, but in exchange requires giving up $\sliceCap(s[:i))$, the right
to append to the first half.

There is one more fact that is sometimes useful, which is that
$\sliceCap(s) \dashv\vdash \sliceCap(s[i:])$ (this is somewhat intuitive;
capacity is past the end of the array, so dropping elements from the front has
no impact). As a corollary of \ruleref{slice-split}, the proof can also split full
ownership of a slice into two (asymmetric) parts:
\begin{mathpar}
  \inferH{slice-split-full}{}{
{\begin{array}{ll}
  &\sliceFull(s, l) \dashv\vdash \\
  &\quad \sliceRep(s[:i], \take(l, i)) \sep {} \\
  &\quad \sliceFull(s[i:], \drop(l, i))
\end{array}}
}
\end{mathpar}

These two reasoning principles correspond to two ways to think about indexing:
taking a prefix while using the original slice as capacity for the prefix (the
first theorem, \ruleref{slice-take-full}), or splitting the slice into two parts
(the second theorem \ruleref{slice-split} and
its corollary \ruleref{slice-split-full}). It was interesting that we discovered these principles only
while proving theorems about subslicing in GooseLang. Even as experienced Go
developers, we had never thought about subslicing and appending in enough detail
to appreciate this difference in ownership reasoning.

\subsection{Maps}%
\label{sec:goose:maps}

\newidentmacro{mapVal}
\newidentmacro{mapRep}
\newidentmacro{mapDelete}
\newidentmacro{mapInsert}
\newidentmacro{mapIter}

After slices, maps are the next most commonly used collection type in
Go. GooseLang models maps using an implementation that represents a map as a
list of key-value pairs, stored in a single memory location in reverse insertion
order. Go's builtin maps are
\emph{not} thread-safe, so the model enforces single-threaded access by
marking the map as being read while reading from it; this re-uses the
race detection for other pointers to ensure that any write to a map while iterating
over it is considered
undefined behavior, while allowing concurrent read-read access. Maps
support all the usual Go operations: insertions, reads (including returning
whether the key is present), \cc{len} to get the number of elements
in the map, deletion, and iteration.

The implementation of maps is the most involved out of any of the Go
primitives. It required directly implementing maps (albeit
inefficiently, using an association list) using recursive GooseLang
code. We improved our confidence in this
implementation both by running it on concrete examples (using the infrastructure
described in \cref{sec:goose:testing}) and proving a specification for the
various map operations. Both of these
essentially rule out type errors,
and the specification is a redundant check on the behavior. Simple tests and verification
rule out easy-to-make mistakes like reading the oldest write to a key rather than
the latest, or duplicate keys during iteration (the implementation must
skip over a key after observing it once).

The model represents
a Go map as a pointer to an abstract map value, a GooseLang value
that encodes the entire map data as a list of key-value pairs. The
specification is based on a pure relation $\mapVal(v, m)$ that relates
this encoded value to a Gallina map $m$, which uses \cc{gmap} from
stdpp;\footnote{\url{https://gitlab.mpi-sws.org/iris/stdpp}}
for simplicity $m$ is of type \cc{gmap u64 val} and Goose only supports
maps whose key type is \cc{uint64}.
Map values are not a visible notion to the Go code, since
it always interacts with maps via their pointer, so the specifications
all use $\mapRep(\ell, m) = \exists v.\, \ell \mapsto v * \mapVal(v, m)$. The
indirection is important, since the Go map value
\texttt{m:\;map[uint64]V} is in fact a reference to a map that is
mutated in-place (unlike a slice, which has both pure data --- pointer,
length, and capacity --- and heap data). Concurrency is simple for these
specifications because Go's maps are not thread safe, so all the specifications
assume full ownership over the map's reference.

Many of the map specifications relate an implementation to a model over the
Gallina map. For example, this is the specification for map deletion:
\begin{mathpar}
\infer{}{\hoareV{\mapRep(l, m)}{\mapDelete(l, k)}{\mapRep(l, \textlog{delete}(m, k))}}
\end{mathpar}

Map iteration has a more sophisticated implementation and specification.
Consider a generic loop over a map in Go like the following:

\begin{verbatim}
for k, v := range m {
  body(k, v)
}
\end{verbatim}
%
The model for this entire construct is given by $\cc{MapIter}(m, \textlog{body})$, where
$m$ is a reference to the map and $body$ is an expression for the body of the
loop. Goose translates generic loop bodies, so the Go code does not literally
need to consist of a call to a separate function. Recall that the representation
for a map is a list of all appended key-value pairs in reverse insertion order
(insertions do not delete old values). To iterate over the map's contents, the
implementation gets the next key-value pair, then deletes that key from the rest
of the map before making a recursive call, avoiding iterating over old values
that are no longer logically part of the map. The code for the model uses the
order of key-value pairs in the order of the GooseLang representation, but Go
actually randomizes map iteration order; see the relevant discussion of
limitations in \cref{sec:goose:limitations}.

The possibility of
\emph{iterator invalidation} adds a subtlety to Go's map iteration --- it
would be incorrect for the body of the loop to modify the map (it might be sound
to write to the map without modifying the domain, but Goose does not attempt to model
this). The model of maps puts the entire contents of a map in one heap location.
In order to ensure iterator invalidation is undefined behavior, the model marks the map's
reference as being read for the entire duration of iteration, using the
$\goosekw{StartRead}$ and $\goosekw{FinishRead}$ GooseLang primitives at the
beginning and end of $\cc{MapIter}$, respectively. If the map value in the heap is
$\nareading{n}{v}$, these operations increment and decrement
(respectively) the reader count $n$, so that any writes within the body have
undefined behavior.

Iteration has a \emph{higher-order} specification that assumes a specification
for the body, showing it preserves a loop invariant $P$ over the part of the map
consumed so far:
\begin{mathpar}
  \infer{
    \forall m_{0}, k, v.\,
    k \notin m_{0} \land m[k] = v \to \\\\
    \hoare{P(m_0)}{\textlog{body} \app k \app v)}{P( m_{0}\mapupd{k}{v} )}
}
{
  \hoare{\mapRep(\ell, m) \sep P(\emptyset)}%
{\cc{MapIter}(\ell, \textlog{body})}%
{\mapRep(\ell, m) \sep P(m)}
}
\end{mathpar}

GooseLang includes some alternate specifications, proven on top of this one, that
express the invariant in slightly different ways --- for example, it is often
useful for a proof to express its loop invariant in terms of both the map
iterated over so far and the remaining subset of the map.

Getting the size of a map has a relatively simple specification but a somewhat
complex implementation. Like map iteration, the code for \cc{MapLen} (modeling
\cc{len(m)} in Go) must delete old keys from the map's association list in order
to avoid counting overwritten values. A subtlety is that the map length is a
64-bit integer, and this counter could in principle overflow while iterating
over the map. This situation is unrealistic since a map with $2^{64}$ elements
would take an unreasonable amount of memory, so the model \emph{assumes} that
the map size stays below $2^{64}$. The specification for \cc{MapLen} is:
\begin{mathpar}
\infer{}{%
\hoareV{\mapRep(\ell, m)}%
{\cc{MapLen}(\ell)}%
{\Ret{s} s = |m| * \mapRep(\ell, m)}%
}
\end{mathpar}
The fact that the type of the return value $s$ is a 64-bit integer reflects the
bounded-size assumption.
