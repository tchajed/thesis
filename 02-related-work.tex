DaisyNFS draws on several lines of research. This chapter both explains the
research DaisyNFS directly builds upon and other work solving similar problems.

\section{The Flashix file system}

The Flashix project deserves special attention since it also develops a verified
concurrent and crash-safe file system~\cite{bodenmuller:concurrent-flashix}.
Flashix targets flash storage, a lower-level storage technology than the drives
that DaisyNFS targets. The project has different goals around concurrency,
extending a sequential verified file system with limited concurrency. Flashix
has an idiosyncratic approach to reasoning about concurrency and crashes
specific to their file-system design, as opposed to Perennial's general
reasoning principles. Flashix is implemented using abstract data types in a
high-level language; a code generator transforms this code into executable C,
but this process is both not verified and has difficulty producing the most
efficient code using in-place updates.

While Flashix does have mechanized proofs, the approaches for both concurrency
and crash safety are layered on top in such a way that the top-level
specification and proof are not entirely within the proof assistant. Perennial
on the other hand formalizes the complete specification down to a simple
semantics for crashes and concurrency. Flashix also effectively has crash
reasoning throughout the system, whereas DaisyNFS factors out this reasoning to
GoTxn and then layers sequential proofs on top.

The Flashix project found that components in the stack intertwine due to the
realism of the system; modularity helped but the interfaces still had many
dependencies. We had a similar experience within the transaction system, where
performance constrained the APIs of internal layers and forced us to export
complicated APIs. GoTxn was perhaps trickier than the components of Flashix
because we model code at a lower level of abstraction, so even issues like
concurrency and memory safety show up in each interface. We were inspired to
develop the DaisyNFS design to get truly sequential reasoning by the pain in
verifying GoTxn, and found modularity and clean abstractions much easier once
code ran within a transaction.

Where it lacks in generality, Flashix does make up for in verifying a taller
storage stack than DaisyNFS. Flash storage has a more limited API than a
standard drive --- in particular flash blocks must be completely erased before
being reused. A flash file system works around this limitation with \emph{garbage
collection}, where occasionally the system identifies a mostly-unused block,
moves its valid data elsewhere, and then erases it to reclaim space. This is
implemented by maintaining a logical-to-physical block mapping, similar to
virtual memory. DaisyNFS does not require any of this code. We do generally run
on SSDs which internally use flash blocks and themselves implement garbage
collection and a logical block mapping so that they can support overwriting data
in-place.

\section{Crash-safety foundations}

There are a variety of foundational tools for reasoning about crash safety,
largely for sequential systems.

FSCQ~\cite{chen:fscq,chen:dfscq,hchen-phd} is a verified sequential file
system that features Crash Hoare Logic (CHL). CHL extends the basic Hoare triple
with a \emph{crash condition} that holds at all intermediate points in the
function's execution. Crash conditions handle a core difficulty of crashes,
namely that they stop the system at some intermediate step. There are two
remaining challenges: a crash wipes in-memory state, and the system might crash again
while recovering after a crash. CHL connects the system's crash conditions to a
specification for recovery to handle these two issues. A crash predicate
transformation captures what can be assumed when recovery starts, and for
crashes during recovery CHL requires that recovery be \emph{idempotent} in the
sense that its crash condition implies its own precondition. CHL was used to
specify and verify FSCQ~\cite{chen:fscq} and a more performant successor
DFSCQ~\cite{chen:dfscq}.

Yggdrasil~\cite{sigurbjarnarson:yggdrasil} takes a different approach to crash
safety. The basic definition is \emph{crash refinement}, which says that a
system implements an interface correctly, including a specification for what a
crash followed by recovery is allowed to do. Note that unlike CHL this
specification is about a collection of methods implementing an abstract,
specification transition system, not about individual methods. Yggdrasil uses
crash refinement to specify and verify a file system comparable to the file
system in xv6, a teaching operating system. The implementation uses Z3 to check
crash refinement, which the authors show is able to handle a system of this
complexity by breaking down the implementation into small enough layers.

Argosy~\cite{chajed:argosy}, which I led the development of but is not part of
this thesis, combines aspects of FSCQ and Yggdrasil. The key new idea is to
develop the metatheory for \emph{recovery refinement} that shows how systems
compose when both have recovery procedures --- what is non-trivial to handle is
that a crash in the composed recovery procedure requires starting over from the
beginning. Recovery refinement can be viewed as an extension of crash refinement
with verified metatheory for recovery, largely left implicit in the Yggdrasil
paper. Argosy also shows how to encode the conditions of recovery refinement
using CHL so that a single layer is verified using the CHL program logic.

VeriBetrKV~\cite{hance:veribetrkv} takes yet another approach to reasoning about
crashes, this time friendly to encoding in Dafny, a sequential verification
system with integrated support for programming and verification. The main idea
related to crash safety is to adopt the style from
IronFleet~\cite{hawblitzel:ironfleet} and think of a storage system as a
distributed system made up of the CPU and the storage device. The extension
needed for crash reasoning is to add a crash transition to the storage device
that non-deterministically wipes any buffered but unacknowledged writes, and
then to show that when this happens it corresponds to an appropriate
application-level transition modeling crashes (similar to the crash refinement
definition from Yggdrasil and recovery refinement in Argosy, although this is
associated with a code transition rather than a dedicated recovery procedure).
VeriBetrKV is used to verify a persistent key-value store based on
B\textsuperscript{$\epsilon$} trees, a data structure that also underlies
BetrFS~\cite{jannen:betrfs}.

Perennial has crash conditions that look similar to CHL's crash conditions,
albeit as part of a concurrent program logic rather than a sequential one. We do
carry out a refinement proof in the style of Argosy, which is similar to the
specification style in VeriBetrKV and Yggdrasil, but because it is connected to
concurrent reasoning the proof techniques are more sophisticated.

\section{Concurrency foundations}

There are a number of approaches proposed to verifying concurrent programs. It
would be hard to do justice to the historical development of concurrency
reasoning. Looking at approaches that are actively used in research and
connected to executable implementation, two broad strategies are commonly used:
developing concurrent \emph{program logics}, and using \emph{refinement}-based
techniques.

Many refinement-based techniques are based on the idea of \emph{reduction},
which appeared originally in Lipton's theory of ``movers''~\cite{lipton:movers}.
The idea behind reduction is to reason about a program through program
transformations that show the program is equivalent to a simpler program. These
techniques can reason about concurrency by showing that a concurrent program is
equivalent to a program with sequential or atomic regions. These ideas have been
used as part of the CIVL verifier~\cite{hawblitzel:civl,kragl:civl-layers} and
Armada~\cite{lorch:armada}; my own prior work on CSPEC~\cite{chajed:cspec} was
also based on movers, before working on Perennial. Reduction-based techniques
generally reason about a program through a series of transformations, each
making the program slightly simpler, keeping the proof of each transformation's
correctness more manageable.

One large verified system, CertiKOS, is based on a custom verification
infrastructure called Certified Concurrent Abstraction
Layers~\cite{gu:certikos-ccal} which based on refinement but not reduction. This
work is notable for verifying a system (a simple operating system) not just at
an abstract protocol level but all the way down to concurrent code. The proof
composes with the CompCert compiler correctness theorem to carry the guarantees
down to the assembly code of the operating system.

Program logics are an alternative approach based on giving specifications to
each function in the program, within a logic that has useful rules for proving
and composing specifications. Hoare logic is a classic program
logic for sequential programs that based on pre- and post-conditions.
Concurrency makes it harder to construct a logic that can usefully reason about
many concurrency patterns (completeness) while also giving specifications that
hold in the presence of concurrent threads (soundness). One productive line of
work has been based on concurrent separation logic (CSL)~\cite{brookes:csl}.

The Verified Software Toolchain is notable for connecting a CSL-based logic
(VST-Floyd) down to proofs of C code, including a connection to CompCert to
carry these guarantees down to assembly~\cite{cao:vst-floyd}. It has been used
for a number of sequential verified C programs and recently to some shared
memory C libraries.

Perennial builds directly on Iris~\cite{jung:iris-jfp}. Iris is a general
framework for concurrency, featuring a base logic with key features for
concurrency (step indexing and separation-logic resources, including
\emph{higher-order} resources used to define invariants for example) and a
program logic built on the base logic. This decomposition was valuable for
Perennial because we made two core changes to Iris: first, the program logic
itself is extended with crash safety reasoning, and second, the framework is
applied to our GooseLang models of Go code. In both cases the generality of Iris
was valuable, as the framework is not tied to reasoning about a particular
programming language or even to its usual program logic. Prior to this work,
Iris had not typically been used for reasoning about executable code (though
projects like RefinedC~\cite{sammler:refinedc} and our own work on Goose are
changing that).

\section{Crash safety and concurrency}

Program logics other than Perennial
have been developed for formal reasoning about concurrent, crash-safe systems.
Fault-Tolerant Concurrent Separation Logic (FTCSL)~\cite{ntzik:faults} extends
the Views~\cite{dinsdale:views} concurrency logic to incorporate crash-safety.
POG~\cite{raad:pog} is a program logic for reasoning about the interaction of
x86-TSO weak-memory consistency and non-volatile memory.  Neither logic has a
mechanism for modular proofs of layers, which we found essential to scale
verification to a file system. Both are restricted to pen-and-paper proofs,
whereas Perennial has machine-checked proofs. In the case of x86-TSO with
persistence, merely defining the semantics at the ISA level was a line of
research work leading up to POG~\cite{raad:px86,raad:px86-extended}. A similar
semantics effort defines the semantics of ext4 under crashes, but without an
accompanying program logic~\cite{kokologiannakis:persevere}.

\section{Transaction systems}

As far as we know, GoTxn is the first transaction system that makes data durable
and has a verified implementation. There is much related work on verifying
aspects of transaction systems with and without durability, and on unverified
transaction systems.

\citet{ChkliaevHS99} verify serializability of two-phase locking and other
transaction concurrency control mechanisms in the PVS theorem prover. Their
proof formalizes two-phase locking as an abstract protocol consisting of
sequences of read, write, and locking operations, as opposed to a concrete
implementation as in \sys. \citet{pollak-2PL} uses a variant of the
CAP separation logic~\citep{dinsdale:cap} to give a pencil-and-paper
proof of serializability for a two-phase locking implementation.

\citet{mohsen:stm} developed a framework for verifying software transactional memory algorithms, modeled
as I/O automata. They applied their framework to sophisticated STM algorithms, such as
NOrec algorithm~\cite{dalessandro:norec}. The STM algorithms considered do not
handle persistence, so the framework does not address crash-safety reasoning.

A specification called the Push/Pull model of
transactions~\cite{koskinen:pushpull} is similar to the \emph{lifting} technique
in the journal system's specification~(\cref{sec:txn:lifting}) --- the core
problem addressed is that a journal operation atomically modifies a small number
of objects, but other objects can change between the start of the operation and when
it commits. The Push/Pull model also discusses reasoning on top of the
specification, using Lipton's reduction~\cite{lipton:movers} rather than
separation-logic ownership to handle concurrency. However that work is about
on-paper specifications and proofs, while we also prove an implementation meets
our specification and proved \simplenfs on top.

DFSCQ~\cite{chen:dfscq} verifies a high-performance file system built on top of
a logging system with asynchronous disks and log-bypass writes, which are
challenging optimizations that GoTxn does not support. ARIES is a database
write-ahead logging protocol that was verified (with a pen-and-paper proof) in
FTCSL~\cite{ntzik:faults}; it is more sophisticated than GoTxn's write-ahead log
in that it can both undo and redo operations.

\tej{talk about jbd2}

\section{Related to DaisyNFS}

The Dafny side of DaisyNFS is a new implementation but its design and aspects of
the proof strategy were inspired by other verified file systems like
DFSCQ~\cite{chen:dfscq} (especially its indirect block implementation described
in Konradi's master's thesis~\cite{akonradi-meng}) and
Yggdrasil~\cite{sigurbjarnarson:yggdrasil}.

Isotope~\cite{shin:isotope} is a block-level transaction system similar to GoTxn
in its API, but without formal verification. Its logging design is based on
multi-version concurrency control (MVCC) rather than our use of pessimistic
locking. The Isotope paper develops a file system called IsoFS and two
persistent key-value stores; GoTxn is in principle also suitable to implement a
key-value store, but we have only used it in combination with DaisyNFS.\@ Isotope
is similar to DaisyNFS: it factors out isolation and atomicity to a transaction
system, making it simpler to implement a correct file system. Unlike GoTxn and
DaisyNFS, Isotope is still prone to subtle concurrency bugs in the transaction
system and bugs in the IsoFS code, whereas we use this split design to verify
both the transaction system and file-system code. One value of GoTxn is to
simply be more precise about what the transaction API does; Isotope has two
additional APIs to help tune the system, a \cc{mark_accessed} call to enable
sub-block concurrency and a \cc{please_cache} call that improves performance. It
seems the calling code must call \cc{mark_accessed} correctly for safety,
whereas the latter is only an optimization, but the precise requirements and
guarantees of Isotope are not clearly laid out.

AtomFS~\cite{zou:atomfs} is a verified, concurrent file system, but its
implementation does not store data durably. The proof structure is quite
different from DaisyNFS all of the concurrency reasoning is part of the
file-system operations and there is no interaction between persistence and
concurrency. AtomFS is verified using a relational logic with rely-guarantee
reasoning; unlike separation logic, the basic specification in this logic is a
refinement statement that relates an implementation to a (simpler) specification
program.

We chose to verify an NFS server because it is widely used in practice
and the expected behavior of NFS operations is well documented in
RFCs.  FUSE is an alternative for implementing file systems in user
space, but its operations have a less clear specification.

To be conducive to verification, \sys is implemented differently than
many NFS servers; in particular using two-phase locking is not common
practice.  Other user-level NFS servers are typically implemented on
top of an existing file system, relying on the underlying file system
for logging and locking. The Linux NFS server is implemented inside
the kernel using VFS and the ext3/ext4 file systems (if exporting an
ext3/ext4 file system).  Ext3 and ext4 use a journaling system, but
the file system and VFS layers perform locking.  WAFL~\cite{wafl:hitz}
is NFS appliance that provides snapshots and logs NFS requests to
NVRAM.  It has evolved its locking plan to obtain good
parallelism~\cite{curtis:wafl}.  Both the Linux NFS server and WAFL
are more complicated and have more features than \sys.

\resume
