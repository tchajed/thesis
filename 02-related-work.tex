While verifying programs is an old idea, going back at least to Floyd and
Hoare's work in the late 1960s~\cite{floyd:meanings,hoare:logic}, prior to this
thesis (in 2015) there was almost no work on reasoning about a program's
execution in the presence of a crash, let alone the combination of concurrency
and crashes. The full pipeline of systems verification is also quite new:
connecting the formal reasoning to executable programs, writing machine-checked proofs, and scaling up the techniques to
systems with sizable implementations.

The Perennial framework for reasoning about crashes and concurrency draws on two
lines of research: sequential crash safety verification and concurrency
verification, described in
\cref{sec:rel:crashes,sec:rel:concurrency,sec:rel:crashes-concurrency}. DaisyNFS
draws from two other lines of prior work, on verified transactions and file
systems, described in \cref{sec:rel:verified-fs,sec:rel:verified-txn}.

The most closely related work is Flashix, another verified concurrent and
crash-safe file system. \Cref{sec:rel:flashix} compares the two systems'
contributions and results. Finally, \cref{sec:rel:txn} discusses some related work
on using transactions as part of a file system.


\section{Foundations for verifying sequential crash safety}
\label{sec:rel:crashes}

There are a variety of foundational tools for reasoning about crash safety,
largely for sequential systems.

FSCQ~\cite{chen:fscq,chen:dfscq,hchen-phd} is a sequential file
system verified with Crash Hoare Logic (CHL). CHL's basic specifications have
the form $\hoareC{P}{e}{Q}{Q_{c}}$, extending the Hoare triple $\hoare{P}{e}{Q}$
with a \emph{crash condition} $Q_{c}$ that holds at all intermediate points in the
function's execution. Crash conditions handle a core difficulty of crashes,
namely that they stop the system at some intermediate step. There are two
remaining challenges: a crash wipes in-memory state, and the system might crash again
while recovering after a crash. CHL connects the system's crash conditions to a
specification for recovery to handle these two issues. A crash predicate
transformation captures what can be assumed when recovery starts, and for
crashes during recovery CHL requires that recovery be \emph{idempotent} in the
sense that its crash condition implies its own precondition. CHL was used to
specify and verify FSCQ~\cite{chen:fscq} and a more performant successor
DFSCQ~\cite{chen:dfscq}.

Yggdrasil~\cite{sigurbjarnarson:yggdrasil} takes a different approach to crash
safety. The basic definition is \emph{crash refinement}, which says that a
system implements an interface correctly, including a specification for what a
crash followed by recovery is allowed to do. Note that unlike CHL this
specification is about a collection of methods implementing an abstract,
specification transition system, not about individual methods. Yggdrasil uses
crash refinement to specify and verify a file system comparable to the file
system in xv6, a teaching operating system. The implementation uses Z3 to check
crash refinement, which the authors show is able to handle a system of this
complexity by breaking down the implementation into small enough layers.

Argosy~\cite{chajed:argosy}, which I led the development of but is not part of
this thesis, combines aspects of FSCQ and Yggdrasil. The key new idea is to
develop the metatheory for \emph{recovery refinement} that shows how systems
compose when both have recovery procedures --- what is non-trivial to handle is
that a crash in the composed recovery procedure requires starting over from the
beginning. Recovery refinement can be viewed as an extension of crash refinement
with verified metatheory for recovery, largely left implicit in the Yggdrasil
paper. Argosy also shows how to encode the conditions of recovery refinement
using CHL so that a single layer is verified using the CHL program logic.

VeriBetrKV~\cite{hance:veribetrkv} takes yet another approach to reasoning about
crashes, this time friendly to encoding in Dafny, a sequential verification
system with integrated support for programming and verification. The main idea
related to crash safety is to adopt the style from
IronFleet~\cite{hawblitzel:ironfleet} and think of a storage system as a
distributed system made up of the CPU and the storage device. The extension
needed for crash reasoning is to add a crash transition to the storage device
that non-deterministically wipes any buffered but unacknowledged writes, and
then to show that when this happens it corresponds to an appropriate
application-level transition modeling crashes (similar to the crash refinement
definition from Yggdrasil and recovery refinement in Argosy, although this is
associated with a code transition rather than a dedicated recovery procedure).
VeriBetrKV is used to verify a persistent key-value store based on
B\textsuperscript{$\epsilon$} trees, a data structure that also underlies
BetrFS~\cite{jannen:betrfs}.

Perennial has crash conditions that look similar to CHL's crash conditions,
albeit as part of a concurrent program logic rather than a sequential one. We do
carry out a refinement proof in the style of Argosy, which is similar to the
specification style in VeriBetrKV and Yggdrasil, but because it is connected to
concurrent reasoning the proof techniques are more sophisticated.

\section{Foundations for verifying concurrent programs}
\label{sec:rel:concurrency}

There are a number of approaches proposed to verifying concurrent programs, and
Iris in particular is the basis for Perennial. It
would be hard to do justice to the historical development of concurrency
reasoning. Looking at approaches that are actively used in research and
connected to executable implementation, two broad strategies are commonly used:
developing concurrent \emph{program logics}, and using \emph{refinement}-based
techniques.

Many refinement-based techniques are based on the idea of \emph{reduction},
which appeared originally in Lipton's theory of ``movers''~\cite{lipton:movers}.
The idea behind reduction is to reason about a program through program
transformations that show the program is equivalent to a simpler program. These
techniques can reason about concurrency by showing that a concurrent program is
equivalent to a program with sequential or atomic regions. These ideas have been
used as part of the CIVL verifier~\cite{hawblitzel:civl,kragl:civl-layers} and
Armada~\cite{lorch:armada}; my own prior work on CSPEC~\cite{chajed:cspec} was
also based on movers, before starting Perennial. Reduction-based techniques
generally reason about a program through a series of transformations, each
making the program slightly simpler, keeping the proof of each transformation's
correctness more manageable.

One large verified system, CertiKOS, is based on a custom verification
infrastructure called Certified Concurrent Abstraction
Layers~\cite{gu:certikos-ccal} which based on refinement but not reduction. This
work is notable for verifying a system (a simple operating system) not just at
an abstract protocol level but all the way down to concurrent code. The proof
composes with the CompCert compiler correctness theorem to carry the guarantees
down to the assembly code of the operating system.

Program logics are an alternative approach based on giving specifications to
each function in the program, within a logic that has useful rules for proving
and composing specifications. Hoare logic is a classic program
logic for sequential programs that based on pre- and post-conditions.
Concurrency makes it harder to construct a logic that can usefully reason about
many concurrency patterns (completeness) while also giving specifications that
hold in the presence of concurrent threads (soundness). One productive line of
work has been based on concurrent separation logic (CSL)~\cite{brookes:csl}.

The Verified Software Toolchain is notable for connecting a CSL-based logic
(VST-Floyd) down to proofs of C code, including a connection to CompCert to
carry these guarantees down to assembly~\cite{cao:vst-floyd}. It has been used
for a number of sequential verified C programs and recently to some shared
memory C libraries.

Microsoft VCC~\cite{cohen:vcc,cohen:vcc-local} is another verification system
that is based on annotating C code with pre- and post-conditions and so-called
two-state invariants that are similar to rely-guarantee
reasoning~\cite{jones:rg,feng:lrg}. VCC is typically used to encode a form
of concurrent refinement reasoning while also using specifications similar to
that of a program logic. The system was used to verify a portion of Hyper-V. The
system is implemented as a verification-condition generator; unlike the program
logics described in this section, VCC has no soundness argument (even on paper)
to specify what its specifications mean and argue that it enforces the right
conditions for that meaning to hold.

Perennial builds directly on Iris~\cite{jung:iris-jfp}. Iris is a general
framework for concurrency, featuring a base logic with key features for
concurrency (step indexing and separation-logic resources, including
\emph{higher-order} resources used to define invariants for example) and a
program logic built on the base logic. This decomposition was valuable for
Perennial because we made two core changes to Iris: first, the program logic
itself is extended with crash safety reasoning, and second, the framework is
applied to our GooseLang models of Go code. In both cases the generality of Iris
was valuable, as the framework is not tied to reasoning about a particular
programming language or even to its usual program logic. Prior to this work,
Iris had not typically been used for reasoning about executable code (though
projects like RefinedC~\cite{sammler:refinedc} and our own work on Goose are
changing that).

\section{Reasoning about crashes and concurrency}
\label{sec:rel:crashes-concurrency}

Program logics other than Perennial have been developed for formal reasoning
about concurrent, crash-safe systems. Fault-Tolerant Concurrent Separation Logic
(FTCSL)~\cite{ntzik:faults} extends the Views~\cite{dinsdale:views} concurrency
logic to incorporate crash-safety. POG~\cite{raad:pog} is a program logic for
reasoning about the interaction of x86-TSO weak-memory consistency and
non-volatile memory. Both of these logics are only defined on paper, and do not
support mechanized proof. Perennial goes beyond their reasoning principles: it
includes ownership reasoning and its interaction with crashes, and a style of
logically atomic crash specifications for modularly specifying libraries and
composing their proofs. FTCSL and POG have no mechanism of modular proofs of
layers, which we found essential to scale verification to a file system.

In the case of x86-TSO with persistence, POG required a line of research to
define the semantics at the
ISA level, validating this semantics against the hardware and in conversations
with Intel engineers~\cite{raad:px86,raad:px86-extended}. A similar semantics effort defines the
semantics of ext4 under crashes, but without an accompanying program
logic~\cite{kokologiannakis:persevere}. It would be an interesting direction for
future work to combine the persistency semantics of x86 with Perennial, to
verify libraries that use non-volatile memory.

\section{Verified transaction systems}
\label{sec:rel:verified-txn}

As far as we know, GoTxn is the first transaction system that makes data durable
and has a verified implementation. There is much related work on verifying
aspects of transaction systems with and without durability, and on unverified
transaction systems.

\citet{ChkliaevHS99} verify serializability of two-phase locking and other
transaction concurrency control mechanisms in the PVS theorem prover. Their
proof formalizes two-phase locking as an abstract protocol consisting of
sequences of read, write, and locking operations, as opposed to a concrete
implementation as in GoTxn. \citet{pollak-2PL} uses a variant of the
CAP separation logic~\citep{dinsdale:cap} to give a pen-and-paper
proof of serializability for an abstract, protocol-level description of
two-phase locking, connecting the protocol to atomic reasoning about
transactions.

\citet{mohsen:stm} developed a framework for verifying software transactional memory algorithms, modeled
as I/O automata. They applied their framework to sophisticated STM algorithms, such as
NOrec algorithm~\cite{dalessandro:norec}. The STM algorithms considered do not
handle persistence, so the framework does not address crash-safety reasoning.

A specification called the Push/Pull model of
transactions~\cite{koskinen:pushpull} is similar to the \emph{lifting} technique
in the journal system's specification~(\cref{sec:txn:lifting}) --- the core
problem addressed is that a journal operation atomically modifies a small number
of objects, but other objects can change between the start of the operation and when
it commits. The Push/Pull model also discusses reasoning on top of the
specification, using Lipton's reduction~\cite{lipton:movers} rather than
separation-logic ownership to handle concurrency. However that work is about
on-paper specifications and proofs, while we also prove an implementation meets
our specification and proved DaisyNFS on top.

DFSCQ~\cite{chen:dfscq} verifies a high-performance file system built on top of
a logging system with asynchronous disks and log-bypass writes, which are
challenging optimizations that GoTxn does not support. ARIES is a database
write-ahead logging protocol that was verified (with a pen-and-paper proof) in
FTCSL~\cite{ntzik:faults}; it is more sophisticated than GoTxn's write-ahead log
in that it can both undo and redo operations.

An important but unverified journaling system is jbd2, the ``journaling block
device'' that underpins ext3 and ext4 in Linux. The design of jbd2 is broadly
similar to that of GoTxn's journaling layer; GoTxn goes one step further and
also has automatic locking for transactions. One difference in the interfaces of
GoTxn and jbd2 is that in jbd2, transactions can reserve space ahead of time.
This has the benefit that a transaction can start writing to the journal on disk
while it is being prepared, potentially improving performance, but it also means
that a concurrent transaction must wait for all previously started transactions
to finish before becoming persistent, since a journaled operation can only be
logically completed when all prior operations are finished. This is a reasonable
tradeoff for a file system since transactions are generally short-lived, but it
sacrifices a bit of tail latency when an operation must be persisted, for
example when an application issues an \cc{fsync()}.

\section{Verified file systems}
\label{sec:rel:verified-fs}

The Dafny side of DaisyNFS is a new implementation but its design and aspects of
the proof strategy were inspired by other verified file systems like
DFSCQ~\cite{chen:dfscq} (especially its indirect block implementation described
in Alex Konradi's master's thesis~\cite{akonradi-meng}) and
Yggdrasil~\cite{sigurbjarnarson:yggdrasil}.

AtomFS~\cite{zou:atomfs} is a verified, concurrent file system, but its
implementation does not store data durably. The proof structure is quite
different from DaisyNFS in that all of the concurrency reasoning is part of the
file-system operations and there is no interaction between persistence and
concurrency. AtomFS is verified using a relational logic with rely-guarantee
reasoning; unlike separation logic, the basic specification in this logic is a
refinement statement that relates an implementation to a (simpler) specification
program.

We chose to verify an NFS server because it is widely used in practice and the
expected behavior of NFS operations is well documented in RFCs. FUSE is an
alternative for implementing file systems in user space that was used for the
verified file systems mentioned above, but its operations have a less clear
specification. FUSE also increases the trusted code to include both the FUSE
kernel component and the user-space library that connects the implementation to
the kernel.

\section{The Flashix file system}
\label{sec:rel:flashix}

The Flashix project deserves special attention since it also develops a verified
concurrent and crash-safe file system~\cite{bodenmuller:concurrent-flashix}.
Flashix targets flash storage, a lower-level storage technology than the drives
that DaisyNFS targets. Crash and concurrency reasoning in Flashix is based based
on refinement, in particular using Lipton's theory of
reduction~\cite{lipton:movers}, extended with conditions for crash
safety~\cite[\S 13.3]{Pfaehler2018}. Flashix is implemented using abstract data
types in a high-level language; a code generator transforms this code into
executable C, but this process is both not verified and has difficulty producing
the most efficient code using in-place updates.

The crash-safety aspects of Flashix's refinement approach are implemented in the
KIV theorem prover as axioms --- that is, the system generates proof obligations
based on the theory for a program being verified, but the connection between the
obligations and the desired refinement property is given by an on-paper proof.
The Perennial program logic on the other hand is implemented in Coq with an
associated soundness theorem that allows to extract a final theorem that is a
simple statement about executions. Flashix reasons about crashes in both the
journaling and file-system layers, whereas DaisyNFS factors out this reasoning
to GoTxn and then layers sequential proofs on top.

The concurrency in Flashix is a bit different from DaisyNFS. At the file-system
layer, the system has a reader-writer lock over the entire directory structure,
and additionally supports concurrent writes to different files. Internally, the
system can also perform garbage collection and erase-block management
concurrently with file-system operations. DaisyNFS in contrast can concurrently
write to distinct files or directories, and concurrently installs from the
write-ahead log to the data region, but does not support read-read concurrency
to different directories. Flashix achieves comparable performance to UBIFS (an
existing flash file system in Linux)~\cite{bodenmuller:concurrent-flashix}, and
DaisyNFS achieves comparable performance to the Linux kernel NFS server.
Comparing the performance of both systems to each other is difficult, since they
run on different types of hardware. The Flashix benchmarks do not evaluate
scalability to many cores, focusing on the impact of concurrent garbage
collection.

%The Flashix project reports that components were ``intertwined'' and could not be
%separated as cleanly for reasoning purposes as they would have
%liked~\cite[\S 8]{bodenmuller:concurrent-flashix}.
%We had a similar experience within the transaction system, where
%performance constrained the APIs of internal layers and forced us to export
%complicated APIs. Interfaces in GoTxn were perhaps trickier than the components of Flashix
%because Goose models the implementation at a lower level of abstraction, so even issues like
%concurrency and memory safety show up in each interface. We were inspired to
%develop the DaisyNFS design to get truly sequential reasoning from the
%experience of working in Perennial compared to automated verification in \fstar
%and Dafny, and found modularity and clean abstractions much easier once
%code ran within a transaction.

Flashix verifies a taller
storage stack than DaisyNFS. Flash storage has a more limited API than a
standard drive --- in particular flash blocks must be completely erased before
being reused. A flash file system works around this limitation with \emph{garbage
collection}, where occasionally the system identifies a mostly-unused block,
moves its valid data elsewhere, and then erases it to reclaim space. This is
implemented by maintaining a logical-to-physical block mapping, similar to
virtual memory. DaisyNFS does not require any of this code, since it runs on
devices that internally implement these features, though Perennial could be used
to model flash storage and verify the code that interacts with it directly.


\section{Transactions to simplify system design}
\label{sec:rel:txn}

To be conducive to verification, DaisyNFS is implemented differently than
many NFS servers; in particular using two-phase locking is not common
practice.  Other user-level NFS servers are typically implemented on
top of an existing file system, relying on the underlying file system
for logging and locking. Ext3 and ext4 use a journaling system underneath, but
the file system and VFS layers perform locking. This locking is inherited when
these file systems are exported with the Linux NFS server. WAFL~\cite{wafl:hitz}
is an NFS appliance that provides snapshots and logs NFS requests to
NVRAM.\@  It has evolved its locking plan to obtain good
parallelism~\cite{curtis:wafl}.  Both the Linux NFS server and WAFL
are more complicated and have more features than DaisyNFS.\@

Isotope~\cite{shin:isotope} is a block-level transaction system similar to GoTxn
in its API, but without formal verification, which was used to implement a file
system called IsoFS. Its logging design is based on
multi-version concurrency control (MVCC) rather than our use of pessimistic
locking. The Isotope paper uses Isotope not only for the IsoFS file system but
also two persistent key-value stores. While GoTxn is in principle also suitable to implement a
key-value store, we have so far only used it in combination with DaisyNFS.\@

IsoFS has a similar design to DaisyNFS: it factors out isolation and atomicity to the transaction
system, making it easy to handle crashes and concurrency. Unlike GoTxn and
DaisyNFS, Isotope is still prone to subtle concurrency bugs in the transaction
system and bugs in the IsoFS code, whereas DaisyNFS uses the split design to
verify both the transaction system and the transactions themselves. The transaction API provided
by Isotope has an interesting performance optimization that GoTxn could support:
the API includes a \cc{please_cache} call that keeps an address in the
transaction system's cache, used for small but frequently-accessed metadata like
the allocator state in the file system.
