This thesis describes an approach to verify software with a combination of
concurrency in the implementation and crash-safety guarantees, applied to the
DaisyNFS file system. It spans from general verification foundations
through the design and proof of the file system itself. The foundations
include Perennial, a program logic for crashes and concurrency, and Goose, an
approach for reasoning about Go code. DaisyNFS is designed around a verified
transaction system called GoTxn, which makes it feasible to scale verification
by enabling sequential reasoning for transactions on top.

\section{What is the value of verification?}

The reader might wonder about the broader value of verification and the
contribution of the thesis. This section makes some broader points about the
value of verification, both in general and in the specific case of this thesis.

\paragraph{Verification is valuable for critical systems.} It will never be cost
effective to verify all software; at a minimum, for verification to be useful a
piece of software needs a specification that is more likely to be correct than
the implementation.
Instead, the aim is to use verification for critical systems, \emph{critical}
because failures are especially bad, and \emph{systems} that have well-defined
guarantees to make to other systems and applications running on top. It is also
helpful in the cost-benefit tradeoff if the system is part of a ``narrow waist''
used by many other pieces of software, since then the impact of bugs is felt to
a greater extent. Finally, alongside the need for a specification it is also
important that the verification respond to changes in both specification and
implementation. If both change rapidly and the proofs cannot be re-done, then
verification won't keep up with the pace of development. File systems fit all of
these criteria: bugs can lead to data loss which the application cannot
mitigate, the interface is stable and relatively standard, and essentially all
applications go through the file system to store persistent state.

\paragraph{Cost versus benefit.}
I think about verification in terms of whether the benefits in terms of
reliability outweigh the costs of writing the proofs. Research in verification
generally aims to reduce the cost, or to identify areas with particularly large
benefits. This thesis in particular focuses on making verification of crash-safe
and concurrent systems possible, in order to give an upper bound on the cost.
Further research would hopefully bring the costs down so that verification is an
appealing approach for the next generation of storage systems.

A real achievement for verification would be to use verification not just as
alternative to the practices above, but to build something with more daring optimizations,
features, or speed than would otherwise be possible; this would increase the
benefits rather than decreasing the cost. Storage systems that holds persistent
state have the potential to be just such an application. There aren't that many
widely-used
file systems --- most people use one of ext4, btrfs, or XFS on Linux, NTFS on
Windows, and APFS on macOS --- and new file systems are generally adopted
slowly. APFS was surprising for having only a 3-year development period before
Apple widely deployed it. Ext4 is the next newest of that list, introduced in
2008 (by extending ext3, which was first released in 2001). Verification has the
potential to create a file system that is more rapidly adopted by virtue of its
proof. This might be especially applicable for some new hardware, like a file
system for persistent memory or new zoned storage (ZNS) SSDs.

% ext4: 2008
% ext3: 2001
% XFS: 1994 (first released on IRIX OS for SGI)
% NTFS: 1993
% APFS: 2017

\paragraph{Mechanized proofs can be adapted to change.} Informal
reasoning even with a program logic can gain much of the value of formal
reasoning in that it helps the author think systematically, and permits varying
the level of detail to suit the author's needs. However, a complete and
mechanized proof is especially valuable when it comes to \emph{changing} the
code and proof: it is difficult and tedious to make a systematic change to an
on-paper proof and be confident that all relevant parts of the proof have been
re-analyzed and updated.

In contrast, a mechanized proof can be updated and re-checked, particularly in
an automated tool. In developing the DaisyNFS Dafny proof, I had a wonderful
experience with adapting a proof when adding indirect blocks: at some point a
function verified before I had even understood if or why it was correct!
Adapting existing proofs in an interactive theorem prover is more challenging
than with automated tools (although there is a relatively new field of \emph{proof
repair}~\cite{ringer:proof-repair} that aims to address this), and we found
adapting the GoTxn proofs in Perennial to be more challenging.
\Cref{sec:eval:incremental} talks about incremental changes in the work
described by this thesis in particular.

\paragraph{Verification helps discover accurate, precise specifications.} Some
systems are extremely reliable, due to extensive testing, real-world usage, and
development, but they still lack a clear description of the interface exposed.
The process of verification helps discover a precise, mathematical
specification, and the proofs confirm that this specification is actually an
accurate description of the implementation. Precise and accurate specifications
are useful even for unverified software. While documentation is a helpful form
of informal specification, mathematical specifications have the advantage of
reducing ambiguity.

One example from the work in this thesis came up in the GoTxn write-ahead log
specification. When we came up with the idea of a history of multiwrites as the
specification, we able to explain absorption, where a value overwrites an older
write that hasn't been logged yet, as an optimization and not a detail the
caller should be concerned with. Merely writing down the specification also
helped clarify some of the internal invariants. Another example was some
difficulty we ran into interpreting the description of ``weak-cache consistency
(WCC)'' metadata in the NFS protocol. The RFC explains this aspect in several
places and combines describing what information should be returned, rationalizes
the need for this feature, and describes how the client should use it. A
mathematical description teases these apart so that the first is formally
described and the rest are commentary on top. The end result was quite simple:
the server should return both the old attributes and final attributes of
whatever file or directory the WCC data pertains to. Because these attributes
include modification timestamps, they permit the client to invalidate their
local cache when the old timestamp doesn't match the client's cached value.

\paragraph{Verification guides debugging.} With a verified system, bugs are
still inevitable since there is unverified code surrounding the verified code,
the assumptions of the proof can be violated, and the specification can be
wrong. However, an advantage of formal verification, particularly fully
machine-checked proofs, is that when bugs are discovered it's safe to start
debugging with all the code outside the verification as well as looking at the
specification. We ran into several bugs while developing DaisyNFS; several are
described in \cref{sec:eval:testing}. Some were due
to incorrect specifications; for example, at one point we returned the wrong
``before'' WCC data, causing Linux to constantly invalidate its cache and send
extra RPCs and reducing performance. Others were due to unverified code
violating the assumptions in the verified code.

% \paragraph{Interactive theorem proving is a great way to prototype.} I do not
% anticipate that busy engineers trying to write production code will write proofs
% in Coq, much less develop a custom program logic. However, this workflow is
% excellent for prototyping the right reasoning principles, some reasons for which
% are outlined above. Working out proofs interactively gives a good feel for how
% the logic works, from which automation can be designed bit by bit (some of it as
% Coq automation itself). In Perennial (as in Iris), proof steps are at a
% sufficiently high level of abstraction to make the proofs doable for prototyping
% purpose, even though it has no search-based automation, sophisticated
% algorithms, or use of an SMT solver.


\paragraph{Mechanized proofs help develop a sound logic.} Within verification
there is a spectrum of formality that includes ``foundational'' techniques like
Perennial with a soundness theorem as well as tools like Dafny that generate
verification conditions for a solver but no proof of soundness. When developing
a new program logic, one reason to go through the trouble of mechanizing the
proofs and carrying out a soundness proof is to validate the program logic
itself. For example, the formalism helps work out a specification for crash
safety and concurrency, one that the mechanized proof shows has the right
meaning (due to the soundness theorem) and is usable (due to the verified
reasoning principles in the logic). After gaining confidence and intuition with
the safety-net of a soundness proof, it is possible to make further progress
with informal reasoning, or by implementing the logic in an automated system.

Concurrency makes soundness especially challenging and important, since specifications can be quite
subtle to use. One recent example is prophecy variables, where in the process of
developing a mechanized proofs Jung et al.\ found an unsoundness in the use of
prophecy variables in a proof from Viktor Vafeiadis's PhD
thesis~\cite{jung:prophecy,vafeiadis-phd}, a proof that was conducted informally
and on-paper. Higher-order reasoning, especially the ability to store procedures
in the heap, has also led to some subtle reasoning principles like the
anti-frame rule~\cite{pottier:anti-frame}, whose soundness proof was rather
sophisticated~\cite{schwinghammer:semantic-anti-frame}.


\section{Discussion}

\paragraph{Memory safety at interface boundaries.}
A surprisingly difficult aspect of the proof was addressing memory safety
considerations while describing interfaces.
DaisyNFS has two important interface boundaries, one between GooseLang and the
disk, and another to describe the GoTxn interface. Both APIs involve data in the
form of byte slices. The challenging aspect of using bytes in a method is
specifying how \emph{ownership} transfers. For example, the \cc{DiskWrite(a, v)}
operation passes a buffer \cc{v} to the disk. It could be that ownership of
\cc{v} needs to be transferred to the disk, since it uses the buffer later, or
that \cc{v} should be read-only for the duration of the call and ownership is
returned to the caller, or that if \cc{v} is permanently read-only the disk and
caller can share the buffer.

Expressing ownership as a logical idea in the logic is relatively easy using
separation logic, but the semantics of \cc{DiskWrite(a, v)} has to be
axiomatized since it is part of an external interface. In order to simplify expressing the ownership transfer, the
GooseLang \cc{DiskRead} returns a freshly allocated buffer, but it would have
been better for performance if the caller supplied a buffer that the
\cc{DiskRead} wrote to (as in the usual \cc{read} system call). Similarly, the
GoTxn interface to Dafny makes additional copies to simplify ownership
reasoning, especially in the transaction refinement proof. A better solution would
have been to develop a specification style for expressing ownership in the
semantics of the operations themselves, along with associated reasoning
principles.

Rust would assist with better handling of ownership in interfaces, but does
not completely solve the problem. Where it would fit in is that if an interface
makes \emph{assumptions} about ownership transfer, Rust would help in enforcing
those assumptions at compile time. While convenient for the verified code to
know if ownership is violated before attempting a proof, this would be
especially good for enforcing that code calling into a verified interface
respects its assumptions about ownership. What Rust doesn't solve is that it is
still necessary to express ownership assumptions in the interface, a challenge
independent of the implementation language.

\paragraph{Read-only sharing.}
Another challenge, related to memory safety, was reasoning about read-only
sharing in the internals of GoTxn. Changing data structures after they were written to support read-only
sharing was difficult, since every specification needs to incorporate fractional
permissions. Read-only locking was challenging to retrofit support for, even
though it may have improved read-read concurrency in GoTxn and DaisyNFS. This
also showed up in the data structure for the in-memory log, which needs to share
read-only data blocks between threads issuing reads to the write-ahead log, the logger
thread, and the installer thread.

\paragraph{Modular proofs.}
\Cref{ch:crash-logatom} describes a style for specifying a library in Perennial.
Modularity was essential to enable the GoTxn proof. Better support for
modularity, perhaps formalizing some of the aspects of the specification style,
would have more cleanly separated each library's proofs. Where this is
particularly important is in making changes to the code that affect an
interface, in which case it can be difficult to tell from the code exactly what
properties the caller is assuming about the interface. The specification style
was developed in parallel with all the proofs, which means that proofs do not
all follow the best practices developed along the way.

\section{Future work}

\paragraph{Apply the approach to Rust or C.} It would be interesting to port
the Goose approach to Rust or C. This would not necessarily be for better
performance but as a first step towards better systems integration. For example,
Rust or C would be more practical for verifying critical parts of the Linux
kernel. An important challenge for verifying in-kernel code would be modeling
the specifications, both those assumed by the verified code and what is promised
to the rest of the kernel.

\paragraph{Asynchronous model of the disk.} The disk model in Goose assumes
writes are durable as soon as the write returns, but real disks typically buffer
writes internally. Goose has a new asynchronous disk model, used for an
independent example, but it would be good future work to change GoTxn to use
this new model. Above the write-ahead log this should have no effect on the
specifications. Going beyond asynchronous durability, it would also be
interesting to model a different interface entirely like that of libaio where
requests are submitted to a queue and completions are delivered separately.

\paragraph{Improve the write-ahead log.} After implementing GoTxn, we made
relatively few changes, as reported in \cref{sec:eval:incremental}. Several
improvements to the write-ahead log would be interesting to make, which were
challenging due to the complexity of the existing proof.

First, it would be worth experimenting to see if the proof of installation could
be separated from the rest of the proof. The important task would be identifying
the right specification for the logging code which is enough to reason about the
combination of installation and logging.

Second, due to the future dependency in the write-ahead log's \cc{Read}
(described in \cref{sec:txn:wal}), it is specified as two separate
operations; it would be interesting to add \emph{prophecy variables} to
Perennial, similar to the Iris implementation~\cite{jung:prophecy}, and give
\cc{Read} a single linearizable specification.

Finally, the write-ahead log uses physical logging where all updates represent
full-block overwrites. It would be interesting to add more sophisticated
operations like sub-block updates that are natively supported by the write-ahead
log (rather than by the layer above). More ambitiously, it would be interesting
to verify \emph{logical logging} where updates represent high-level operations
like a file write. Logical logging would require a more sophisticated
specification and proof because the write-ahead log's entries would be
interpreted by the caller, resulting in a higher-order specification where the
caller passes a function which might need to be run during recovery, and
multiple times.

\paragraph{Verify a file system directly on top of GoJournal.} Dafny allows
the DaisyNFS proof to take advantage of automation that is enabled by sequential
reasoning. It would be interesting to do a direct comparison against a proof in
Perennial where due to ownership the proofs would still require only sequential
reasoning. The GoJournal paper reports verifying a subset of a file system
directly~\cite{chajed:gojournal}, but we never attempted to apply the approach
to a full-fledged file system. What would be especially interesting is if the
Perennial proof started with the Dafny proof's structure and invariants; perhaps
these invariants (developed under the constraints of Dafny and for
automation-friendliness) would also be useful in an interactive theorem proving
setting.

\paragraph{Do more with the NFS specification.} The NFS specification for
DaisyNFS is hand-written and integrated with the code. It would be an
interesting project in its own right to turn this into a complete, well-tested
formalization of the RFC. For example, the Dafny specification could make a serious
attempt to capture all the non-determinism allowed by the RFC. As a standalone
artifact the specification could also be used to test servers (comparing to the
allowed behaviors). An executable version of the specification (though perhaps
inefficient and not durable) would be valuable to test the specification itself
and explore what clients do when faced with different server behavior.
