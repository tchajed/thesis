This thesis describes an approach to verify software with a combination of
concurrency in the implementation and crash-safety guarantees. The approach is applied to the
DaisyNFS file system. The work spans from general verification foundations
through the design and proof of the file system itself. The foundations
include Perennial, a program logic for crashes and concurrency, and Goose, an
approach for reasoning about Go code. DaisyNFS is designed around a verified
transaction system called GoTxn, which makes it feasible to scale verification
by enabling sequential reasoning for the transactions that implement the
file-system operations.

\section{Lessons learned about verification}

In the process of conducting this research, we made some broader
observations about verification that this section shares.

% \paragraph{Verification is valuable for critical systems.} It will never be cost
% effective to verify all software; at a minimum, for verification to be useful a
% piece of software needs a specification that is more likely to be correct than
% the implementation.
% Instead, the aim is to use verification for critical systems, \emph{critical}
% because failures are especially bad, and \emph{systems} that have well-defined
% guarantees to make to other systems and applications running on top. It is also
% helpful in the cost-benefit tradeoff if the system is part of a ``narrow waist''
% used by many other pieces of software, since then the impact of bugs is felt to
% a greater extent. Finally, alongside the need for a specification it is also
% important that the verification respond to changes in both specification and
% implementation. If both change rapidly and the proofs cannot be re-done, then
% verification won't keep up with the pace of development. File systems fit all of
% these criteria: bugs can lead to data loss which the application cannot
% mitigate, the interface is stable and relatively standard, and essentially all
% applications go through the file system to store persistent state.

% \paragraph{Mechanized proofs can be adapted to change.} Informal
% reasoning even with a program logic can gain much of the value of formal
% reasoning in that it helps the author think systematically, and permits varying
% the level of detail to suit the author's needs. However, a complete and
% mechanized proof is especially valuable when it comes to \emph{changing} the
% code and proof: it is difficult and tedious to make a systematic change to an
% on-paper proof and be confident that all relevant parts of the proof have been
% re-analyzed and updated.
%
% In contrast, a mechanized proof can be updated and re-checked, particularly in
% an automated tool. In developing the DaisyNFS Dafny proof, I had a wonderful
% experience with adapting a proof when adding indirect blocks: at some point a
% function verified before I had even understood if or why it was correct!
% Adapting existing proofs in an interactive theorem prover is more challenging
% than with automated tools (although there is a relatively new field of \emph{proof
% repair}~\cite{ringer:proof-repair} that aims to address this), and we found
% adapting the GoTxn proofs in Perennial to be more challenging.
% \Cref{sec:eval:incremental} talks about incremental changes in the work
% described by this thesis in particular.

\paragraph{Verification helps discover accurate, precise specifications.} Some
systems are extremely reliable, due to extensive testing, real-world usage, and
development, but they still lack a clear description of the interface exposed.
The process of verification helps discover a precise, mathematical
specification, and the proofs confirm that this specification is actually an
accurate description of the implementation. Precise and accurate specifications
are useful even for unverified software. While documentation is a helpful form
of informal specification, mathematical specifications have the advantage of
reducing ambiguity.

One example from the work in this thesis came up in the GoTxn write-ahead log
specification. When we came up with the idea of a history of multiwrites as the
specification, we able to explain absorption, where a value overwrites an older
write that hasn't been logged yet, as an optimization and not a detail the
caller should be concerned with. Merely writing down the specification also
helped clarify some of the internal invariants. Another example was some
difficulty we ran into while interpreting the description of ``weak-cache consistency
(WCC)'' metadata in the NFS protocol. The RFC explains this aspect in several
places and combines describing what information should be returned, rationalizing
the need for this feature, and describing how the client should use it. A
mathematical description teases these apart so that the first is formally
described and the rest are commentary on top. The end result was quite simple:
the server should fetch and return the old attributes along with the post-operation attributes of
whatever file or directory the WCC data pertains to. Because these attributes
include modification timestamps, they permit the client to invalidate their
local cache when the old timestamp doesn't match the client's cached value.

\paragraph{Verification guides debugging.} With a verified system, bugs are
still inevitable since there is unverified code surrounding the verified code,
the assumptions of the proof can be violated, and the specification can be
wrong. However, an advantage of formal verification, particularly fully
machine-checked proofs, is that when bugs are discovered it's safe to start
debugging by investigating the code \emph{outside} the verification, including
carefully looking at the
specification. We ran into several bugs while developing DaisyNFS; several are
described in \cref{sec:eval:testing}. Some were due
to incorrect specifications; for example, at one point the \cc{CREATE} and
\cc{MKDIR} specs did not forbid empty names. Others were due to unverified code
violating the assumptions in the verified code.

\paragraph{Verification as an enabler.}
A real achievement for verification would be to use verification not just as
alternative to testing, but to build something with more daring optimizations,
features, or speed than would otherwise be possible. Storage systems that holds persistent
state have the potential to be just such an application. There aren't that many
widely-used
file systems --- most people use one of ext4, btrfs, or XFS on Linux, NTFS on
Windows, and APFS on macOS --- and new file systems are generally adopted
slowly. APFS was surprising for having only a 3-year development period before
Apple widely deployed it. Ext4 is the next newest of that list, introduced in
2008 (by extending ext3, which was first released in 2001). Verification has the
potential to create a file system that is more rapidly adopted by virtue of its
proof. This might be especially applicable for some new hardware, like a file
system for persistent memory or new zoned storage (ZNS) SSDs.

% ext4: 2008
% ext3: 2001
% XFS: 1994 (first released on IRIX OS for SGI)
% NTFS: 1993
% APFS: 2017

While GoTxn and DaisyNFS don't have any radical optimizations, this observation
did come up in our work. Making the logger and installer concurrent complicated
the implementation and informal reasoning, and verification gave confidence that this was implemented correctly.
Another example was the change Mark made to the installer, removing additional
absorption; normally such a change would be considered subtle and would require
thinking carefully about whether the code is still
correct. Mark made the change and verified it within a couple weeks, and we
could accept the new code readily once the proof was complete.

\paragraph{Choosing what to verify carefully.}
In prior work on verified file systems, we typically exposed the file system via
Filesystem in Userspace (FUSE). This path had two problems that switching to
NFS solved. First, FUSE can be inefficient~\cite{vangoor:fuse}, adding overhead
before even getting to the underlying file system's performance. Second, the API
that FUSE file systems implement is not exactly the same as the POSIX or Linux
APIs, so it can be challenging to know what the right specification for the file
system should be and how this relates to what clients observe.

The lesson we learned here is that it is important to carefully choose what part
of the system to verify, thinking about how it interfaces with the rest of the
world. Both NFS and FUSE require unverified code to take system calls and invoke
the server or userspace file system respectively, but NFS has the important
advantage that the interface the server exposes is written down in the form of
an RFC.

% \paragraph{Interactive theorem proving is a great way to prototype.} I do not
% anticipate that busy engineers trying to write production code will write proofs
% in Coq, much less develop a custom program logic. However, this workflow is
% excellent for prototyping the right reasoning principles, some reasons for which
% are outlined above. Working out proofs interactively gives a good feel for how
% the logic works, from which automation can be designed bit by bit (some of it as
% Coq automation itself). In Perennial (as in Iris), proof steps are at a
% sufficiently high level of abstraction to make the proofs doable for prototyping
% purpose, even though it has no search-based automation, sophisticated
% algorithms, or use of an SMT solver.


%% this sub-section is perhaps the most defensive of all of these points
% \paragraph{Mechanized proofs help develop a sound logic.} Within verification
% there is a spectrum of formality that includes ``foundational'' techniques like
% Perennial with a soundness theorem as well as tools like Dafny that generate
% verification conditions for a solver but no proof of soundness. When developing
% a new program logic, one reason to go through the trouble of mechanizing the
% proofs and carrying out a soundness proof is to validate the program logic
% itself. For example, the formalism helps work out a specification for crash
% safety and concurrency, one that the mechanized proof shows has the right
% meaning (due to the soundness theorem) and is usable (due to the verified
% reasoning principles in the logic). After gaining confidence and intuition with
% the safety-net of a soundness proof, it is possible to make further progress
% with informal reasoning, or by implementing the logic in an automated system.
%
% Concurrency makes soundness especially challenging and important, since specifications can be quite
% subtle to use. One recent example is prophecy variables, where in the process of
% developing a mechanized proofs Jung et al.\ found an unsoundness in the use of
% prophecy variables in a proof from Viktor Vafeiadis's PhD
% thesis~\cite{jung:prophecy,vafeiadis-phd}, a proof that was conducted informally
% and on-paper. Higher-order reasoning, especially the ability to store procedures
% in the heap, has also led to some subtle reasoning principles like the
% anti-frame rule~\cite{pottier:anti-frame}, whose soundness proof was rather
% sophisticated~\cite{schwinghammer:semantic-anti-frame}.


\section{Discussion}

\paragraph{Memory safety at interface boundaries.}
A surprisingly difficult aspect of the proof was addressing memory safety
considerations while describing interfaces.
DaisyNFS has two important interface boundaries, one between GooseLang and the
disk, and another to describe the GoTxn interface. Both APIs involve data in the
form of byte slices. The challenging aspect of using bytes in a method is
specifying how \emph{ownership} transfers. For example, the \cc{DiskWrite(a, v)}
operation passes a buffer \cc{v} to the disk. It could be that ownership of
\cc{v} needs to be transferred to the disk, since it uses the buffer later, or
that \cc{v} should be read-only for the duration of the call and ownership is
returned to the caller, or that if \cc{v} is permanently read-only the disk and
caller can share the buffer. It is important for the proof to be sound that the
correct ownership discipline is enforced.

Expressing ownership as a logical idea in the logic is relatively easy using
separation logic, but the semantics of \cc{DiskWrite(a, v)} has to be
given operationally and not just given as a specification. In order to simplify expressing the ownership transfer, the
GooseLang \cc{DiskRead} returns a freshly allocated buffer, but it would have
been better for performance if the caller supplied a buffer that the
\cc{DiskRead} filled (as in the usual \cc{read} system call). Similarly, the
GoTxn interface to Dafny makes additional copies to simplify ownership
reasoning, especially in the transaction refinement proof. A better solution would
have been to develop a specification style for expressing ownership in the
semantics of the operations themselves, along with associated reasoning
principles.

Rust would assist with better handling of ownership in interfaces, but does
not completely solve the problem. Where it would fit in is that if an interface
makes \emph{assumptions} about ownership transfer, Rust would help in enforcing
those assumptions at compile time. It is convenient for the verified code to
know if ownership is violated before attempting a proof, but automatic
enforcement would be even more important for ensuring that the code calling into
a verified interface respects its ownership assumptions.
What Rust doesn't solve is that it is
still necessary to express ownership assumptions in the interface's semantics, a challenge
independent of the implementation language.

\paragraph{Read-only sharing.}
Another challenge was reasoning about read-only
sharing in the internals of GoTxn. Changing data structures after they were written to support read-only
sharing was difficult, since every specification needs to incorporate fractional
permissions. As a result read-only locking in GoTxn would be challenging to retrofit support for, even
though it might improve read-read concurrency in GoTxn and DaisyNFS.\@ This challenge
also shows up in the data structure for the in-memory log, which needs to share
read-only data blocks between threads issuing reads to the write-ahead log, the logger
thread, and the installer thread.

\paragraph{Modular proofs.}
\Cref{ch:crash-logatom} describes a style for specifying a library in Perennial.
Modularity was essential to enable the GoTxn proof. Better support for
modularity, perhaps formalizing some of the aspects of the specification style,
would have more cleanly separated each library's proofs. Where this is
particularly important is in making changes to the code that affect an
interface, in which case it can be difficult to tell from the code exactly what
properties the caller is assuming about the interface. The specification style
was developed in parallel with all the proofs, which means that proofs do not
all follow the best practices developed along the way.

\section{Future work}

\paragraph{Apply the approach to the kernel.} It would be interesting to port
the Goose approach to Rust or C and then apply it to code in the kernel. Go
already gets good performance, but because it has a runtime a Go library cannot
be made to run in the kernel (an interesting exception is to use Goose for
Biscuit~\cite{cutler:biscuit}, an OS kernel written in Go). An important
challenge for verifying in-kernel code would be modeling the specifications,
both those assumed by the verified code and what is promised to the rest of the
kernel. Since these are trusted specifications, it would be interesting to more
carefully validate them, perhaps with dynamic instrumentation.

\paragraph{Asynchronous model of the disk.} The disk model in Goose assumes
writes are durable as soon as the write returns, but real disks typically buffer
writes internally. Goose has a new asynchronous disk model,
and it would be good future work to port GoTxn to
this new model. Above the write-ahead log this should have no effect on the
specifications. Going beyond asynchronous durability, it would also be
interesting to model a different interface entirely like that of libaio where
requests are submitted to a queue and completions are delivered separately.

\paragraph{Improve the write-ahead log.} After implementing GoTxn, we made
relatively few changes, as reported in \cref{sec:eval:incremental}. Several
improvements to the write-ahead log would be interesting to make, which were
challenging due to the complexity of the existing proof.

First, it would be worth experimenting to see if the proof of installation could
be separated from the rest of the proof. The important task would be identifying
the right specification for the logging code which is enough to reason about the
combination of installation and logging.

Second, due to the future dependency in the write-ahead log's \cc{Read}
(described in \cref{sec:txn:wal}), it is specified as two separate
operations; it would be interesting to add \emph{prophecy variables} to
Perennial, similar to the Iris implementation~\cite{jung:prophecy}, and give
\cc{Read} a single linearizable specification.

Finally, the write-ahead log uses physical logging where all updates represent
full-block overwrites. It would be interesting to add more sophisticated
operations like sub-block updates that are natively supported by the write-ahead
log (rather than by the layer above). More ambitiously, it would be interesting
to verify \emph{logical logging} where updates represent high-level operations
like a file write. Logical logging would require a more sophisticated
specification and proof because the write-ahead log's entries would be
interpreted by the caller, resulting in a higher-order specification where the
caller passes a function which might need to be run during recovery, and
multiple times.

\paragraph{Verify a file system directly on top of GoJournal.} Dafny allows
the DaisyNFS proof to take advantage of automation that is enabled by sequential
reasoning. It would be interesting to do a direct comparison against a proof in
Perennial where due to ownership the proofs would still require only sequential
reasoning. The GoJournal paper reports verifying a subset of a file system
directly~\cite{chajed:gojournal}, but we never attempted to apply the approach
to a full-fledged file system. What would be especially interesting is if the
Perennial proof started with the Dafny proof's structure and invariants; perhaps
these invariants (developed under the constraints of Dafny and for
automation-friendliness) would also be useful in an interactive theorem proving
setting.

\paragraph{Do more with the NFS specification.} The NFS specification for
DaisyNFS is hand-written and integrated with the code. It would be an
interesting project in its own right to turn this into a complete, well-tested
formalization of the RFC. For example, the Dafny specification could make a serious
attempt to capture all the non-determinism allowed by the RFC. As a standalone
artifact the specification could also be used to test servers (comparing to the
allowed behaviors). An executable version of the specification, even if
inefficient and not durable, would be valuable to test the specification itself
and explore what clients do when faced with different server behavior.
