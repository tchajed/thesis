This thesis describes an approach to verify software with a combination of
concurrency in the implementation and crash-safety guarantees, applied to the
DaisyNFS file system. It spans from foundations for verifying this software in
general through the design and proof of the file system itself. The foundations
include Perennial, a program logic for crashes and concurrency, and Goose, an
approach for reasoning about Go code. DaisyNFS is designed around a verified
transaction system called GoTxn, which makes it feasible to scale verification
by enabling sequential reasoning for transactions on top.

\section{What is the value of verifying a file system?}

The reader might wonder what the value of this kind of large verification effort
is.

\paragraph{Making verification feasible.}
Prior to the work in this thesis, there simply didn't exist techniques for this
kind of proof, thus we could not ask the question of whether it would be worth
verifying a concurrent file system. This thesis makes it possible, and shows
that the cost is not unreasonable. A second implementation and proof would not
be as challenging, building on these foundations. I hope the costs eventually
come down to make verification an appealing alternative to verification for the
next generation of storage systems.

\paragraph{Cost versus benefit.} There are many less formal ways to make
software more reliable, including testing, fuzzing, and code auditing. Formal
verification also has a spectrum; we could potentially apply property-based
testing, model checking, or symbolic execution to parts of the code. Rather than
verifying the implementation, we could instead attempt to model and specify key
parts of the algorithms or system design, then verify those. What is the
additional value of fully machine-checked proofs, all the way down to the code?
Verification in the real world needs to have benefits that outweigh the costs.

A real achievement for verification would be to use verification not just as
alternative to testing, but to build something with more daring optimizations,
features, or speed then would otherwise be possible. I think file systems (or
more generally, some storage system that holds persistent state) have the
potential to be just such an application. There aren't that many widely-used
file systems --- most people use one of ext4, btrfs, or XFS on Linux, NTFS on
Windows, and APFS on macOS --- and new file systems are generally adopted very
slowly. APFS was surprising for having only a 3-year development period before
Apple widely deployed it. Ext4 is the next newest of that list, introduced in
2008 (by extending ext3, which was first released in 2001).

% ext4: 2008
% ext3: 2001
% XFS: 1994 (first released on IRIX OS for SGI)
% NTFS: 1993
% APFS: 2017

\paragraph{Verification guides debugging.} With a verified system, bugs are
still inevitable since there is unverified code surrounding the verified code,
the assumptions of the proof can be violated, and the specification can be
wrong. However, an advantage of formal verification, particularly fully
machine-checked proofs, is that when bugs are discovered it's safe to start
debugging with all the code outside the verification as well as looking at the
specification.

\paragraph{Working out the details gives confidence in the methodology.} One
reason to go through the trouble of mechanizing the proofs is to validate new
techniques in a way that gives confidence, much like how mechanizing a program proof increases
confidence in the correctness of the reasoning. For example, the formalism helps
work out a specification for crash safety and concurrency, one that the
mechanized proof shows has the right meaning (due to the soundness theorem) and
is usable (due to the verified reasoning principles in the logic). With this
confidence in the meaning of a specification and intuition for how they can be
soundly manipulated, now it is possible to carry out reasoning informally, or
implement the logic in an automated system.

Concurrency makes this especially important, since specifications can be quite
subtle to use. One recent example is prophecy variables, where in the process of
developing a mechanized proofs Jung et al found an unsoundness in the use of
prophecy variables in a proof from Viktor Vafeiadis's PhD
thesis~\cite{jung:prophecy,vafeiadis-phd}. Higher-order reasoning, especially
the ability to store procedures in the heap, has also led to some subtle
reasoning principles like the anti-frame rule~\cite{pottier:anti-frame}, whose
soundness proof was rather
sophisticated~\cite{schwinghammer:semantic-anti-frame}.

\paragraph{Mechanized proofs force the reasoning to be complete.} On-paper
reasoning about a system gains much of the value of formal reasoning, that of
forcing the author of a system to think through the cases systematically. While
it works to a degree, it is easy to delude oneself into thinking that a proof is
complete while actually having neglected an important corner case.

\paragraph{Interactive theorem proving is a great way to prototype.} I do not
anticipate that busy engineers trying to write production code will write proofs
in Coq, much less develop a custom program logic. However, this workflow is
excellent for prototyping the right reasoning principles, some reasons for which
are outlined above. Working out proofs interactively gives a good feel for how
the logic works, from which automation can be designed bit by bit (some of it as
Coq automation itself). In Perennial (as in Iris), proof steps are at a
sufficiently high level of abstraction to make the proofs doable for prototyping
purpose, even though it has no search-based automation, sophisticated
algorithms, or use of an SMT solver.

\paragraph{Verification is valuable for critical systems.} It will never be cost
effective to verify all software; at a minimum software needs a specification
that is more likely to be correct than the implementation to be verified.
Instead, the aim is to use verification for critical systems, \emph{critical}
because failures are especially bad, and \emph{systems} that have well-defined
guarantees to make to other systems and applications running on top. It is also
helpful in the cost-benefit tradeoff if the system is part of a ``narrow waist''
used by many other pieces of software, since then the impact of bugs is felt to
a greater extent. Finally, alongside the need for a specification it is also
important that the verification respond to changes in both specification and
implementation. If both change rapidly and the proofs cannot be re-done, then
verification won't keep up with the pace of development. File systems fit all of
these criteria: bugs can lead to data loss which the application cannot
mitigate, the interface is stable and relatively standard, and essentially all
applications go through the file system to store persistent state.


\section{Discussion}

\paragraph{Memory safety.}
A surprisingly difficult aspect of the proof was addressing memory safety
considerations. These show up in two ways: describing ownership at interface
boundaries and reasoning about read-only sharing.

DaisyNFS has two important interface boundaries, one between GooseLang and the
disk, and another to describe the GoTxn interface. Both APIs involve data in the
form of byte slices. The challenging aspect of using bytes in a method is
specifying how \emph{ownership} transfers. For example, the \cc{DiskWrite(a, v)}
operation passes a buffer \cc{v} to the disk. It could be that ownership of
\cc{v} needs to be transferred to the disk, since it uses the buffer later, or
that \cc{v} should be read-only for the duration of the call and ownership is
returned to the caller, or that if \cc{v} is permanently read-only the disk and
caller can share the buffer.

Expressing ownership as a logical idea in the logic is relatively easy using
separation logic, but the semantics of \cc{DiskWrite(a, v)} has to be
axiomatized. In order to simplify expressing the ownership transfer, the
GooseLang \cc{DiskRead} returns a freshly allocated buffer, but it would have
been better for performance if the caller supplied a buffer that the
\cc{DiskRead} wrote to (as in the usual \cc{read} system call). Similarly, the
GoTxn interface to Dafny makes additional copies to simplify ownership
reasoning, especially in the transaction refinement proof. A better solution would
have been to develop a specification style for expressing ownership in the
semantics of the operations themselves, along with associated reasoning
principles.

Another challenge in the internals of GoTxn was reasoning about read-only
sharing. Changing data structures after they were written to support read-only
sharing was difficult, since every specification needs to incorporate fractional
permissions. Read-only locking was challenging to retrofit support for, even
though it may have improved read-read concurrency in GoTxn and DaisyNFS.

\paragraph{Modular proofs.}

\Cref{ch:crash-logatom} describes a style for specifying a library in Perennial.
Modularity was essential to enable the GoTxn proof. Better support for
modularity, perhaps formalizing some of the aspects of the specification style,
would have more cleanly separated each library's proofs. Where this is
particularly important is in making changes to the code that affect an
interface, in which case it can be difficult to tell from the code exactly what
properties the caller is assuming about the interface. The specification style
was developed in parallel with all the proofs, which means that proofs do not
all follow the best practices developed along the way.

\section{Future work}

\paragraph{Apply the approach to Rust or C.} It would be interesting to port
the Goose approach to Rust or C. This would not necessarily be for better
performance but as a first step towards better systems integration. For example,
Rust or C would be more practical for verifying critical parts of the Linux
kernel. An important challenge for verifying in-kernel code would be modeling
the specifications, both those assumed by the verified code and what is promised
to the rest of the kernel.

\paragraph{Asynchronous model of the disk.} The disk model in Goose assumes
writes are durable as soon as the write returns, but real disks typically buffer
writes internally. Goose has a new asynchronous disk model, used for an
independent example, but it would be good future work to change GoTxn to use
this new model. Above the write-ahead log this should have no effect on the
specifications. Going beyond asynchronous durability, it would also be
interesting to model a different interface entirely like that of libaio where
requests are submitted to a queue and completions are delivered separately.

\paragraph{Improve the write-ahead log.} After implementing GoTxn, we made
relatively few changes, as reported in \cref{sec:eval:incremental}. Several
improvements to the write-ahead log would be interesting to make, which were
challenging due to the complexity of the existing proof.

First, it would be worth experimenting to see if the proof of installation could
be separated from the rest of the proof. The important task would be identifying
the right specification for the logging code which is enough to reason about the
combination of installation and logging.

Second, due to the future dependency in the write-ahead log's \cc{Read}
(described in \cref{sec:txn:wal}), it is specified as two separate
operations; it would be interesting to add \emph{prophecy variables} to
Perennial, similar to the Iris implementation~\cite{jung:prophecy}, and give
\cc{Read} a single linearizable specification.

Finally, the write-ahead log uses physical logging where all updates represent
full-block overwrites. It would be interesting to add more sophisticated
operations like sub-block updates that are natively supported by the write-ahead
log (rather than by the layer above). More ambitiously, it would be interesting
to verify \emph{logical logging} where updates represent high-level operations
like a file write. Logical logging would require a more sophisticated
specification and proof because the write-ahead log's entries would be
interpreted by the caller, resulting in a higher-order specification where the
caller passes a function which might need to be run during recovery, and
multiple times.

\paragraph{Verifying a file system directly on top of GoJournal.} Dafny allows
the DaisyNFS proof to take advantage of automation that is enabled by sequential
reasoning. It would be interesting to do a direct comparison against a proof in
Perennial where due to ownership the proofs would still require only sequential
reasoning. The GoJournal paper reports verifying a subset of a file system
directly~\cite{chajed:gojournal}, but we never attempted to apply the approach
to a full-fledged file system. What would be especially interesting is if the
Perennial proof started with the Dafny proof's structure and invariants; perhaps
these invariants (developed under the constraints of Dafny and for
automation-friendliness) would also be useful in an interactive theorem proving
setting.

\paragraph{Do more with the NFS specification.} The NFS specification for
DaisyNFS is hand-written and integrated with the code. It would be an
interesting project in its own right to turn this into a complete, well-tested
formalization of the RFC. For example, the Dafny specification could make a serious
attempt to capture all the non-determinism allowed by the RFC. As a standalone
artifact the specification could also be used to test servers (comparing to the
allowed behaviors). An executable version of the specification (though perhaps
inefficient and not durable) would be valuable to test the specification itself
and explore what clients do when faced with different server behavior.
