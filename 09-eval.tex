In this chapter we evaluate DaisyNFS and GoTxn in terms of performance
(\cref{sec:eval:bench,sec:eval:scale}), correctness (\cref{sec:eval:testing}),
and ease of change (\cref{sec:eval:incremental}).

\section{Performance}
\label{sec:eval:bench}

To evaluate the performance of DaisyNFS, we ran three benchmarks: the LFS smallfile
and largefile benchmarks, and a development workload that consists of \cc{git
clone} from a local repository followed by running \cc{make}. These are the same
benchmarks used by DFSCQ~\cite{chen:dfscq} (a state-of-the art sequential
verified file system). As a baseline, this evaluation uses a Linux
NFS server exporting an ext4 file system mounted with \cc{data=journal} mode.
The NFS server lets us compare fairly since both go through the Linux NFS client
and use the same underlying protocol. Using \cc{data=journal} forces all data to
go through the journal and disables log-bypass writes, which ensures that
ext4 and DaisyNFS both guarantee NFS RPCs are committed durably when they
return, and makes the designs more comparable. In
\cc{data=ordered} mode Linux achieves better performance but can
lose recently written data if the system crashes.

All of these benchmarks were run using Linux~5.13, Dafny~3.5.0, Go~1.18 on an Amazon
EC2 i3.metal instance, which has 72 cores running at 2.7~GHz, 512~GB of RAM, and a local 1.9~TB
NVMe SSD.\@ To reduce variability we limit the experiment to a single 36-core
socket, disable turbo boost, and disable processor sleep states; the coefficient
of variation for all experiments is under 5\% so we omit error bars for visual
clarity.

%% cpupower frequency-set -g performance
%% threads=32 in the [nfsd] section of /etc/nfs.conf

%% SINGLE-CORE EXPERIMENTS:
%%   for n in $(seq 8 15) $(seq 24 31); do echo 0 > /sys/devices/system/cpu/cpu$n/online; done

\begin{figure}
  \includegraphics{daisy-nfs/fig/bench.pdf}
  \caption[Performance for smallfile, largefile, and app benchmarks]%
  {Performance of Linux NFS and DaisyNFS for \cc{smallfile},
    \cc{largefile}, and \cc{app} workloads, on an NVMe disk.
    DaisyNFS achieves comparable performance to ext4 in \cc{data=journal} mode.}
  \label{fig:eval:bench}
\end{figure}

\begin{figure}
  \includegraphics{daisy-nfs/fig/extended-bench.pdf}
  \caption{Performance across various file-system configurations. The smallfile
    benchmark is run with a single client (\cc{smallfile-1}) and also with 25
    parallel clients (\cc{smallfile-25}). The ``log bypass'' variant of Linux
    uses the \cc{data=ordered} to write data directly rather than through the
    journal. The ``seq txn'' variant of DaisyNFS has a global transaction lock
    while ``seq wal'' adds additional locking to the write-ahead log.}
  \label{fig:bench-configs}
\end{figure}

\begin{figure}
  \includegraphics{daisy-nfs/fig/extended-bench-ram.pdf}
  \caption{Performance across various file-system configurations on in-memory disk.}
  \label{fig:bench-configs-ram}
\end{figure}

The first set of results are for the three benchmarks, run on an NVMe disk, all
with a single client. The numbers are shown in \cref{fig:eval:bench}. DaisyNFS achieves
comparable performance on these benchmarks after implementing many optimizations
(for example, parsing data structures in-place) and features (for example,
returning attributes to enable client-side caching). Both CPU and I/O efficiency
are important to get good performance.

The smallfile benchmark is a metadata-heavy benchmark that repeatedly creates a
file, writes 100 bytes to it, calls \cc{fsync()}, and then deletes the file. At
the NFS level this results in four RPCs per iteration: \cc{LOOKUP} (which
fails), \cc{CREATE}, \cc{WRITE}, and \cc{REMOVE}. Getting the client to issue
exactly these four RPCs took some work, since Linux relies on getting the right
``weak cache-consistency'' data from all the operations to maintain its local
cache. Without this caching, it would issue more operations to retrieve and
re-retrieve the directory attributes and permissions.

DaisyNFS gets slightly better throughput compared to Linux on the largefile benchmark, which is
intended to measure bulk data writes. Note that in this benchmark ext4 would be
significantly faster in \cc{data=ordered} mode due to no longer writing all data
through the journal; with that option it gets 315~MB/s even over
NFS.\@ The largefile benchmark creates a 300~MB file by appending repeatedly, then
syncs it. The Linux NFS client buffers the entire append process until the final
sync, at which point it issues the writes in many chunks in parallel. This
benchmark is challenging to support efficiently because the writes at the NFS
server do not come in order, so some are past the end of the file. The semantics
of such a write are to fill the gap with zeros, but both DaisyNFS and Linux get good
performance despite this because they implicitly encode those zeros without even
allocating a block. DaisyNFS gets comparable performance on this benchmark because
GoTxn is good at handling concurrent synchronous writes.

DaisyNFS achieves good performance on the app workload, which consists of running
\cc{git clone} on the xv6 repository followed by \cc{make}. xv6 is an operating
system, so building it requires running the usual development tools---gcc, ld,
ar---but also running dd to generate a kernel image. Builds take about 3s,
which are reported as a throughput number so higher is better.

\section{Scalability}
\label{sec:eval:scale}

DaisyNFS executes NFS operations concurrently to achieve better performance with
multiple cores. This section evaluates the scalability of the whole system,
especially the concurrency provided by GoTxn. The benchmark used is the smallfile benchmark from
\cref{sec:eval:bench}, with a varying number of cores. Because this experiment runs
on a physical disk, other threads have a chance to prepare transactions while the
transaction system is committing to disk.

\begin{figure}
  \includegraphics{daisy-nfs/fig/scale.pdf}
  \caption[Concurrent smallfile performance]%
{Combined throughput of the \cc{smallfile} microbenchmark running on
    an NVMe drive while
    varying the number of concurrent clients. DaisyNFS's performance scales with the
    number of cores similar to ext4; both eventually saturate the disk and scale
    sub-linearly.}
  \label{fig:eval:scale}
\end{figure}

The results are shown in \cref{fig:eval:scale}. The graph shows that DaisyNFS
achieves both good absolute performance and gets higher throughput with more
clients, comparable to the scalability of the Linux NFS server. These benchmarks
eventually become bottlenecked on the disk. We ran an analogous experiment on an
in-memory disk to reduce this bottleneck, shown in \cref{fig:eval:scale-ram}.
Linux scales much better than DaisyNFS in this experiment. DaisyNFS has a common
lock between these threads for installing sub-block object writes into a full
block; this lock is there when running on a physical disk but its effect is less
pronounced since the system spends relatively more time in disk I/O.

\begin{figure}
  \includegraphics{daisy-nfs/fig/scale-ram.pdf}
  \caption[Concurrent smallfile performance, with RAM disk]%
  {Combined throughput of the \cc{smallfile} microbenchmark running on an
    in-memory disk while varying the number of concurrent clients. \tej{this is
      out-of-date but probably hasn't changed much}}
  \label{fig:eval:scale-ram}
\end{figure}

To investigate the effect of concurrency in more detail, we ran the three
benchmarks on a variety of file-system configurations, with the results shown in
\cref{fig:bench-configs}.
The variants of DaisyNFS make changes to GoTxn to reduce concurrency: the
``seq wal'' configuration adds locks to the write-ahead log so that the
disk operations that normally execute lock-free --- installation, logging, and
reading installed data --- instead happen with a common lock for all write-ahead
log operations. This permits transactions to execute concurrently while not
accessing GoTxn but no disk parallelism. The ``seq txn'' variant leaves the
write-ahead log the same but changes the per-address locking in GoTxn to a
\emph{global} transaction lock, preventing any transactions from running
concurrently but allowing concurrency between logging and installation in the
write-ahead log.

Looking at the \cc{smallfile-1} and \cc{smallfile-25} benchmarks, we can see
that both forms of concurrency are important for scalability, and the
write-ahead log concurrency makes a big difference even with a single client
since it can get higher write throughput from the disk (on an in-memory disk,
these trends are less visible; the work outside of DaisyNFS in the Linux client
and local network still gets parallelized to improve throughput, even though
these configurations have little concurrency with a fast disk). On the other
hand a global transaction lock actually performs \emph{better} on the largefile
benchmark. This operation has many conflicting parallel operations to different
parts of the same file, and the results shows that two-phase locking has some
overhead compared to a simpler locking scheme.

Another comparison in these results is to Linux with its log bypass optimization
(the default \cc{data=ordered} mode), where writes can be written directly to a
file's data blocks rather than going through the log. This only makes a difference for asynchronous NFS
writes, since synchronous writes are journaled even with this option. The smallfile benchmark has only
synchronous writes, so unsurprisingly performance is the same, but on the
largefile benchmark Linux with log bypass achieves 80\% higher throughput. An
improvement of up to $2\times$ better throughput is expected here since unlike
in all the other configurations, data writes are written only once rather than
to both the journal and the data region, and the actual benefit is high since
the benchmark is dominated by disk writes. It's worth noting that even Linux
does not saturate the disk, which can get 500 MB/s with direct sequential
writes, due to the overhead of running over NFS.

\section{Testing the trusted code and spec}
\label{sec:eval:testing}

For the proof of \cref{thm:daisy-correctness} to apply to the actual server, we trust that (1) the
Dafny code is a ``safe'' use of the transaction system, (2) sequential
refinement is correctly encoded into Dafny, (3) the libraries for Go primitives
are correctly specified in Dafny, and (4) the unverified Go code calling the
Dafny methods and implementing the NFS wire protocol is correct. The
user must follow the assumed execution model and run initialization from an
empty disk and recovery after each boot. Finally, the disk needs to behave
according to its assumed specification, which requires that it preserve written
data and not corrupt it.

Beyond satisfying this formal theorem statement, we want two more things from
the implementation and specification: first that the specification as formalized
actually reflects the RFC, and second we would like DaisyNFS to be compatible
with existing clients, including implementing enough of the RFC's functionality.
These fall outside the scope of verification so we cover them with testing.

To evaluate the file system's correctness and compatibility as a whole, we
mounted it using the Linux NFS client and ran the fsstress and fsx-linux tests,
two suites used for testing the Linux kernel. In order to look for bugs in crash
safety and recovery, we also ran CrashMonkey~\cite{mohan:crashmonkey}, which
found no bugs after running all supported 2-operation tests.

While other experiments in the evaluation interact with DaisyNFS via the Linux
client, these tests interact with the server directly from a hand-written
client. That client is then tested with symbolic execution over the entire server.
This testing produces a wider range of requests than are possible via the Linux
client. The process helped us find and fix several bugs in the unverified parts
of DaisyNFS and in the specification itself, which are reported in
\cref{fig:daisynfs-bugs}.

Two of the specification bugs are particularly interesting. The bounded inode bug
was due to an \cc{ino} argument of type \cc{Ino}; this type is a Dafny
\emph{subset type}, thus adding an implicit precondition that \cc{ino <
NUM_INODES}, which can be violated by the (unverified) Go code. The fix is to instead
use a \cc{uint64} and check the bound in verified code. The RENAME bug was due
to having an incomplete specification (and implementation) that did not capture
that RENAME should only overwrite when the source and destination are
compatible.

\begin{figure}
  \begin{center}
  \begin{tabular}{@{}p{8cm}p{2.7cm}@{}}
    \toprule
    \textbf{Bug} & \textbf{Why?} \\
    \midrule
    XDR decoder for strings can allocate $2^{32}$ bytes & Unverified \\
    File handle parser panics if wrong length & Unverified \\
    WRITE panics if not enough input bytes & Unverified \\
    Directory REMOVE panics in dynamic type cast & Unverified \\
    Panic on unexpected enum value & Unverified \\
    The names \cc{.} and \cc{..} are allowed & Not in RFC 1813 \\
    RENAME can create circular directories & Not in RFC 1813 \\
    CREATE/MKDIR allow empty name & Specification \\
    Proof assumes caller provides bounded inode & Specification \\
    RENAME allows overwrite where spec does not & Specification \\
    \bottomrule
  \end{tabular}
  \end{center}
  \caption{Bugs found by testing at the NFS protocol level.}
  \label{fig:daisynfs-bugs}
\end{figure}

%As a sanity check on the transaction system, we fuzzed the interface presented
%to Dafny using a test harness capable of performing arbitrary reads, writes and
%commits while following the transaction system's preconditions. The test checks
%that the transaction system does not crash and returns the correct data on read.
%The fuzzing did generate complex inputs with multiple chained operations but did
%not find any bugs, but we note that our fuzzing is limited to
%single-threaded and crash-free behavior. \joe{I think we could cut this para now, or perhaps make it more focused by saying it's about testing the trusted boundary between Dafny and GoTxn. But I'm not quite sure it actually does test that?}

\section{Incremental improvements}
\label{sec:eval:incremental}

\begin{figure}
\begin{center}
\begin{tabular}{lrr}
  \toprule
  \textbf{Feature} & \textbf{Time} & \textbf{Lines} \\
  \midrule
  In-place directory updates & 2 days & 600\\
  Multi-block directories & 5 days & 800 \\
  NFS attributes & 4 days & 500 \\
  Freeing space (\cref{sec:dafny:freeing}) & 3 days & 1400\\
  \bottomrule
\end{tabular}
\end{center}
\caption{Incremental improvements were implemented quickly and without much
  code (which includes both implementation and proof).}
\label{fig:features}
\end{figure}

DaisyNFS was implemented and verified over the course of three months by
one of the authors, until it had support for enough of NFS to run. We
added several features incrementally after the initial prototype
worked, both to improve performance and to support more
functionality. Some of the interesting changes are listed in
\cref{fig:features}.  To improve performance, we switched to
operating on the serialized representation of directories directly
(decoding fields on demand and encoding in-place) and then added also
multi-block directories.  We added support for attributes so that the file
system stores the mode, uid/gid, and modification timestamp for files and directories.
Finally, we implemented the freeing plan described
in \cref{sec:dafny:freeing}, which required additional code through the
whole stack (but by design no changes to the file-system invariant).
We believe additional features such as symbolic links
could be added incrementally with modest effort because
of sequential reasoning and proof automation.

For the smallfile benchmark, as mentioned it took some work to return enough
attribute information that Linux reliably cached everything it could, to avoid
extra RPCs. This work primarily consisted of expanding the attributes operations
return (the RFC says these post-operation attributes are optional but
encouraged), and at the same time expanding the specifications to say this
metadata is correct. These improvements were carried out in many small steps,
but they were similar to the incremental improvements above in terms of taking a
small amount of time and few changes to lines of proof; most of the work went
into figuring out what was missing in the implementation.

%% \begin{itemize}
%%   \item Directories were initially parsed into a functional data structure, then
%%   re-serialized to make changes. To improve performance, we switched to
%%   operating on the serialized representation directly (decoding fields on demand
%%   and encoding in-place). (2 days, 600 lines changed)
%%   \item Single-block directories were too limiting, so we switched to multiple
%%   blocks, which are read into memory on demand. We still operate on the
%%   serialized representation, but now of only part of the directory. (5 days, 800
%%   lines changed)
%%   \item Initially we only tracked file types. We added support for attributes so
%%   that the mode, uid/gid, and modification timestamp would be stored correctly.
%%   (4 days, 500 lines changed)
%%   \item The file system initially only freed the first 8 direct blocks of an
%%   inode. We implemented the space-freeing plan described in
%%   \cref{sec:dafny:freeing} through the whole stack afterward. (3 days, 1400
%%   lines changed)
%% \end{itemize}
