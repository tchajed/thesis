In this chapter we evaluate DaisyNFS and GoTxn in terms of performance
(\cref{sec:eval:bench,sec:eval:scale}), correctness (\cref{sec:eval:testing}),
and ease of change (\cref{sec:eval:incremental}).

\tej{add evaluation questions answered by each section}

\section{Performance}
\label{sec:eval:bench}

To evaluate the performance of DaisyNFS, we ran three benchmarks: the LFS smallfile
and largefile benchmarks, and a development workload that consists of \cc{git
clone} from a local repository followed by running \cc{make}. These are the same
benchmarks used by DFSCQ~\cite{chen:dfscq} (a state-of-the art sequential
verified file system). As a baseline, this evaluation uses a Linux
NFS server exporting an ext4 file system mounted with \cc{data=journal} mode.
The NFS server lets us compare fairly since both go through the Linux NFS client
and use the same underlying protocol. Using \cc{data=journal} forces all data to
go through the journal and disables log-bypass writes, which ensures that
ext4 and DaisyNFS both guarantee NFS RPCs are committed durably when they
return, and makes the designs more comparable. In
\cc{data=ordered} mode Linux achieves better performance but can
lose recently written data if the system crashes.

All of these benchmarks were run using Linux~5.13, Dafny~3.5.0, Go~1.18 on an Amazon
EC2 i3.metal instance, which has 72 cores running at 2.7~GHz, 512~GB of RAM, and a local 1.9~TB
NVMe SSD.\@ To reduce variability we limit the experiment to a single 36-core
socket, disable turbo boost, and disable processor sleep states; the coefficient
of variation for all experiments is under 5\% so we omit error bars for visual
clarity.

The three benchmarks test different aspects of file system performance:

\paragraph{Smallfile.}
The smallfile benchmark is a metadata-heavy benchmark that repeatedly creates a
file, writes 100 bytes to it, calls \cc{fsync()}, and then deletes the file. The
performance reported is the throughput in terms of files/s for this whole process. At
the NFS level this results in four RPCs per iteration: \cc{LOOKUP} for the file
being created (which fails), \cc{CREATE}, \cc{WRITE}, and \cc{REMOVE}. Getting
the client to issue exactly these four RPCs took some work, since Linux relies
on getting the right ``weak cache-consistency'' data from all the operations to
maintain its local cache. Without this caching, it issues more operations like
\cc{GETATTR} and \cc{ACCESS} in between operations to update the directory
attributes and permissions.

\paragraph{Largefile.}
The largefile benchmark is a data-heavy benchmark that creates a file, then
repeatedly appends to the file until it is 300 MB, and finally calls
\cc{fsync()} and closes the file. The performance reported is the throughput in
terms of MB/s for these appends. The Linux NFS client buffers the entire append
process until the final sync, at which point it issues the writes in many chunks
in parallel. This benchmark is challenging to support efficiently because the
writes at the NFS server do not come in order, so some are past the end of the
file and then the gap is filled in by later RPCs.

\paragraph{App.}
The application workload ``app'' consists of running \cc{git clone} on the xv6
repository followed by \cc{make}. The performance reported converts this latency
to a throughput number ``app/s'' so that higher is better. The compilation part
of this workload does take about 1.2s, out of an overall build that typically
takes about 2.5s under Linux NFS. These 1.2s are unaffected by the file system's
performance. The xv6 code being compiled is an operating system, so building it
requires running the usual development tools---gcc, ld, ar---but also running dd
to generate a kernel image. The main purpose of the app benchmark is to
demonstrate that DaisyNFS covers a useful range of the NFS interface, since
\cc{git clone} especially uses a wider mix of operations and arguments than the
other benchmarks, and also that the file system implements these operations
efficiently enough to not slow down the workload.

%% cpupower frequency-set -g performance
%% threads=32 in the [nfsd] section of /etc/nfs.conf

%% SINGLE-CORE EXPERIMENTS:
%%   for n in $(seq 8 15) $(seq 24 31); do echo 0 > /sys/devices/system/cpu/cpu$n/online; done

\begin{figure}[ht]
  \includegraphics{daisy-nfs/fig/bench.pdf}
  \caption[Performance for smallfile, largefile, and app benchmarks]%
  {Performance of Linux NFS and DaisyNFS for \cc{smallfile},
    \cc{largefile}, and \cc{app} workloads, on an in-memory disk.
    DaisyNFS achieves comparable performance to ext4 in \cc{data=journal} mode.}
  \label{fig:eval:bench}
\end{figure}

The first set of results are for the three benchmarks, run on an in-memory disk, all
with a single client. These benchmarks are challenging to support efficiently
since an in-memory disk means performance isn't dominated by slow I/O, and a
single client demands single-core efficiency and not just parallelizing
overhead. The numbers are shown in \cref{fig:eval:bench}. DaisyNFS achieves
comparable performance on these benchmarks after implementing many optimizations
(for example, parsing data structures in-place) and features (for example,
returning attributes to enable client-side caching). Both CPU and I/O efficiency
are important to get good performance.

DaisyNFS gets slightly better throughput compared to Linux on the largefile
benchmark. Note that in this benchmark ext4 is significantly faster in
\cc{data=ordered} mode due to no longer writing all data through the journal, as
discussed shortly in \cref{sec:eval:scale}. The NFS client's behavior on
this benchmark means it has writes past the end of the file; the semantics of
such a write are to fill the gap with zeros. DaisyNFS and Linux get good
performance despite this because they implicitly encode those zeros without even
allocating a block. DaisyNFS gets comparable performance on this benchmark
because GoTxn is good at handling concurrent synchronous writes.

DaisyNFS achieves good performance on the app workload.

\section{Scalability}
\label{sec:eval:scale}

DaisyNFS executes NFS operations concurrently to achieve better performance with
multiple cores. This section evaluates the scalability of the whole system,
especially the concurrency provided by GoTxn. The benchmark used is the smallfile benchmark from
\cref{sec:eval:bench}, with a varying number of cores. Because this experiment runs
on a physical disk, other threads have a chance to prepare transactions while the
transaction system is committing to disk.

\begin{figure}[hp]
  \includegraphics{daisy-nfs/fig/scale.pdf}
  \caption[Concurrent smallfile performance]%
{Combined throughput of the \cc{smallfile} microbenchmark running on
    an NVMe drive while
    varying the number of concurrent clients. DaisyNFS's performance scales with the
    number of cores similar to ext4; both eventually scale
    sub-linearly due to lock contention. \tej{add lines for DaisyNFS with seq
      txn/wal directly to this graph}}
  \label{fig:eval:scale}
\end{figure}

\begin{figure}[hp]
  \includegraphics{daisy-nfs/fig/scale-ram.pdf}
  \caption[Concurrent smallfile performance, with RAM disk]%
  {Combined throughput of the \cc{smallfile} microbenchmark running on an
    in-memory disk while varying the number of concurrent clients.}
  \label{fig:eval:scale-ram}
\end{figure}

The results are shown in \cref{fig:eval:scale}. The graph shows that DaisyNFS
achieves both good absolute performance and gets higher throughput with more
clients, comparable to the scalability of the Linux NFS server. These benchmarks
don't have perfect scalability due to lock contention. We ran an analogous experiment on an
in-memory disk to reduce this bottleneck, shown in \cref{fig:eval:scale-ram}.
Linux scales much better than DaisyNFS in this experiment, indicating that its
scalability in \cref{fig:eval:scale} is limited by disk throughput. DaisyNFS has a common
lock between these threads for installing sub-block object writes into a full
block; this lock is there when running on a physical disk but its effect is less
pronounced since the system spends relatively more time in disk I/O. The Go
mutex profile points to this lock as being the biggest source of contention in
DaisyNFS.

\begin{figure}[hp]
  \includegraphics{daisy-nfs/fig/extended-bench.pdf}
  \caption{Performance across various file-system configurations. The smallfile
    benchmark is run with a single client (\cc{smallfile-1}) and also with 25
    parallel clients (\cc{smallfile-25}). The ``log bypass'' variant of Linux
    uses the \cc{data=ordered} to write data directly rather than through the
    journal. The ``seq txn'' variant of DaisyNFS has a global transaction lock
    while ``seq wal'' adds additional locking to the write-ahead log.}
  \label{fig:bench-configs}
\end{figure}

\begin{figure}[hp]
  \includegraphics{daisy-nfs/fig/extended-bench-ram.pdf}
  \caption{Performance across various file-system configurations on in-memory disk.}
  \label{fig:bench-configs-ram}
\end{figure}


To investigate the effect of concurrency in more detail, we ran the three
benchmarks on a variety of file-system configurations, with the results shown in
\cref{fig:bench-configs}.
The variants of DaisyNFS make changes to GoTxn to reduce concurrency: the
``seq wal'' configuration adds locks to the write-ahead log so that the
disk operations that normally execute lock-free --- installation, logging, and
reading installed data --- instead happen with a common lock for all write-ahead
log operations. This permits transactions to execute concurrently while not
accessing GoTxn but no disk parallelism. The ``seq txn'' variant leaves the
write-ahead log the same but changes the per-address locking in GoTxn to a
\emph{global} transaction lock, preventing any transactions from running
concurrently but allowing concurrency between logging and installation in the
write-ahead log.

The \cc{smallfile-25} benchmark measures the importance of both forms of
concurrency for scalability, showing much lower performance. Comparing the
absolute numbers, these workloads are still not entirely sequential, since the
work outside of DaisyNFS in the Linux client and local network still gets
parallelized. DaisyNFS can coalesce concurrent operations and journal them
together to reduce total I/O, which is important on the physical disk but hardly
helps with an in-memory disk. There the lock contention for preparing a
transaction dominates performance and results in lower throughput than Linux,
although comparable absolute performance.

The \cc{smallfile-1} benchmark is interesting because the client issues a single
operation at a time, and yet seq wal performs noticeably worse. This is because
the write-ahead log is parallelizing logging writes and installing them to the
disk (rather than alternating between logging and installation), and the disk is
able to get better throughput by doing these concurrently.

The \cc{largefile} benchmark is interesting because the client issues many
concurrent operations, but they all conflict since they write to the same file.
Linux and DaisyNFS handle this contention fine, still getting good throughput
given that all writes have to be written to both the journal and data region in
both systems. However, we can see that a global txn lock actually performs
\emph{better} than the per-address locking, showing a case where with enough
conflicts two-phase locking has some overhead compared to a simpler locking
scheme. The seq wal case gets lower performance for similar reasons to with
smallfile-1, since it again alternates logging and installation rather than
issues them concurrently.

The \cc{app} benchmark is less heavy on writes, and we find that its performance
is barely affected by concurrency in DaisyNFS.

Another comparison shown in the same figure is to Linux with its log bypass
optimization (the default \cc{data=ordered} mode), where writes can be written
directly to a file's data blocks rather than going through the journal. This
only makes a difference for asynchronous NFS writes, since synchronous writes
are journaled even with this option. The smallfile benchmark has only
synchronous writes, so unsurprisingly performance is the same, but on the
largefile benchmark Linux with log bypass achieves 80\% higher throughput. An
improvement of up to $2\times$ better throughput is expected here since unlike
in all the other configurations, data writes are written only once rather than
to both the journal and the data region, and the actual benefit is high since
the benchmark is dominated by disk writes. It's worth noting that even Linux
does not saturate the disk, which can achieve 500 MB/s in sequential write
throughput, due to the overhead of running over NFS.

\section{Testing the trusted code and spec}
\label{sec:eval:testing}

For the proof of \cref{thm:daisy-correctness} to apply to the actual server, we trust that (1) the
Dafny code is a ``safe'' use of the transaction system, (2) sequential
refinement is correctly encoded into Dafny, (3) the libraries for Go primitives
are correctly specified in Dafny, and (4) the unverified Go code calling the
Dafny methods and implementing the NFS wire protocol is correct. The
user must follow the assumed execution model and run initialization from an
empty disk and recovery after each boot. Finally, the disk needs to behave
according to its assumed specification, which requires that it preserve written
data and not corrupt it.

Beyond satisfying this formal theorem statement, we want two more things from
the implementation and specification: first that the specification as formalized
actually reflects the RFC, and second we would like DaisyNFS to be compatible
with existing clients, including implementing enough of the RFC's functionality.
These fall outside the scope of verification so we cover them with testing.

To evaluate the file system's correctness and compatibility as a whole, we
mounted it using the Linux NFS client and ran the fsstress and fsx-linux tests,
two suites used for testing the Linux kernel. In order to look for bugs in crash
safety and recovery, we also ran CrashMonkey~\cite{mohan:crashmonkey}, which
found no bugs after running all supported 2-operation tests.

While other experiments in the evaluation interact with DaisyNFS via the Linux
client, these tests interact with the server directly from a hand-written
client. That client is then tested with symbolic execution over the entire server.
This testing produces a wider range of requests than are possible via the Linux
client. The process helped us find and fix several bugs in the unverified parts
of DaisyNFS and in the specification itself, which are reported in
\cref{fig:daisynfs-bugs}.

Two of the specification bugs are particularly interesting. The bounded inode bug
was due to an \cc{ino} argument of type \cc{Ino}; this type is a Dafny
\emph{subset type}, thus adding an implicit precondition that \cc{ino <
NUM_INODES}, which can be violated by the (unverified) Go code. The fix is to instead
use a \cc{uint64} and check the bound in verified code. The RENAME bug was due
to having an incomplete specification (and implementation) that did not capture
that RENAME should only overwrite when the source and destination are
compatible.

\begin{figure}
  \begin{center}
  \begin{tabular}{@{}p{8cm}p{2.7cm}@{}}
    \toprule
    \textbf{Bug} & \textbf{Why?} \\
    \midrule
    XDR decoder for strings can allocate $2^{32}$ bytes & Unverified \\
    File handle parser panics if wrong length & Unverified \\
    WRITE panics if not enough input bytes & Unverified \\
    Directory REMOVE panics in dynamic type cast & Unverified \\
    Panic on unexpected enum value & Unverified \\
    The names \cc{.} and \cc{..} are allowed & Not in RFC 1813 \\
    RENAME can create circular directories & Not in RFC 1813 \\
    CREATE/MKDIR allow empty name & Specification \\
    Proof assumes caller provides bounded inode & Specification \\
    RENAME allows overwrite where spec does not & Specification \\
    \bottomrule
  \end{tabular}
  \end{center}
  \caption{Bugs found by testing at the NFS protocol level.}
  \label{fig:daisynfs-bugs}
\end{figure}

%As a sanity check on the transaction system, we fuzzed the interface presented
%to Dafny using a test harness capable of performing arbitrary reads, writes and
%commits while following the transaction system's preconditions. The test checks
%that the transaction system does not crash and returns the correct data on read.
%The fuzzing did generate complex inputs with multiple chained operations but did
%not find any bugs, but we note that our fuzzing is limited to
%single-threaded and crash-free behavior. \joe{I think we could cut this para now, or perhaps make it more focused by saying it's about testing the trusted boundary between Dafny and GoTxn. But I'm not quite sure it actually does test that?}

\section{Incremental improvements}
\label{sec:eval:incremental}

\begin{figure}
\begin{center}
\begin{tabular}{lrr}
  \toprule
  \textbf{Feature} & \textbf{Time} & \textbf{Lines} \\
  \midrule
  In-place directory updates & 2 days & 600\\
  Multi-block directories & 5 days & 800 \\
  NFS attributes & 4 days & 500 \\
  Freeing space (\cref{sec:dafny:freeing}) & 3 days & 1400\\
  \bottomrule
\end{tabular}
\end{center}
\caption{Incremental improvements to the file-system code were implemented
  quickly and without much code (which includes both implementation and proof).}
\label{fig:features}
\end{figure}

DaisyNFS was implemented and verified over the course of three months by
one of the authors, until it had support for enough of NFS to run. We
added several features incrementally after the initial prototype
worked, both to improve performance and to support more
functionality. Some of the interesting changes are listed in
\cref{fig:features}.  To improve performance, we switched to
operating on the serialized representation of directories directly
(decoding fields on demand and encoding in-place) and then added also
multi-block directories.  We added support for attributes so that the file
system stores the mode, uid/gid, and modification timestamp for files and directories.
Finally, we implemented the freeing plan described
in \cref{sec:dafny:freeing}, which required additional code through the
whole stack (but by design no changes to the file-system invariant).
We believe additional features such as symbolic links
could be added incrementally with modest effort because
of sequential reasoning and proof automation.

For the smallfile benchmark, as mentioned it took some work to return enough
attribute information that Linux reliably cached everything it could, to avoid
extra RPCs. This work primarily consisted of expanding the attributes operations
return (the RFC says these post-operation attributes are optional but
encouraged), and at the same time expanding the specifications to say this
metadata is correct. These improvements were carried out in many small steps,
but they were similar to the incremental improvements above in terms of taking a
small amount of time and few changes to lines of proof; most of the work went
into figuring out what was missing in the implementation.

GoTxn had fewer incremental changes to the code and proof after its initial
development. Three changes are worth noting. First, the write-ahead log was
split into three components: the on-disk circular buffer, the in-memory data
structure caching the log, and finally the rest of the code integrating these
components. This organization become clear while writing the specification and
thinking about the proof. The write-ahead log is the largest single library in
GoTxn, with the largest lines of code even after splitting off these two data
structures, so factoring its complexity was essential to the proof. The second
change was a change Mark Theng made to remove some unnecessary code that
absorbed multiple writes to the same address before installing, when
benchmarking showed this code actually hurt performance. Finally, the
transactional part of GoTxn implemented with two-phase locking was a later idea,
which pleasantly enough was verified on top of the existing lifting-based
journaling specification without any changes to the underlying proof.

%% \begin{itemize}
%%   \item Directories were initially parsed into a functional data structure, then
%%   re-serialized to make changes. To improve performance, we switched to
%%   operating on the serialized representation directly (decoding fields on demand
%%   and encoding in-place). (2 days, 600 lines changed)
%%   \item Single-block directories were too limiting, so we switched to multiple
%%   blocks, which are read into memory on demand. We still operate on the
%%   serialized representation, but now of only part of the directory. (5 days, 800
%%   lines changed)
%%   \item Initially we only tracked file types. We added support for attributes so
%%   that the mode, uid/gid, and modification timestamp would be stored correctly.
%%   (4 days, 500 lines changed)
%%   \item The file system initially only freed the first 8 direct blocks of an
%%   inode. We implemented the space-freeing plan described in
%%   \cref{sec:dafny:freeing} through the whole stack afterward. (3 days, 1400
%%   lines changed)
%% \end{itemize}
