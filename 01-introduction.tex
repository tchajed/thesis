One crucial service in an operating system is its \emph{file system}, the piece
of software that takes a disk (which simply stores a long sequence of bytes) and
makes it more useful by giving applications the abstraction of files and directories. The file
system is important because applications rely on it to store data, and to
persist that data even if the computer shuts down (perhaps unexpectedly, say due
to a power failure) and reboots. Due to their importance to nearly all applications, many
file systems have internally complicated implementations, with optimizations and
concurrency for high performance. However, due to this complexity, file systems
sometimes have bugs, especially subtle bugs that only manifest if the system
crashes at an inopportune time or if concurrent operations interleave in just
the wrong way. These bugs can result in the file system misbehaving, for example
losing some or all files when the file system's internals are corrupted.

A file system's developers generally use testing to identify and fix bugs in the
implementation. Unfortunately, testing is fundamentally difficult due to the
combination of a concurrent implementation and the requirement that the file
system be correct even when the computer crashes. Both of these facets of a file
system mean even a handful of operations can have a large number of possible
executions, making it difficult to scale testing and have high confidence in the
implementation.

This thesis takes an alternate approach to testing to obtain a correct file
system: we formally verify a new file-system implementation against a high-level
specification of the file system's behavior. The high-level approach of this
thesis is to \emph{implement} a file system, define a mathematical
\emph{specification} of what the file system is supposed to do, and then
\emph{prove} that the implementation always meets the specification regardless
of how the program executes and even under crashes. To increase confidence in
the proof, we carry it out with a computer; such a machine-checked proof
formalizes the specification and proof as code that are automatically checked
for correctness. The main contributions of this thesis lie in the proof, whereas
we use standard techniques in the implementation and give a straightforward
specification.

More concretely the verified
artifact from this thesis is \textbf{DaisyNFS}, which implements the Network
File System (NFS) protocol on top of a disk with block-based access. NFS is a
widely-used protocol for exporting a file system across a network. We chose to
implement NFS since the behavior of an NFS server is relatively well-specified
by the RFC 1813 standard, compared to the requirements on a Linux file system,
for example. The specification for our file system stipulates that each
operation is implemented atomically (with respect to both other threads and on
crash), and behaves according to a formal model of the NFS specification as laid
out in prose in RFC 1813.

\todo{How to add some structure to intro while still giving a short, coherent
overall view? Or should we not bother and leave that short overview to the
abstract or end of the intro?}

DaisyNFS is implemented in two main layers, a division which is important to
situate our contributions. While there are many intermediate abstractions, the
most important is an intermediate abstraction that supports starting and
committing transactions that consist of a sequence of reads and writes which
appear to execute atomically, including if the system crashes. The lower level
of the system is a \emph{transaction system} that implements this abstraction on
top of a raw disk. Next, the \emph{file-system layer} implements each NFS
operation as a single transaction, using the transaction system to automatically
make each file-system operation atomic for
concurrency and crashes. Transactions greatly simplify making a file
system correct, since they handle concurrency and crash safety so the
file system can focus on correctly implementing its data structures and
algorithms.

The structural division is important because a core contribution of the thesis
is an approach to give proofs for the two layers with different techniques.
The transaction
system exposes a simple API but its internals involve lots of
concurrency and crash-safety reasoning. Performance is important since
this layer limits the performance and concurrency of the file system.
This layer's verification uses specialized infrastructure we developed
and describe in the thesis: \textbf{Perennial} is a new program logic
for reasoning about the combination of concurrency and crash safety, and
\textbf{Goose} is a tool that translates the Go implementation of the
system to a model that we can apply Perennial to.

For the file-system implementation and proof, we use the Dafny
verification language. The file-system operations interact with the
transaction system to store and retrieve data. To run the system, we
compile the Dafny code to Go, which imports and calls into the
transaction system as a library. Dafny only supports sequential
reasoning, which is sufficient at this layer because the transaction
system guarantees that the Dafny code appears to run sequentially. In contrast
with Perennial, Dafny is an automated reasoning framework, resulting in simpler
and more productive proofs in this layer.

Using Perennial for the transaction sytem and Dafny for the file-system
operations lets us use the sharpest tool for each part of the proof. However, it
introduces a challenge in the proof: it is not obvious how to connect a verified
transaction system with the Dafny proof. This thesis develops an appropriate
specification for the transaction system and then links the Dafny proof with the
transaction system's specification, to
argue that the linked code is correct. This linking theorem has a proof which stitches together
the Dafny and Coq proofs, so that all of the code is verified with a
machine-checked proof but we verify the system using two very different proof
systems.

The contribution of this thesis lies in how we prove DaisyNFS correct; the
system implementation itself uses standard techniques to achieve good
performance. We developed new verification techniques to reason about crash
safety and concurrency, to reason about code written in an efficient language,
and to take advantage of the transaction system for simpler proofs in the
file-system layer. In particular this thesis makes the following contributions:
\begin{itemize}
  \item Perennial 2.0 is a verification framework for reasoning about crash
  safety and concurrency. We extend concurrent separation logic with new
  techniques to bring its power to make proofs modular to programs with crash
  safety in their specification.
  \item Goose is a system for reasoning about Go using Perennial. We needed a
  new and flexible system to support our custom verification infrastructure, and
  the use of Go improved productivity for implementation and especially
  performance optimization.
  \item A verified transaction system gives code the
  illusion of atomic transactions, with respect to both crashes and other
  threads. The proof uses Perennial to establish a theorem about any client of the
  transaction system.
  \item DaisyNFS is a file system that implements each file system operation as
  a transaction, using the verified transaction system to safely run
  concurrently. A key contribution is a linking theorem showing how
  the Dafny and Perennial proofs can be composed.
\end{itemize}

DaisyNFS implements the NFSv3 protocol, a standard protocol for exposing
a file system over the network. We use DaisyNFS by mounting it with the
Linux NFS client and then interacting with the mounted file system using
the standard system-call API. A performance evaluation of DaisyNFS shows
that it is competitive with the Linux kernel NFS server exporting an
ext4 file system.

\section{State of the art}

Production file systems are generally validated by testing. While testing is
indispensible for development, the nature of a file system makes it difficult to
catch all bugs with only testing. The fundamental difficulty is the
non-determinism from wanting crash safety even in the middle of execution and
having concurrency in the implementation for good performance.

The importance of file-system correctness has been recognized by the academic
community, thus there are many approaches for increasing confidence with
improved testing. One line of work has explored systematically testing crashes
at intermediate points~\cite{mohan:crashmonkey,pillai:appcrash}. Another line of
work has focused on fuzz testing as a way to induce crash-safety
bugs~\cite{xu:janus,kim:hydra}.

some preview of verification techniques

\section{Approach}

What does it mean to give a machine checked, formal proof of a system? At a high
level, program proofs always have three components: an implementation, a
specification, and a proof. When doing machine-checked proofs, all three are
physically represented as code in a verification system. The verification system
checks the proof against the implementation and specification, ensuring that the
proof is complete.

This thesis integrates interactive, foundational proofs using custom
infrastructure (in Coq) as well as automated verification using a
verification-aware programming language (Dafny). These are both machine-checked,
formal proofs, but the interaction models of the two systems are different
enough that we explain them separately.

In Coq, the core feature is proofs based on dependent type theory, which is
expressive enough to represent essentially any math. A first step when using Coq
is to connect the code to a model in Coq of its semantics. The model encodes any
assumptions we make about how the program behaves; the rest of the proof will be
about these behaviors. The semantics is typically structured as a transition
system, where an execution is a sequence of states and as the program executes,
along with some notion of observable behavior, like external I/O or return
values. From now I'll generally refer to the model as simply being the program,
but we'll make a distinction when discussing what it means to have verified
the system.

Now that we have a program in Coq, we can reason about it. The goal of
verification is to prove that the program meets its specification, and
specifications tend to give the allowed behaviors of the program. Usually its
also implicit in verification that the program shouldn't ``go wrong'', for
example by reading an out-of-bounds address in an array. In principle it might
be possible to prove such a theorem directly, by considering all the behaviors
of the program, but such a proof would be monstrous and hard to split up.

A common structure to tame the complexity of reasoning about a program is to use
a \emph{program logic}. The program logic is a way of expressing and proving
statements about the program, including properties about individual functions
and language constructs like \cc{if}-statements and \cc{while}-loops. The proof
in a program logic will often mirror the structure of the code, since each
function has its own specification and groups of related functions have related
specs.

Program logics for concurrency are still an active area of research; only
recently have they reached the maturity to give completely mechanized proofs of
moderate-sized programs. There are few logics that also can reason about crash
safety. Our approach in this thesis is to build a new program logic with all the
concurrency-reasoning features of a modern program logic, plus new features for
reasoning about crashes. What makes this feasible is Iris, which builds a
concurrent program logic on top of a ``base logic''; we are able to reuse the
base logic and extend the program logic with crash-safety reasoning, while
reusing many difficult parts of implementing a new logic.

Using our new program logic, we verified a transaction system. While the proof
has many interesting details described elsewhere in the thesis, for the purposes
of the overall approach we can simply take a look at the overall specification.
What we prove about the transaction system is that any program that uses
transactions really has transactional behavior: its execution is equivalent to a
version of the program where the transactions run atomically.

Next, we use the transaction system in Dafny as the basis for a file system.
The transaction system is implemented as an ordinary Go package, with methods to
start a transaction, execute reads and writes within it, and finally commit or
abort it. Using Dafny's \cc{extern} feature we make these methods available with
assumed specifications that match the specifications proven in Coq.

Dafny verification works quite differently from Coq. Dafny is a programming
language with verification features; contrast this with Coq, which supports
general math that can \emph{model} programs. A Dafny method can be annotated
with a specification. The Dafny checker converts a method and its specification
to a logical formula, which is true if and only if the specification holds for
the method. It then queries a \emph{solver} to determine if the formula is true.
Contrast all of this with Coq, where the user manually develops the program
logic and connects the rules of this logic. The process cannot be perfectly
automated because it is impossible to answer whether a general logical formula
is true or not, but the user can insert annotations to help out the solver, and
generally fewer annotations are needed than lines of proof for Coq. The main
downside to this approach is that it fixes a sequential programming language, so
unlike Coq it isn't possible to reason about concurrency and crashes.

One of the contributions of this thesis is that the file-system design isolates
concurrency and crash safety into the transaction system. This structure leaves
the rest of the implementation only to \emph{sequentially} implement the
file-system logic and data structures. Because this is sequential, crash-free
execution, we use Dafny to implement and verify each operation, then run this
code wrapped in a transaction. Later we will have more to say about the file
system's proof, but for now suffice it to say that we show it correctly
implements the NFS protocol.

Since we use two different formal systems, we do not set up and prove a single,
unified theorem about the whole file system, which is indeed not even
implemented in one language. Instead, the file system is compiled by linking the
Dafny code, after compilation to Go, with the transaction system. Similar to
this code linking step, we prove a linking theorem, showing that the proof from
Coq and proof from Dafny compose properly. The proofs from both systems are
checked automatically so we call them mechanized, but this final linking step is
outside of both systems so we must give it on paper. However, we take care to
make the transaction system's specification strong enough that the two theorems
compose easily. Later we discuss what parts of the system are \emph{trusted} to
guarantee that the whole system is correct; notably, we learn from Coq and Dafny
that the transaction system and file-system code are independently correct, and
the trust only lies in the interface between them.

\section{Challenges}

What is hard about verifying a concurrent file system? There are two broad
challenges: the first is how to structure the file system, and given our
structure, the second is how to tame reasoning about concurrency and crash
safety together.

The first challenge in verifying a concurrent file system is how to structure it
to manage the proof. We address this in our system design, using transactions to
isolate crash safety and concurrency. While transactions are not fundamental to
correctly implementing a file system (see Featherstich, for example), they
helped with verification since they provide a strong specification to all the
software written on top, in a way we were able to formalize and prove.

The second challenge is in proving the transaction system correct: we implement
new techniques to deal with the interactions of concurrency and crash safety in
the transaction system's implementation. The verification challenge is that the
system is highly concurrent, including with lock-free access to disk. At a high
level, concurrency and crashes create non-determinism: the proof must consider a
large number of possible executions. On another level, the implementation is
large, so we must specify and verify individual components and then stitch these
specifications together. There is promising progress on taming this complexity
for concurrent programs, but pragmatically we require new techniques to deal
with the complexities added by lock-free programming and crashes.

While in the introduction we have often given equal weight to the transaction
system and the file-system implementation, the reader will observe that much
more of the thesis is about verifying the transaction system. Partly the
transaction system is more complex, but the more fundamental reason is that the
infrastructure for the transaction system did not exist. In particular we
designed two systems that are needed to even start the proof: Goose connects Go
code to the proof, so that the implementation is efficient, while Perennial is
our new program logic for concurrent storage systems. On the other hand the
purpose of the file-system design is to use standard, simpler infrastructure for
the file system, and this shows in the relative simplicity of that part of the
proof.

\section{How to read this thesis}

\Cref{sec:daisy-nfs} describes the programming interface, specification,
implementation, and proof for the overall file system. When describing the
implementation and proof, a significant component is the transaction system,
which we only give an interface and specification for. The details of that proof
are in \cref{sec:txn}. The transaction system's proof required two
contributions that are part of this thesis. \Cref{sec:goose} is optional
reading on how the proof connects to the code. \Cref{sec:perennial} gives
background needed to understand how the proof itself works.
