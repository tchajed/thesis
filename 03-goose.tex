\tej{probably want to introduce Goose differently, be more goal focused}

The transaction system is implemented in Go and verified in Perennial.
To carry this out, we need a way to connect the implementation to
something Perennial can reason about. The approach we take is to
translate the implementation into a model in Perennial. As long as the
model exhibits all the behaviors of the real code, then a specification
proven for the model should hold for the real Go code.

The verification infrastructure for the transaction system is all
developed as part of this thesis. This chapter describes the overall
approach, which we call Goose. (We will also use Goose in some places to
refer to the subset of Go supported.) Before we can translate Go code, we need
some way of writing a model of the source code. It turns out to be
convenient to use another programming language for this purpose, which
we call GooseLang. After translating, we will want to carry out proofs
on top of the generated GooseLang code, so GooseLang comes equipped with
reasoning principles in Perennial.

This chapter is intentionally fairly independent of the rest of the
thesis for a reader interested in verifying Go code but not the
specifics of the transaction system proof or the file system built on
top. Crash safety is also of little importance in this chapter. We focus
on normal execution of Go here; crashes are modeled simply by stopping
execution and wiping out all of the state except for the disk.

\section{Goals and contributions}

\tej{name what Goose is trying to do earlier --- borrow from the high-level
overview below}

\section{Why Go?}

Go turns out to be a very convenient language for building verification
infrastructure. The Goose translator effectively gives a semantics to
the source code, in the form of the semantics of the generated GooseLang
code. For the verification to be meaningful, the translation must
preserve the semantics of the source, or at least over-approximate it.

We needed to build the translator while being careful to capture the
source code accurately. One highly practical aspect of Go is that it has
good tooling, including libraries for parsing and type-checking Go
source code. Not only do these libraries save time in implementing
Goose, they greatly improve reliability since they are written by
experts (the Go compiler team, extracting code from the compiler
itself).

In addition to having an accurate syntax tree and even types for the
source code, Go is a simple language. There wasn't much in the way of
syntax that we needed before the Goose subset of Go was sufficient to
write real systems, and even idiomatic Go didn't require much more than
the basics. The remaining restrictions in Goose were easy enough to work
with that we could implement GoJournal the way we wanted to.

Finally, Go is a good language to build systems in. It has efficient and
useful built-in slice and map collections, and anything we build on top
is verified. The runtime handles concurrency efficiently and has good
support for synchronization using locks and condition variables,
allowing a low-level implementation. We were able to use low-level
interfaces to the operating system to access the disk. Garbage
collection simplifies some code and carries a relatively low performance
impact due to the efficient runtime. The tooling for testing, debugging,
and profiling is extremely good, making it easy to fix bugs (before
verification or in unverified code) and find performance problems while
optimizing.

\section{Related work}

There are many approaches and systems people have used to verify
implementations.

We wanted control over the translation process to simplify the resulting
model that we needed to write proofs for. Using an existing translator
that essentially translates syntax would still leave the task of giving
a semantics to the output code and proving the right specifications in
Perennial to reason about various parts of the semantics.

Most closely related work is Robbert Krebbers's thesis, on a semantics
for C that includes both operational semantics and an ``axiomatic
semantics'' which is a separation logic for interactive proofs (it also
has an interpreter to test and debug, which produces all of the
behaviors of a program).

VST also models C for the purpose of interactive proofs.

Extraction in the other direction is sometimes used, particularly for ease of
verification; see Verdi, FSCQ, and Fiat. Rather than starting with efficient
code (and requiring the developer to write it), it is possible to instead make
the extraction process generate more efficient code (cite Clement's
fiat-to-facade work).

\tej{Much more to say here}

\section{High-level overview}

The goal of the translation is to model a Go program using GooseLang,
which is a programming language defined in Coq for this purpose. When we
say GooseLang is a programming language, we mean it in a theoretical
sense: GooseLang consists of a type of programs in Coq and a small-step
semantics of these programs. Since GooseLang programs support references
to model the Go heap, the semantics is written in terms of transitions
of (program, heap) pairs where the heap maps pointers to values. The
intention of the translation is that the semantics of the translated
function should cover all the behaviors of the Go code, in terms of
return values and effect on the heap. As long as this is true, a proof
that the translated code always satisfies some specification means that
the real running code will, too.

GooseLang is a low-level language, so many constructs in Go translate to
(small) implementations in GooseLang. This implementation choice proved
to be much more convenient than adding primitives to the language for
every Go construct. For example, a slice is represented as a tuple of
pointer, length, and capacity, and appending to a slice requires
checking for available capacity and copying if none is available.
Appending to a slice is a complicated operation, and it was easier to
write it correctly as a program rather than directly as a transition in
the semantics. The one cost to this design strategy is that an arbitrary
GooseLang program is much more general than translated Go programs. This
has no impact on verifying any \emph{specific} Go program.

The extra generality of GooseLang does have some downsides since at one point in
the development, in the transaction system's specification, we refer to an
arbitrary GooseLang program. This theorem is made a bit more complicated since
to there are some ill-defined GooseLang programs that no Go program could
generate which the theorem needs to exclude. The transaction system
specification uses a standard technique of restricting to well-typed GooseLang
programs, and encoding syntactic restrictions in that type system.

An important aspect of GooseLang is supporting interactive proofs on top
of the translated code. The interactive proofs use separation logic, a
variant of Hoare logic, so specifications describe the behavior of each
individual function. In order to support verification of any translated
code, GooseLang comes with a specification for any primitive or function
that the translated code might refer to, including libraries like slices
used to model more sophisticated Go features. GooseLang has many
``pure'' operations that have no effect on the heap, due to many
primitive data types and operations (for example, there are both 8-,
32-, and 64-bit integers, and arithmetic and logical operations for
each). The specifications for these operations are handled with a single
lemma, which is applied automatically with a tactic \cc{wp_pures}.

Since our goal is to support interactive rather than automated proofs,
it is helpful to make the model simple to work with. We try to maintain
a strong correspondence between the model and source code: each Go
package translates to a single Coq file, and each top-level declaration
in the Go code maps to a Gallina definition (a GooseLang constant or
function). Goose has a special case for translating immutable variables
to let bindings in GooseLang (rather than allocating a pointer that will
only be read). As a result, factoring out a sub-expression to a variable
has little impact on proofs, since it just adds one more pure step.

While the model is simple in terms of control flow and structure, we can
safely translate any given Go operation to a sophisticated model as long
as the proof abstracts it away. The subsequent sections in this chapter
walk through several features of Go. In each case we first implement the
feature in GooseLang, which as a model of its behavior primarily aims to
be faithful to Go. Next, we develop reasoning principles for the
features, in the form of separation logic assertions (for example, to
represent a slice) and Hoare triples (for example, to specify the
behavior of Append). The key is that the model is trusted to capture
Go's behavior so some sophistication is useful, whereas the reasoning
principles aim to hide that complexity to make proofs practical.

\section{GooseLang}

\tej{give a formal description of the target language}

\tej{also need to describe Hoare triples and separation logic, might as well do
that here since it's part of the foundations of reasoning about GooseLang}

\section{Supported and unsupported
features}

Each function is translated to a single Coq definition, which is a
GooseLang function. For concurrency, Goose supports the \cc{go}
statement and the synchronization primitives \cc{*sync.Mutex} and
\cc{sync.Cond}.

Go's primitive uint64, uint32, uint8 (byte), and boolean types are all
supported, as well as most of the pure functions on those types. Goose
also supports pointers, structs, and methods on structs. Finally, Goose
supports Go's built-in data structures, slices and maps.

Notably missing in Goose but prominent in Go is support for interfaces
and channels. We believe both are easy enough to support, but interfaces
were not necessary for our implementation, and rather than channels we
use mutexes and condition variables for more low-level control over
synchronization.

Control flow is also slightly tricky since a Go function is translated
to a single GooseLang expression that should evaluate to the function's
return value. We can support many specific patterns, especially common
cases like early returns inside \cc{if} statements and loops with
\cc{break} and \cc{continue}, but more complex control flow ---
particularly returning from within a loop --- is not supported. If we
wanted to fix this the right solution would probably be to represent all
functions in continuation-passing style, though this would complicate
the translation of every function call.

We do not support Go's defer statement. It would be nice to support some
common and simple patterns, particularly for unlocking, by translating
\cc{defer} statically; Go's general \cc{defer} statement is much more
complicated to model since it can actually be issued dynamically and
pushes to a stack of calls that are executed in reverse order at return
time. This generality is rarely exploited so it would be useful to model even
just uses of \cc{defer} that can be statically analyzed.

We do not support mutual recursion between Go functions, and
additionally require the translation to be in the right order so
definitions appear before they are used. The subtlety here is that
definition management in Go, as in most imperative languages,
conceptually treats all top-level definitions as simultaneous, whereas
Coq processes definitions sequentially. Using Coq definition management
to model Go definition management imposes a limitation compared to Go,
but is much simpler to work with compared to modeling a Go package as a
set of mutually recursive definitions. In such a model it would be
necessary to first give specifications to every definition that may be
assumed by other proofs, and to ensure the proof isn't circular.

\section{Modeling pointers}

Pointers turn out to be slightly subtle because of concurrency. In
short, GooseLang disallows concurrent reads and writes to the same
location by making such racy access undefined behavior (any
specification for a program implies that if the precondition holds, the
program never exhibits undefined behavior). The hardware provides some
guarantees, but they are relatively weak: for example, different cores
can observe writes at different times. Go's own memory model specifies
even weaker guarantees. Rather than attempt to formalize Go's rules
(which are complex and involve defining a partial order over all program
instructions), we side step the issue and make any races undefined,
which works for our intended use cases since we always synchronize
concurrent access with locks.

To disallow concurrent reads and writes we first need to detect them.
The GooseLang semantics does this by augmenting the heap with extra
information giving the number of current readers. Rather than making
reads a single atomic step, we split them into two primitives. The first
increments the number of readers and the second decrements the count and
returns the current value. The semantics of a write are only defined if
there are no readers and undefined otherwise.

Next, we need reasoning principles to abstract away this complexity from
program verification. Separation logic turns out to provide the right
language to reason about racy access. When a thread owns
$l \mapsto v$, we know no other thread can have access to location
$l$, so the specifications for reads and writes are unaffected by the
operations being non-atomic (although their proofs are a bit more
complicated to deal with the new semantics). The only change is that the
Read operation is no longer an atomic primitive but a function that
takes two execution steps. In Iris this means that two threads cannot
share memory with an invariant and must mediate access with a lock,
which transfers ownership of the $l \mapsto v$ for multiple execution
steps.

\section{Modeling locks}

As is typical in Goose, locks are not built-in to GooseLang but modeled
using an implementation based on simpler primitives. Since locks are the
only synchronization primitive, implementing them requires shared
concurrent access, which ordinary pointers do not have in GooseLang.
Instead, the language also includes a primitive atomic compare-and-swap
operation that is only used to implement a model of locks. We could also
use the same operation to model Go's low-level atomic operations, like
\cc{atomic.CompareAndSwapUint64} and \cc{atomic.LoadUint64}, but
have not implemented this yet since we don't have code that uses these
low-level synchronization primitives.

The GooseLang lock model is a simple spin lock, represented as a boolean
that is true if the lock is held. The acquire operation $Acquire(l)$
repeatedly executes $CAS(l, false, true)$ to atomically test that the
lock is false and set it to true if so, while release stores false to
the lock. The implementation as a spin lock is merely an operational
model that captures what the lock does: acquire blocks until the lock is
free and sets it to locked, while release frees the lock. The real locks
we use, Go's builtin \cc{*sync.Mutex}, are more efficient than spin
locks, because the runtime understands how to schedule threads that are
waiting on locks. This scheduling and performance difference is not
visible from the model, which allows any thread to be scheduled after
any atomic operation.

The reasoning principles for locks are more sophisticated than the spin
lock implementation. As is typical in concurrent separation logic, we
associate a \emph{lock invariant} to the lock, which is a predicate that
holds when the lock is free. Because this is a separation logic, we can
also interpret the lock invariant as ownership over some data (for
example, some region of memory); the lock mediates access to this
ownership, handing it out when the lock is acquired and requiring it
back when the lock is released. We prove this specification sound
against the GooseLang spin-lock implementation.

\section{Structs}

One of the first features needed when writing any Go is support for
structs. We treat a struct value as just a tuple of its fields, while
the struct definition gives the names of those fields. This data is
enough to implement constructing a struct from its fields and accessing
a field by name, which we implement in Gallina.

Structs in memory are more interesting than struct values. Structs could
be stored in a single location; due to our non-atomic semantics for
memory, this would be sound even for structs larger than a machine word.
However, this model would be too restrictive: it is safe for threads to
concurrently access \emph{different fields}, just not the same field,
and we actually take advantage of this property (largely to write more
natural Go code; working around this restriction requires splitting
structs up if they are stored in memory).

To support this concurrency, we model a struct in memory with a
flattened representation, with each base element in a separate memory
cell. The flattening applies recursively to fields that are themselves
structs, until a base literal is reached (like an integer or boolean);
base elements are at most a machine word, but can be smaller. When
allocating a new pointer, the semantics flattens composite values and
stores the elements in a sequence of contiguous addresses.

With a flattened representation we need non-trivial code to read a
struct through a pointer, particularly when some of its fields are
themselves flattened structs. We implemented this code by augmenting the
``schema'' that represents a struct type with not only the fields, but
their types as well. The exact types are not important, but we do need
the entire tree of how big each field is and the shape of each field in
order to determine the location and extent of any given field. Using
types to represent these shapes makes the translation much simpler,
since we have access to the type of every sub-expression from the Go
type checker. Any load of a value from memory is translated to a Gallina
LoadTyped macro that takes a Coq representation of the type being loaded
and uses it to determine what offsets to load.

For the purpose of proofs we represent a pointer to an arbitrary type
$t$ with a typed points-to fact of the form $l \mapsto_t v$. This
definition expands to a number of primitive points-to facts, one for
each base element. The specification for loading says
$\{l \mapsto_t v\} LoadTyped(t, l) \{RET v, l \mapsto_t v\}$, which
(much like the non-atomic primitive Load) hides the fact that something
non-atomic is happening and looks like an ordinary dereference.
Similarly, StoreTyped also takes a type, although the specification
requires the caller to prove that the value has the right shape (in
reality it always will because the Go code we translate from is
well-typed).

The payoff of structs being many independent locations is that it is
possible to model references to individual struct fields. From a pointer
to the root of the struct, a field pointer is simply an offset from that
pointer (skipping the flattened representations of the previous fields).
This offset calculation is much like the code to read a struct from
memory, except that it merely computes a single offset rather than
iterating over all the fields and offsets.

The Go language reference specifies that each field acts like an
independent ``variable'' (which is stored in the GooseLang heap when it
is mutable in Go), so this model should accurately reflect the
specification. Moreover modeling structs as independent locations is
also justified as being similar to how the implementation works. Structs
in memory are in reality represented by contiguous memory, and field
access is implemented by computing a pointer from the base of the
struct. The main difference between the physical implementation and the
model is that we use a single, abstract memory location for each field,
whereas the implementation encodes all data into bytes.

Recall that $l \mapsto_t v$ is internally composed of untyped
points-to facts for all the base elements of $v$. In order to reason
about $v$'s fields, we introduce a new struct field points-to fact,
written $l \mapsto_{t.f} v$, which asserts ownership of just field
$f$ of a struct of type $t$ rooted at $l$, and gives that field's
value as $v$. A recursive function gives an ``exploded'' set of struct
fields by iterating over $t$'s fields and $v$ simultaneously. Then,
we give a proof that $l \mapsto_t v$ is equivalent to the separating
conjunction of this exploded list. The result is a convenient lemma for
reasoning about a struct using its fields: in the forward direction, the
equivalence breaks a large typed points-to into individual fields (with
the values computed from $v$), while in the other direction it allows
to prove a $l \mapsto_t v$ by gathering up all the fields.

The struct field points-to is indispensable in proofs, because the
pattern of \cc{x.f} in Go when $x$ is a pointer is in fact a field
load (in C, this would be written \cc{x->f}). The model
for loading a struct field is a function \cc{loadField(x, t, f)}
which is implemented in two steps, first computing the offset to field
$f$ and then dereferencing the computed pointer (in both cases the struct type $t$
describes how to interpret field $f$). Having a field points-to gives
a natural specification for this type of load:
$\hoare{l \mapsto_{t.f} v}{\mathtt{loadField}(x, t, f)}{\Ret{v} l \mapsto_{t.f} v}$.

The lemmas about breaking apart and recombining structs are all proven
against a simpler model of structs that only requires flattening and
offset calculations. In a sense the model is the trusted code, but the
fact that the struct maps-to exploding lemma is true that all of the
expected Hoare triples hold provides strong evidence that the model is
also doing the right thing. For example, the exploding lemma shows that
field offsets are disjoint, since the struct maps-to can be broken into
field points-to facts for each field.

Something to emphasize above: all of the struct code is generic for
struct type $t$, which in the code is concretely the ``schema''
described above, a list of fields and types (the code calls this a
``descriptor'' and uses $d$ as the metavariable, to avoid confusion
with a generic type $t$).

\section{Slices}

One of the most commonly used data structures in Go is the slice
\cc{[]T}, which is a dynamically-sized array of values of type
\cc{T}. Goose supports a wide range of operations on slices,
including appending and sub-slicing; it turns out that the semantics of
mutable slices is non-trivial in Go, resulting in an interesting
semantics and reasoning principles.

A slice is a combination of a pointer, length, and capacity. Slices are
views into a contiguous memory allocation; this view can be narrowed
with sub-slicing operations of the form \cc{s[i:j]}, resulting
in aliased slices. The elements between the length and capacity are not
directly accessible but are used to support efficient amortized appends.
Go's built-in slice operations include bounds checks on all slice
operations and panic if a memory access or sub-slice operation goes out
of bounds.

GooseLang has a primitive for contiguous memory, which we use to model
the allocation underlying a slice (though these are not directly
accessible to Go code, since they do not carry enough information for
bounds checking). On top of these we model slices as a tuple of a base
pointer, length, and capacity.

The GooseLang slice model is directly inspired by the implementation of
slices, including modeling slice capacity. Initially we had a more
abstract model that ignored capacity (which would appear to be just an
optimization), but were surprised to find that this was insufficient to
even accurately model subslicing and appending. Directly modeling slice
capacity was the simplest solution to obtain a model that is faithful to
the Go implementation. The Go language reference isn't specific about
what the slice capacity after various operations should be, so our
GooseLang model picks a non-deterministic capacity in several places
(within appropriate bounds).

The most basic operations on slices are indexing and storing. The
GooseLang model of $s$ is a three-tuple, but for clarity we will refer
to its elements as $ptr(s)$, $len(s)$, and $cap(s)$. The
translation of \cc{s[i]} is essentially a load from
$ptr(s) + i$ (or undefined behavior if this offset is out-of-bounds).
Similarly \cc{s[i] = x} stores to the same location. We
translate Go's \cc{len(s)} directly to $len(s)$. Go also supports
accessing a slice's capacity, but this is rarely used and Goose does not
support it.

The Go \cc{append} operation is the most sophisticated to model. The
behavior of \cc{append(s, x)} where \cc{s: []T} and
\cc{x: T} depends on whether there is extra capacity to store the
new element \cc{x}. If there is capacity, then \cc{x} is stored
there and the append returns a new slice with the same pointer but a
larger length. If there is no capacity, then \cc{append} must
allocate a new slice, copy the existing elements to it, and then store
\cc{x}. In the latter case \cc{append} returns a slice with a
fresh pointer.

The difficulty with Go slices arise when supporting subslicing. Consider
\cc{s[:i]}, where \cc{i} is less than \cc{len(s)}.
Clearly this slice should have the same pointer and length \cc{i},
but what should its capacity be? Surprisingly, the capacity of this
prefix is the full capacity of \cc{s}, which means that the unused
elements of \cc{s[:i]} \emph{include the elements of \cc{s}}
beyond the index \cc{i}. As a result, \cc{append(s[:i], x)}
in fact modifies \cc{s[i]}. GooseLang takes care to model this
behavior by implementing \cc{append} exactly as above, taking into
account that \cc{append(s, x)} might be an in-place operation.

The GooseLang model is specifically designed to be sound by sticking to
the Go implementation as closely as possible, but we want reasoning
about slices to be convenient and high-level, without worrying about
slice capacity directly. The design of GooseLang nicely separates the
model from the reasoning principles --- we verify specifications against
the concrete model, so that only the model is trusted and not the
separation logic specifications.

\newcommand{\sliceRep}{\mathtt{sliceRep}}
\newcommand{\sliceCap}{\mathtt{sliceCap}}

The GooseLang model of slices is based on two abstract predicates:
$\sliceRep(s, l)$ and $\sliceCap(s)$. To model the slice values
themselves we use $s : Slice$ where $Slice$ is a Gallina record; a
function $SliceVal(s) : Val$ converts the Gallina representation to
the GooseLang tuple that the slice model uses. We will only present the
\emph{untyped} version of this specification where $l : list val$, but
GooseLang also has a typed version where $l : list T$ where there is
some (Gallina) function $\mathtt{to\_val} : T -> Val$. The typed version is
practically convenient in proofs but is only a small extension to the
untyped version.

The first predicate $\sliceRep(s, l)$ gives the abstract value of
$s$, the list of values it contains, excluding additional capacity. It
also represents ownership over all these elements, in terms of the
underlying struct points-to facts. We use this predicate to specify
loads and stores:

\[
  \hoareV{\sliceRep(s, l) * i < |l|}%
{\mathtt{s[i]}}%
{\Ret{v} v = l !! i * \sliceRep(s, l)}
\]
\[
  \hoareV{\sliceRep(s, l) * i < |l|}%
 {\mathtt{s[i] = v}}%
{\Ret{v} v = l !! i * \sliceRep(s, l[i := v])}
\]

Next, $\sliceCap(s)$ is an abstract predicate that represents
\emph{ownership over the capacity} of $s$. It is necessary to append,
since appending might need to write to the capacity, but unneeded to
read and write to a slice.
\[
\hoareV{\sliceRep(s, l) * \sliceCap(s)}%
{\mathtt{append(s, x)}}%
{\Ret{s'} \sliceRep(s', l ++ [x]) * \sliceCap(s')}
\]

This specification is fairly simple. In fact, we often use a shorthand
$sliceFull(s, l) = sliceRep(s, l) * sliceCap(s)$ when the proof will
always retain ownership of slice capacity, in which case the spec looks
even simpler. However, the proof is non-trivial, since in one case it
moves ownership from $sliceCap(s)$ to $sliceRep(s', l ++ [x])$
(where $ptr(s') = ptr(s)$), while in the other it constructs a
completely new allocation for $s'$.

The most interesting rules are for subslicing and how they interact with
capacity. Consider \cc{s[:i]} again. While Go has no formal
notion of ownership, our specifications do. We can model the
\emph{value} for \cc{s[:i]} easily enough; call it
$sliceTake(s, i)$ (it simply reduces the length and keeps the capacity
of $s$, as specified by Go). Now we need to decide how ownership of
$sliceRep(s, l) * sliceCap(s)$ should relate to ownership of
$sliceRep(sliceTake(s, i), take(l, i))$. It turns out there are two
possibilities: we can either give up ownership of the remainder of $s$
in exchange for $sliceCap(sliceTake(s, i))$, or we can ignore the
capacity of the subslice and keep
$sliceRep(sliceDrop(s, i), drop(l, i))$. These are incomparable and
unexpressed in the code: the decision is based on whether we intend to
append to the subslice but stop using the old slice, or whether we want
to continue using the remainder of \cc{s}.

Concretely, GooseLang verifies the following entailment for reasoning
about subslicing in terms of the slice model:

$sliceFull(s, l) \vdash sliceFull(sliceTake(s, i), take(l, i))$

This entailment precisely captures how retaining ownership of the
capacity of $sliceTake(s, i)$ requires giving up the remainder of
$s$.

$sliceRep(s, l) \dashv\vdash sliceRep(sliceTake(s, i), take(l, i)) * sliceRep(sliceDrop(s, i), drop(l, i))$

This alternative bidirectional entailment splits $s$ into two parts,
but gives up ownership over $sliceTake(s, i)$'s capacity in exchange
for using those elements in \cc{s[:i]}. From this point it will
not be possible to prove the safety of appending to \cc{s[:i]},
since this would conflict with the separate ownership over
\cc{s[i:]}.

\section{Maps}

After slices, maps are the next most commonly used collection type in
Go. We implement maps as lists of key-value pairs, stored in a single
memory location in reverse insertion order. Go's builtin maps are
\emph{not} thread-safe, so the model enforces single-threaded access by
marking the map as being read while reading from it; this re-uses the
race detection for other pointers to ensure that racy access to a map is
undefined behavior, while allowing concurrent read-read access. Maps
support all the Go operations: insertions, reads (including returning
whether the key is present), \cc{len} to get the number of elements
in the map, deletion, and iteration. Go map iteration is
non-deterministic and in practice random, but we did not model this
since it would be challenging to do so; however, the reasoning
principles for map iteration do not expose an iteration order.

The implementation of maps is the most involved out of any of the Go
primitives. It required directly implementing maps (albeit
inefficiently, using an association list) using recursive GooseLang
code. GooseLang is an untyped language, so our first attempts had basic
errors like missing arguments. We improved our confidence in this
implementation both by testing it and by verifying it. Both of these
essentially rule out type errors (regardless of what specification we give),
and the specification is simple enough
to be a reliable test of behavior. Both simple tests and verification
cover easy mistakes like reading the oldest write to a key rather than
the latest, or duplicate keys during iteration (the implementation must
skip over a key after observing it once).

The proof and specification for maps is relatively easy since they are
non-concurrent, so the proof assumes ownership over the entire map. We
treat a map as a pointer to an abstract map value, a GooseLang value
that encodes the entire map data as a list of key-value pairs. The
specification is based on a pure relation $mapVal(v, m)$ that relates
this encoded value to a Gallina map $m$, which uses \cc{gmap} from
stdpp; for simplicity we use \cc{gmap u64 val} and limit map
keys to integers. Values are not a visible notion to the Go code, since
it always interacts with maps via their pointer, so the specifications
all use $mapRep(l, m) = \exists v, l \mapsto v * mapVal(v, m)$. The
indirection is important, since the Go map value
\cc{m : map[uint64]V} is in fact a reference to a map that is
mutated in-place (unlike a slice, which has both pure data --- pointer,
length, and capacity --- and heap data).

For example,

$\{mapRep(l, m)\} mapDelete(l, k) \{mapRep(l, delete(m, k))\}$

Map iteration has a more sophisticated specification [citation
needed]:

\tej{write down iteration spec}

\section{Testing Goose}

Goose is a trusted component in the entire verification process. For the
overall system's proof to be sound, we rely on the model to produce all
of the behaviors of the Go code; that is, the behaviors of the Go code
(in practice, using the Go compiler) should be a subset of the behaviors
of its translated GooseLang (according to the Coq semantics). As long as
this is case, the proof is sound in that if the modeled system always
satisfies some property the code will, too.

One subtlety in the trust we place in Goose is that it only applies when
Goose translates code successfully, that code compiles in Coq, and the
model has no undefined behavior. If any of these fail, then the proof of
the system would either not be possible or not go through. Therefore the
most important bugs are those where the translation's behavior differs
from that of Go; these can compromise soundness of the system and lead
to a proof that is not borne out in practice.

To increase out confidence in Goose, we implemented a large suite of
unit tests. While these tests check that Goose continues to translate
existing code (and check that the translation has not unexpectedly
change), for soundness the relevant test is to compare Go to the Goose
output. Unfortunately GooseLang is not natively an executable language.
Its semantics is expressed as a Coq relation that specifies how an
expression is evaluated (or gets stuck, indicating undefined behavior).
To test GooseLang code, we implemented an interpreter in Coq, which can
run GooseLang code and produce either an error due to undefined behavior
or a result. While the interpreter is not very efficient, it has good
enough performance to run the Goose unit tests.

The interpreter is an important part of the testing strategy, but
ultimately the comparison is intended to be between Go and the GooseLang
semantics. Thus we verified that the interpreter produces executions in
accordance with the semantics. The correctness theorem is slightly
subtle in that the interpreter produces only one possible execution, but
the non-determinism is only due to the choice of what locations to use
for pointers, which should not affect any visible behavior.

The technical challenge with implementing and verifying the interpreter
is that the semantics uses a convenient but non-executable way of
expressing the order of evaluation. GooseLang is a lambda calculus, so
its semantics is expressed as a transition system between expressions.
It is easy to give the semantics of a primitive at the \emph{head} of an
expression; for example, we can say what $Store(l, v)$ does in a given
heap if $l$ and $v$ are already values (it stores $v$ in the heap
and evaluates to \texttt{\#()}, the unit value). It is also easy to
interpret \emph{pure} reductions like \cc{x + y} where \cc{x}
and \cc{y} are values since the semantics of these pure expressions
is already given as a Gallina function.

The challenge in the interpreter comes from \emph{context} reductions,
which specify how to find a sub-expression within \cc{e} to reduce
if the head is not immediately a value. The code follows a standard
presentation of context reduction using \emph{evaluation contexts}. The
idea is to define a type of evaluation contexts \cc{K} that
represent an expression with a hole; a function $fill(K, e)$, usually
written $K[e]$, fills the hole with $e$. The possible evaluation
contexts give all the context reductions in one compact rule: if $e$
can step to $e'$, then $K[e]$ can step to $K[e']$. Thus the
semantics has a rule that works when any such $K$ exists, while the
interpreter recurses through an expression (in the right order) and
evaluates a sub-expression, then fills it into the context. We prove
this correct, showing that the interpreter and semantics agree on an
evaluation order. (Specifically, the proof shows that the interpreter
produces one of the valid evaluation orders; the semantics is intended
to have a deterministic order, but this is not proven.)

The test suite is structured as a number of test functions, each
producing a boolean. If the test is written correctly, it should produce
\cc{true}, which we test in Go. Then to check the semantics of the
translation, in GooseLang we check that the interpreter returns true for
each test function. While we could compare more sophisticated results
like integers or structs between the two, this strategy is especially
easy to implement, since there is no need to correlate Go and GooseLang
outputs and compare structured data.

The interpreter and test framework was designed and implemented by
Sydney Gibson, and is described in greater detail in her master's
thesis. The thesis includes more details on evaluating the interpreter
itself, for example documenting bugs caught by the test suite and other
bugs that are now part of our regression tests.
